{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T17:17:49.152533Z",
     "start_time": "2020-05-16T17:17:46.270751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory growth ok\n",
      "tensorflow version = 2.1.0\n",
      "Num GPUs Available:  4\n",
      "GPU 1 selected\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "gpuSelected = 1\n",
    "\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "tf.config.set_visible_devices(tf.config.list_physical_devices('GPU')[gpuSelected], 'GPU')\n",
    " \n",
    "try: \n",
    "    tf.config.experimental.set_memory_growth(physical_devices[gpuSelected], True)\n",
    "    \n",
    "    print(\"memory growth ok\")\n",
    "    print(\"tensorflow version = %s\" % tf.__version__)\n",
    "    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "    print(\"GPU %d selected\" % gpuSelected)\n",
    "except: \n",
    "    print(\"erro\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T12:24:16.782150Z",
     "start_time": "2020-07-06T12:24:15.153437Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T17:17:49.355699Z",
     "start_time": "2020-05-16T17:17:49.154195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports ok\n",
      "imports ok\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D, Embedding\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import UpSampling2D, Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tensorflow.keras.layers import concatenate\n",
    "import shutil\n",
    "sys.path.append(\"/data/acunha/LungDL/JDIArtigo/Eduardo/Brain/\")\n",
    "\n",
    "\n",
    "from DataLoader import DataLoader\n",
    "print(\"imports ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T17:17:52.092162Z",
     "start_time": "2020-05-16T17:17:49.357558Z"
    },
    "code_folding": [
     140,
     178
    ]
   },
   "outputs": [],
   "source": [
    "class MMIGAN():\n",
    "    def __init__(self, img_shape, pathOfResult, dataLoader):        # Input shape\n",
    "         \n",
    "        self.img_shape = img_shape\n",
    "        self.pathOfResult = pathOfResult\n",
    "        self.num_classes = 3\n",
    "        \n",
    "        self.generatorInteration = 3\n",
    "        self.loss_weights = [1, 1, 100]\n",
    "        self.data_loader =  dataLoader\n",
    "        \n",
    "        self.pathOfResult = pathOfResult\n",
    "\n",
    "\n",
    "        # Calculate output shape of D (PatchGAN)\n",
    "        patch = int(self.img_shape[0] / 2**4)\n",
    "        self.disc_patch = (patch, patch, 1)\n",
    "\n",
    "        # Number of filters in the first layer of G and D\n",
    "        self.gf = 64\n",
    "        self.df = 64\n",
    "         \n",
    "        losses = ['mse','sparse_categorical_crossentropy']\n",
    "        optimizer = Adam( 0.0002, 0.5, 0.999)\n",
    "        #optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        #self.discriminator.summary()\n",
    "        self.discriminator.compile(loss=losses,\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "        #self.generator.summary()\n",
    "        \n",
    "        # Input images and their conditioning images\n",
    "        img_A = Input(shape=self.img_shape)\n",
    "        img_B = Input(shape=self.img_shape)\n",
    "        label = Input(shape=(1,))\n",
    "        # By conditioning on B generate a fake version of A\n",
    "        \n",
    "        fake_A = self.generator([img_B, label])\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # Discriminators determines validity of translated images / condition pairs\n",
    "        valid, target_label = self.discriminator([fake_A, img_B])\n",
    "\n",
    "        self.combined = Model(inputs=[img_A, img_B, label], outputs=[valid, target_label, fake_A])\n",
    "        self.combined.compile(loss=['mse', 'sparse_categorical_crossentropy', 'mae'],\n",
    "                              loss_weights=self.loss_weights,\n",
    "                              optimizer=optimizer)\n",
    "     \n",
    "    def build_generator(self):\n",
    "        \"\"\"U-Net Generator\"\"\"\n",
    "\n",
    "        def conv2d(layer_input, filters, f_size=4, bn=True):\n",
    "            \"\"\"Layers used during downsampling\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "\n",
    "        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
    "            \"\"\"Layers used during upsampling\"\"\"\n",
    "            u = UpSampling2D(size=2)(layer_input)\n",
    "            u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "            if dropout_rate:\n",
    "                u = Dropout(dropout_rate)(u)\n",
    "            u = BatchNormalization(momentum=0.8)(u)\n",
    "            u = Concatenate()([u, skip_input])\n",
    "            return u\n",
    "       \n",
    "        inputA = Input(shape=self.img_shape)\n",
    "        inputB = Input(shape=(1,))\n",
    "\n",
    "        d1 = conv2d(inputA, self.gf, bn=False)\n",
    "        d2 = conv2d(d1, self.gf*2)\n",
    "        d3 = conv2d(d2, self.gf*4)\n",
    "        d4 = conv2d(d3, self.gf*8)\n",
    "        x = Model(inputA, d4)\n",
    "        print(\"gerador input A\")\n",
    "        x.summary()\n",
    "        \n",
    "        \n",
    "        \n",
    "        y = Flatten()(Embedding(self.num_classes, 8*8*self.gf*4)(inputB))\n",
    "        y = Dense(8*8*self.gf*4)(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = Reshape((8, 8, self.gf*4))(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = LeakyReLU()(y)\n",
    "        y = UpSampling2D()(y)\n",
    "        y = Conv2D(self.gf*8, kernel_size=5, padding=\"same\")(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = LeakyReLU()(y)\n",
    "\n",
    "        y = Model(inputs=inputB, outputs=y)\n",
    "        print(\"gerador input B\")\n",
    "        y.summary()\n",
    "        \n",
    "        \n",
    "        print(\"combinado A + B\")\n",
    "        \n",
    "        combined = concatenate([x.output, y.output])\n",
    "        d5 = conv2d(combined, self.gf*16)\n",
    "        u1 = deconv2d(d5, d4, self.gf*8)\n",
    "        u2 = deconv2d(u1, d3, self.gf*4)\n",
    "        u3 = deconv2d(u2, d2, self.gf*2)\n",
    "        u4 = deconv2d(u3, d1, self.gf*1)\n",
    "        u5 = UpSampling2D(size=2)(u4) \n",
    "        output1 = Conv2D(1, kernel_size=4, strides=1, padding='same', activation='tanh')(u5)\n",
    "\n",
    "        ###\n",
    "        \n",
    "        \"\"\"\n",
    "        d21 = conv2d(output1, self.gf, bn=False)\n",
    "        d22 = conv2d(d21, self.gf*2)\n",
    "        d23 = conv2d(d22, self.gf*4)\n",
    "        d24 = conv2d(d23, self.gf*8)\n",
    "        \n",
    "        u21 = deconv2d(d24, d23, self.gf*4)\n",
    "        u22 = deconv2d(u21, d22, self.gf*2)\n",
    "        u23 = deconv2d(u22, d21, self.gf*1)\n",
    "        u24 = UpSampling2D(size=2)(u23) \n",
    "        output_img = Conv2D(1, kernel_size=4, strides=1, padding='same', activation='tanh')(u24)\n",
    "        \n",
    "        model = Model([inputA, inputB], output_img)\n",
    "        \"\"\"\n",
    "        ###\n",
    "        model = Model([inputA, inputB], output1)\n",
    "         \n",
    "        print(\"Generator\")\n",
    "        print()\n",
    "        print()\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        def d_layer(layer_input, filters, f_size=4, bn=True):\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "\n",
    "        \n",
    "        img_A = Input(shape=self.img_shape)\n",
    "        img_B = Input(shape=self.img_shape)\n",
    "\n",
    "        # Concatenate image and conditioning image by channels to produce input\n",
    "        combined_imgs = Concatenate(axis=-1)([img_A, img_B])\n",
    "\n",
    "        d1 = d_layer(combined_imgs, self.df, bn=False)\n",
    "        d2 = d_layer(d1, self.df*2)\n",
    "        d3 = d_layer(d2, self.df*4)\n",
    "        d4 = d_layer(d3, self.df*8)\n",
    "\n",
    "        validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
    "        \n",
    "        d5 = Flatten()(d4)\n",
    "        label = Dense(self.num_classes, activation=\"softmax\")(d5)\n",
    "        \n",
    "        m = Model([img_A, img_B], d5)\n",
    "        \n",
    "        model = Model([img_A, img_B], [validity, label])\n",
    "        \n",
    "        print(\"discriminator\")\n",
    "        \n",
    "        \n",
    "        return model\n",
    "    \n",
    "    \n",
    "\n",
    "    def train(self, epochs, batch_size=1, sample_interval=500):\n",
    "        \n",
    "        os.makedirs(self.pathOfResult + \"/logs\", exist_ok=True)\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        # Adversarial loss ground truths\n",
    "        valid = np.ones((batch_size,) + self.disc_patch)\n",
    "        fake = np.zeros((batch_size,) + self.disc_patch)\n",
    "        #target_label = np.ones((batch_size,1))\n",
    "        \n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            dlossPlot = []\n",
    "            glossPlot = []\n",
    "            accPlot = []\n",
    "            \n",
    "            for batch_i, (imgs_A, imgs_B, imgs_C) in enumerate(self.data_loader.loadBatchPaired(batch_size)):\n",
    "               \n",
    "                targetLabels = np.random.randint(0, 3, (batch_size, 1))\n",
    "                inputLabels = np.random.randint(0, 3, (batch_size, 1))\n",
    "                for j in range(0, len(targetLabels)):\n",
    "                    while(targetLabels[j] == inputLabels[j]):\n",
    "                      aux = np.random.randint(0, 3, (1, 1))\n",
    "                      targetLabels[j] = aux[0]\n",
    "                \n",
    "                inputImage = []\n",
    "                targetImage = []\n",
    "                \n",
    "                for i in range(0 , batch_size):\n",
    "                    \n",
    "                    if inputLabels[i] == 0:\n",
    "                      #print(\"InputLabel = 0 -> InputImage = T1\")\n",
    "                      inputImage.append(imgs_A[i])\n",
    "                      \n",
    "                    elif inputLabels[i] == 1:\n",
    "                      #print(\"InputLabel = 1 -> InputImage = T2\")\n",
    "                      inputImage.append(imgs_B[i])\n",
    "                      \n",
    "                    else:\n",
    "                      #print(\"InputLabel = 2 -> InputImage = PD\")\n",
    "                      inputImage.append(imgs_C[i])\n",
    "                      \n",
    "                    if targetLabels[i] == 0:\n",
    "                      #print(\"TargetLabel = 0 -> TargetImage = PD\")\n",
    "                      targetImage.append(imgs_A[i])\n",
    "                    \n",
    "                    elif targetLabels[i] == 1:\n",
    "                      #print(\"TargetLabel = 1 -> TargetImage = T2\")\n",
    "                      targetImage.append(imgs_B[i])\n",
    "                    \n",
    "                    else:\n",
    "                      #print(\"TargetLabel = 2 -> TargetImage = PD\")\n",
    "                      targetImage.append(imgs_C[i])\n",
    "                        \n",
    "               \n",
    "                targetImage = np.asarray(targetImage)\n",
    "                inputImage = np.asarray(inputImage)\n",
    "                \n",
    "                \n",
    "                fakeImages = self.generator.predict([inputImage, targetLabels])\n",
    "\n",
    "                d_loss_real = self.discriminator.train_on_batch([targetImage, inputImage], [valid, targetLabels] )\n",
    "                d_loss_fake = self.discriminator.train_on_batch([fakeImages, inputImage], [fake, targetLabels] )\n",
    "                \n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                # -----------------\n",
    "                #  Train Generator\n",
    "                # -----------------\n",
    "\n",
    "                # Train the generators\n",
    "                for interaction in range(self.generatorInteration):\n",
    "                  g_loss = self.combined.train_on_batch([targetImage, inputImage, targetLabels], [valid, targetLabels, targetImage])\n",
    "                \n",
    "                dlossPlot = np.append(dlossPlot, d_loss[0])\n",
    "                glossPlot = np.append(glossPlot, g_loss[0])\n",
    "                accPlot = np.append(accPlot, 100*d_loss[1])\n",
    "                \n",
    "\n",
    "                elapsed_time = datetime.datetime.now() - start_time\n",
    "               \n",
    "                \n",
    "                # Plot the progress\n",
    "                f = open(self.pathOfResult + \"/logs/%d.txt\" % epoch, \"a+\")\n",
    "                f.write(\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%, op_acc: %.2f%%] [G loss: %f] time: %s\" % (epoch, epochs,\n",
    "                                                                        batch_i, self.data_loader.n_batches,\n",
    "                                                                        d_loss[0], 100*d_loss[3], 100*d_loss[4] ,\n",
    "                                                                        g_loss[0],\n",
    "                                                                        elapsed_time))\n",
    "\n",
    "                #f.close() \n",
    "                print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%, op_acc: %.2f%%] [G loss: %f] time: %s\" % (epoch, epochs,\n",
    "                                                                        batch_i, self.data_loader.n_batches,\n",
    "                                                                        d_loss[0], 100*d_loss[3], 100*d_loss[4] ,\n",
    "                                                                        g_loss[0],\n",
    "                                                                        elapsed_time))\n",
    "                \n",
    "                # If at save interval => save generated image samples\n",
    "                if batch_i % sample_interval == 0:\n",
    "                    self.sample_images(epoch, batch_i, self.pathOfResult)\n",
    "                    \n",
    "                if batch_i == int(self.data_loader.n_batches - 10) or batch_i == 2:\n",
    "                    os.makedirs(pathOfResult, exist_ok=True)\n",
    "                    print(\"save generator\")\n",
    "                    os.makedirs(pathOfResult + '/generator' , exist_ok=True)\n",
    "                    self.generator.save(pathOfResult + '/generator/%d.h5' % epoch) \n",
    "   \n",
    "    def sample_images(self, epoch, batch_i, pathOfResult):\n",
    "        os.makedirs(pathOfResult, exist_ok=True)\n",
    "        \n",
    "        #save generator \n",
    "        \n",
    "        \n",
    "        imgs_A, imgs_B, imgs_C = self.data_loader.loadBatchTest(batch_size=1)\n",
    "\n",
    "\n",
    "        targetLabels = np.random.randint(0, 3, (1, 1))\n",
    "        inputLabels = np.random.randint(0, 3, (1, 1))\n",
    "        for j in range(0, len(targetLabels)):\n",
    "            while(targetLabels[j] == inputLabels[j]):\n",
    "              aux = np.random.randint(0, 3, (1, 1))\n",
    "              targetLabels[j] = aux[0]\n",
    "\n",
    "        inputImage = []\n",
    "        targetImage = []\n",
    "\n",
    "        for i in range(0 , 1):\n",
    "\n",
    "            if inputLabels[i] == 0:\n",
    "                inputImage.append(imgs_A[i])\n",
    "\n",
    "            elif inputLabels[i] == 1:\n",
    "                inputImage.append(imgs_B[i])\n",
    "\n",
    "            else:\n",
    "                inputImage.append(imgs_C[i])\n",
    "\n",
    "            if targetLabels[i] == 0:\n",
    "                targetImage.append(imgs_A[i])\n",
    "\n",
    "            elif targetLabels[i] == 1:\n",
    "                targetImage.append(imgs_B[i])\n",
    "\n",
    "            else:\n",
    "                targetImage.append(imgs_C[i])\n",
    "\n",
    "\n",
    "        #print(inputLabels)\n",
    "        #print(targetLabels)\n",
    "        #print()\n",
    "\n",
    "        targetImage = np.asarray(targetImage)\n",
    "        inputImage = np.asarray(inputImage)\n",
    "\n",
    "\n",
    "\n",
    "        fake = self.generator.predict([inputImage, targetLabels])\n",
    "\n",
    "        #gen_imgs = np.concatenate([imgs_B, fake_A, imgs_A])\n",
    "\n",
    "        aux = np.concatenate((targetImage[0][:, :, 0], inputImage[0][:, :, 0]), axis=1)\n",
    "        aux = np.concatenate((aux, fake[0][:, :, 0]), axis=1)\n",
    "        #aux = Image.fromarray(aux)\n",
    "        #aux.save(\"%s/%d_%d-1.png\" % (pathOfResult, epoch, batch_i))\n",
    "        print(\"SAVE\")\n",
    "        if inputLabels[0] == 0 and targetLabels[0] == 1:\n",
    "          plt.imsave(\"%s/%d_%d-T1-T2.png\" % (pathOfResult, epoch, batch_i), aux, cmap=\"gray\")\n",
    "          print(\"%d_%d-T1-T2.png\" % ( epoch, batch_i))\n",
    "\n",
    "        elif inputLabels[0] == 0 and targetLabels[0] == 2:\n",
    "          plt.imsave(\"%s/%d_%d-T1-PD.png\" % (pathOfResult, epoch, batch_i), aux, cmap=\"gray\")        \n",
    "          print(\"%d_%d-T1-PD.png\" % ( epoch, batch_i))\n",
    "\n",
    "        elif inputLabels[0] == 1 and targetLabels[0] == 0:\n",
    "          plt.imsave(\"%s/%d_%d-T2-T1.png\" % (pathOfResult, epoch, batch_i), aux, cmap=\"gray\")\n",
    "          print(\"%d_%d-T2-T1.png\" % ( epoch, batch_i))\n",
    "        elif inputLabels[0] == 1 and targetLabels[0] == 2:\n",
    "          plt.imsave(\"%s/%d_%d-T2-PD.png\" % (pathOfResult, epoch, batch_i), aux, cmap=\"gray\")\n",
    "          print(\"%d_%d-T2-PD.png\" % ( epoch, batch_i))\n",
    "        elif inputLabels[0] == 2 and targetLabels[0] == 0:\n",
    "          plt.imsave(\"%s/%d_%d-PD-T1.png\" % (pathOfResult, epoch, batch_i), aux, cmap=\"gray\")\n",
    "          print(\"%d_%d-PD-T1.png\" % (epoch, batch_i))\n",
    "        else:\n",
    "          plt.imsave(\"%s/%d_%d-PD-T2.png\" % (pathOfResult, epoch, batch_i), aux, cmap=\"gray\")\n",
    "          print(\"%d_%d-PD-T2.png\" % ( epoch, batch_i))\n",
    "              \n",
    "        plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-16T17:17:46.196Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pareadas| Temos um total de 8160 imagens de T1, 8160 de T2 e 8160 de PD para treinamento, os pacientes ['035', '230', '231', '232', '233', '234', '238', '290', '291', '292', '293', '294', '303', '305', '306', '307', '310', '314', '315', '322', '331', '332', '333', '337', '340', '345', '347', '371', '372', '373', '378', '382', '388', '395', '423', '424', '425', '426', '427', '430', '433', '434', '435', '442', '462', '463', '464', '469', '470', '473', '474', '475', '476', '477', '478', '510', '517', '532', '541', '542'] foram selecionados\n",
      "\n",
      "Pareadas| Temos um total de 1768 imagens de T1, 1768 de T2 e 1768 de PD para teste, os pacients ['543', '547', '548', '553', '561', '563', '571', '573', '574', '588', '595', '596', '597'] foram selecionados\n",
      "\n",
      "Nao Pareadas| Temos um total de 8160 imagens de T1, 8160 de T2 e 8160 de PD para treinamento, os pacientes ['035', '230', '231', '232', '233', '234', '238', '290', '291', '292', '293', '294', '303', '305', '306', '307', '310', '314', '315', '322', '331', '332', '333', '337', '340', '345', '347', '371', '372', '373', '378', '382', '388', '395', '423', '424', '425', '426', '427', '430', '433', '434', '435', '442', '462', '463', '464', '469', '470', '473', '474', '475', '476', '477', '478', '510', '517', '532', '541', '542'] foram selecionados\n",
      "\n",
      "Nao Pareadas| Temos um total de 1768 imagens de T1, 1768 de T2 e 1768 de PD para teste, os pacients ['543', '547', '548', '553', '561', '563', '571', '573', '574', '588', '595', '596', '597'] foram selecionados\n",
      "\n",
      "discriminator\n",
      "gerador input A\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 256, 256, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 128, 128, 64)      1088      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 64, 64, 128)       131200    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64, 64, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 256)       524544    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32, 32, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 16, 16, 512)       2097664   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 16, 16, 512)       2048      \n",
      "=================================================================\n",
      "Total params: 2,758,080\n",
      "Trainable params: 2,756,288\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n",
      "gerador input B\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 1, 16384)          49152     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16384)             268451840 \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 16384)             65536     \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 512)       3277312   \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 16, 16, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 16, 16, 512)       0         \n",
      "=================================================================\n",
      "Total params: 271,846,912\n",
      "Trainable params: 271,812,608\n",
      "Non-trainable params: 34,304\n",
      "_________________________________________________________________\n",
      "combinado A + B\n",
      "Generator\n",
      "\n",
      "\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 256, 256, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 128, 128, 64) 1088        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 16384)     49152       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 128, 128, 64) 0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 16384)        0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 64, 128)  131200      leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16384)        268451840   flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 64, 64, 128)  0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16384)        65536       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 128)  512         leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 8, 8, 256)    0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 256)  524544      batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 8, 8, 256)    1024        reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 32, 32, 256)  0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 8, 8, 256)    0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 256)  1024        leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 16, 16, 256)  0           leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 512)  2097664     batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 512)  3277312     up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 16, 16, 512)  0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 512)  2048        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 16, 16, 512)  2048        leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 16, 16, 512)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 16, 16, 1024) 0           batch_normalization_5[0][0]      \n",
      "                                                                 leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 8, 8, 1024)   16778240    concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 8, 8, 1024)   0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 1024)   4096        leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 16, 16, 1024) 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 512)  8389120     up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 512)  2048        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 16, 16, 1024) 0           batch_normalization_10[0][0]     \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 32, 32, 1024) 0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_12 (Conv2D)              (None, 32, 32, 256)  4194560     up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 256)  1024        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32, 32, 512)  0           batch_normalization_11[0][0]     \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 64, 64, 512)  0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 64, 64, 128)  1048704     up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 64, 64, 128)  512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 64, 64, 256)  0           batch_normalization_12[0][0]     \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 128, 128, 256 0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 128, 128, 64) 262208      up_sampling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 128, 128, 64) 256         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 128, 128, 128 0           batch_normalization_13[0][0]     \n",
      "                                                                 leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2D)  (None, 256, 256, 128 0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 256, 256, 1)  2049        up_sampling2d_5[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 305,287,809\n",
      "Trainable params: 305,247,745\n",
      "Non-trainable params: 40,064\n",
      "__________________________________________________________________________________________________\n",
      "[Epoch 0/30] [Batch 0/816] [D loss: 18.686291, acc:  60%, op_acc: 75.00%] [G loss: 42.127502] time: 0:00:08.650027\n",
      "SAVE\n",
      "0_0-T1-T2.png\n",
      "[Epoch 0/30] [Batch 1/816] [D loss: 39.522762, acc:  36%, op_acc: 40.00%] [G loss: 35.523460] time: 0:00:11.322039\n",
      "[Epoch 0/30] [Batch 2/816] [D loss: 26.481506, acc:  64%, op_acc: 40.00%] [G loss: 28.905102] time: 0:00:13.423451\n",
      "save generator\n",
      "[Epoch 0/30] [Batch 3/816] [D loss: 10.939562, acc:  32%, op_acc: 40.00%] [G loss: 23.092701] time: 0:00:20.427952\n",
      "[Epoch 0/30] [Batch 4/816] [D loss: 6.864286, acc:  48%, op_acc: 55.00%] [G loss: 26.464907] time: 0:00:22.684550\n",
      "[Epoch 0/30] [Batch 5/816] [D loss: 9.118948, acc:  49%, op_acc: 45.00%] [G loss: 20.262337] time: 0:00:24.955776\n",
      "[Epoch 0/30] [Batch 6/816] [D loss: 3.635848, acc:  47%, op_acc: 70.00%] [G loss: 20.756832] time: 0:00:27.278471\n",
      "[Epoch 0/30] [Batch 7/816] [D loss: 6.896111, acc:  48%, op_acc: 40.00%] [G loss: 24.067686] time: 0:00:29.428367\n",
      "[Epoch 0/30] [Batch 8/816] [D loss: 8.758915, acc:  42%, op_acc: 35.00%] [G loss: 22.289839] time: 0:00:31.599146\n",
      "[Epoch 0/30] [Batch 9/816] [D loss: 4.599833, acc:  54%, op_acc: 50.00%] [G loss: 18.951685] time: 0:00:33.942064\n",
      "[Epoch 0/30] [Batch 10/816] [D loss: 11.183941, acc:  41%, op_acc: 50.00%] [G loss: 57.228985] time: 0:00:36.199895\n",
      "[Epoch 0/30] [Batch 11/816] [D loss: 11.323282, acc:  48%, op_acc: 40.00%] [G loss: 32.045815] time: 0:00:38.381294\n",
      "[Epoch 0/30] [Batch 12/816] [D loss: 13.703455, acc:  61%, op_acc: 40.00%] [G loss: 24.314655] time: 0:00:40.561668\n",
      "[Epoch 0/30] [Batch 13/816] [D loss: 7.952753, acc:  37%, op_acc: 55.00%] [G loss: 17.760735] time: 0:00:42.794302\n",
      "[Epoch 0/30] [Batch 14/816] [D loss: 6.176245, acc:  47%, op_acc: 55.00%] [G loss: 26.607656] time: 0:00:45.008720\n",
      "[Epoch 0/30] [Batch 15/816] [D loss: 6.572746, acc:  51%, op_acc: 50.00%] [G loss: 24.156240] time: 0:00:47.226767\n",
      "[Epoch 0/30] [Batch 16/816] [D loss: 3.519791, acc:  51%, op_acc: 70.00%] [G loss: 16.405777] time: 0:00:49.539340\n",
      "[Epoch 0/30] [Batch 17/816] [D loss: 3.787097, acc:  58%, op_acc: 60.00%] [G loss: 15.745346] time: 0:00:51.739893\n",
      "[Epoch 0/30] [Batch 18/816] [D loss: 4.327878, acc:  53%, op_acc: 70.00%] [G loss: 15.036501] time: 0:00:54.142927\n",
      "[Epoch 0/30] [Batch 19/816] [D loss: 1.486636, acc:  55%, op_acc: 70.00%] [G loss: 16.648869] time: 0:00:56.628878\n",
      "[Epoch 0/30] [Batch 20/816] [D loss: 5.459179, acc:  50%, op_acc: 65.00%] [G loss: 18.821962] time: 0:00:59.126428\n",
      "[Epoch 0/30] [Batch 21/816] [D loss: 7.572507, acc:  50%, op_acc: 65.00%] [G loss: 16.785498] time: 0:01:01.355966\n",
      "[Epoch 0/30] [Batch 22/816] [D loss: 2.969345, acc:  47%, op_acc: 80.00%] [G loss: 17.101374] time: 0:01:03.544283\n",
      "[Epoch 0/30] [Batch 23/816] [D loss: 2.952003, acc:  48%, op_acc: 65.00%] [G loss: 14.920909] time: 0:01:05.709560\n",
      "[Epoch 0/30] [Batch 24/816] [D loss: 1.406748, acc:  46%, op_acc: 85.00%] [G loss: 15.042766] time: 0:01:07.908627\n",
      "[Epoch 0/30] [Batch 25/816] [D loss: 0.970445, acc:  48%, op_acc: 90.00%] [G loss: 13.947474] time: 0:01:10.120551\n",
      "[Epoch 0/30] [Batch 26/816] [D loss: 0.823897, acc:  43%, op_acc: 90.00%] [G loss: 12.509283] time: 0:01:12.308394\n",
      "[Epoch 0/30] [Batch 27/816] [D loss: 1.146327, acc:  49%, op_acc: 75.00%] [G loss: 10.034284] time: 0:01:14.530363\n",
      "[Epoch 0/30] [Batch 28/816] [D loss: 3.449127, acc:  44%, op_acc: 65.00%] [G loss: 11.673418] time: 0:01:16.783210\n",
      "[Epoch 0/30] [Batch 29/816] [D loss: 0.574657, acc:  45%, op_acc: 95.00%] [G loss: 12.209549] time: 0:01:18.972392\n",
      "[Epoch 0/30] [Batch 30/816] [D loss: 1.444427, acc:  45%, op_acc: 80.00%] [G loss: 9.552664] time: 0:01:21.074933\n",
      "[Epoch 0/30] [Batch 31/816] [D loss: 3.006783, acc:  46%, op_acc: 70.00%] [G loss: 56.614407] time: 0:01:23.290945\n",
      "[Epoch 0/30] [Batch 32/816] [D loss: 2.129056, acc:  55%, op_acc: 75.00%] [G loss: 20.234173] time: 0:01:25.505431\n",
      "[Epoch 0/30] [Batch 33/816] [D loss: 4.420982, acc:  61%, op_acc: 75.00%] [G loss: 28.998236] time: 0:01:27.805239\n",
      "[Epoch 0/30] [Batch 34/816] [D loss: 10.834881, acc:  38%, op_acc: 30.00%] [G loss: 46.100853] time: 0:01:30.014295\n",
      "[Epoch 0/30] [Batch 35/816] [D loss: 9.008552, acc:  51%, op_acc: 60.00%] [G loss: 24.929050] time: 0:01:32.289166\n",
      "[Epoch 0/30] [Batch 36/816] [D loss: 2.761192, acc:  54%, op_acc: 85.00%] [G loss: 19.460632] time: 0:01:34.507469\n",
      "[Epoch 0/30] [Batch 37/816] [D loss: 1.753253, acc:  53%, op_acc: 75.00%] [G loss: 18.016191] time: 0:01:36.664993\n",
      "[Epoch 0/30] [Batch 38/816] [D loss: 4.221077, acc:  45%, op_acc: 60.00%] [G loss: 20.779470] time: 0:01:39.061929\n",
      "[Epoch 0/30] [Batch 39/816] [D loss: 14.731691, acc:  57%, op_acc: 40.00%] [G loss: 54.713081] time: 0:01:41.290785\n",
      "[Epoch 0/30] [Batch 40/816] [D loss: 9.845986, acc:  49%, op_acc: 40.00%] [G loss: 35.228939] time: 0:01:43.462553\n",
      "[Epoch 0/30] [Batch 41/816] [D loss: 19.287868, acc:  53%, op_acc: 50.00%] [G loss: 27.094086] time: 0:01:45.709207\n",
      "[Epoch 0/30] [Batch 42/816] [D loss: 4.717036, acc:  46%, op_acc: 80.00%] [G loss: 22.111822] time: 0:01:47.982532\n",
      "[Epoch 0/30] [Batch 43/816] [D loss: 1.982879, acc:  51%, op_acc: 90.00%] [G loss: 17.474056] time: 0:01:50.207376\n",
      "[Epoch 0/30] [Batch 44/816] [D loss: 0.988759, acc:  55%, op_acc: 95.00%] [G loss: 16.178785] time: 0:01:52.422115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/30] [Batch 45/816] [D loss: 4.790597, acc:  50%, op_acc: 70.00%] [G loss: 18.029097] time: 0:01:54.635941\n",
      "[Epoch 0/30] [Batch 46/816] [D loss: 4.044944, acc:  54%, op_acc: 90.00%] [G loss: 17.379967] time: 0:01:56.915336\n",
      "[Epoch 0/30] [Batch 47/816] [D loss: 4.920577, acc:  51%, op_acc: 80.00%] [G loss: 22.056871] time: 0:01:59.157380\n",
      "[Epoch 0/30] [Batch 48/816] [D loss: 5.053272, acc:  53%, op_acc: 65.00%] [G loss: 15.160204] time: 0:02:01.246192\n",
      "[Epoch 0/30] [Batch 49/816] [D loss: 4.636422, acc:  52%, op_acc: 80.00%] [G loss: 24.449944] time: 0:02:03.475251\n",
      "[Epoch 0/30] [Batch 50/816] [D loss: 4.301491, acc:  49%, op_acc: 80.00%] [G loss: 21.533125] time: 0:02:05.661006\n",
      "[Epoch 0/30] [Batch 51/816] [D loss: 1.030260, acc:  53%, op_acc: 95.00%] [G loss: 19.462782] time: 0:02:07.866574\n",
      "[Epoch 0/30] [Batch 52/816] [D loss: 1.135785, acc:  60%, op_acc: 95.00%] [G loss: 17.589943] time: 0:02:10.063122\n",
      "[Epoch 0/30] [Batch 53/816] [D loss: 1.572554, acc:  52%, op_acc: 90.00%] [G loss: 16.525909] time: 0:02:12.194535\n",
      "[Epoch 0/30] [Batch 54/816] [D loss: 5.687363, acc:  56%, op_acc: 60.00%] [G loss: 16.692368] time: 0:02:14.397390\n",
      "[Epoch 0/30] [Batch 55/816] [D loss: 2.709026, acc:  49%, op_acc: 85.00%] [G loss: 18.163521] time: 0:02:16.616797\n",
      "[Epoch 0/30] [Batch 56/816] [D loss: 1.651380, acc:  58%, op_acc: 85.00%] [G loss: 15.682919] time: 0:02:18.739137\n",
      "[Epoch 0/30] [Batch 57/816] [D loss: 0.626119, acc:  49%, op_acc: 95.00%] [G loss: 13.357206] time: 0:02:20.975003\n",
      "[Epoch 0/30] [Batch 58/816] [D loss: 0.356780, acc:  57%, op_acc: 100.00%] [G loss: 10.917378] time: 0:02:23.186708\n",
      "[Epoch 0/30] [Batch 59/816] [D loss: 0.955899, acc:  45%, op_acc: 90.00%] [G loss: 13.284896] time: 0:02:25.456055\n",
      "[Epoch 0/30] [Batch 60/816] [D loss: 4.015920, acc:  44%, op_acc: 85.00%] [G loss: 13.543356] time: 0:02:27.597033\n",
      "[Epoch 0/30] [Batch 61/816] [D loss: 1.849220, acc:  50%, op_acc: 80.00%] [G loss: 10.465069] time: 0:02:29.826535\n",
      "[Epoch 0/30] [Batch 62/816] [D loss: 1.229035, acc:  48%, op_acc: 90.00%] [G loss: 9.595604] time: 0:02:32.238492\n",
      "[Epoch 0/30] [Batch 63/816] [D loss: 0.605842, acc:  50%, op_acc: 95.00%] [G loss: 12.113444] time: 0:02:34.478289\n",
      "[Epoch 0/30] [Batch 64/816] [D loss: 1.069834, acc:  50%, op_acc: 95.00%] [G loss: 13.943070] time: 0:02:36.700427\n",
      "[Epoch 0/30] [Batch 65/816] [D loss: 2.092309, acc:  54%, op_acc: 90.00%] [G loss: 13.043669] time: 0:02:38.914191\n",
      "[Epoch 0/30] [Batch 66/816] [D loss: 1.797375, acc:  56%, op_acc: 85.00%] [G loss: 12.700380] time: 0:02:41.189079\n",
      "[Epoch 0/30] [Batch 67/816] [D loss: 3.430669, acc:  48%, op_acc: 65.00%] [G loss: 13.180565] time: 0:02:43.406226\n",
      "[Epoch 0/30] [Batch 68/816] [D loss: 4.027202, acc:  58%, op_acc: 80.00%] [G loss: 13.763276] time: 0:02:45.638709\n",
      "[Epoch 0/30] [Batch 69/816] [D loss: 1.183106, acc:  51%, op_acc: 95.00%] [G loss: 13.056530] time: 0:02:47.828337\n",
      "[Epoch 0/30] [Batch 70/816] [D loss: 0.817474, acc:  51%, op_acc: 90.00%] [G loss: 10.235995] time: 0:02:50.035915\n",
      "[Epoch 0/30] [Batch 71/816] [D loss: 1.025453, acc:  46%, op_acc: 85.00%] [G loss: 12.877568] time: 0:02:52.272623\n",
      "[Epoch 0/30] [Batch 72/816] [D loss: 4.786473, acc:  45%, op_acc: 65.00%] [G loss: 10.796113] time: 0:02:54.548141\n",
      "[Epoch 0/30] [Batch 73/816] [D loss: 2.120752, acc:  49%, op_acc: 90.00%] [G loss: 11.985825] time: 0:02:56.768550\n",
      "[Epoch 0/30] [Batch 74/816] [D loss: 1.695907, acc:  59%, op_acc: 90.00%] [G loss: 9.640393] time: 0:02:58.893847\n",
      "[Epoch 0/30] [Batch 75/816] [D loss: 2.073875, acc:  51%, op_acc: 90.00%] [G loss: 12.309483] time: 0:03:01.186644\n",
      "[Epoch 0/30] [Batch 76/816] [D loss: 1.495637, acc:  50%, op_acc: 95.00%] [G loss: 11.322606] time: 0:03:03.440232\n",
      "[Epoch 0/30] [Batch 77/816] [D loss: 2.011178, acc:  50%, op_acc: 90.00%] [G loss: 12.392048] time: 0:03:05.769370\n",
      "[Epoch 0/30] [Batch 78/816] [D loss: 0.519131, acc:  54%, op_acc: 95.00%] [G loss: 12.287409] time: 0:03:08.116211\n",
      "[Epoch 0/30] [Batch 79/816] [D loss: 1.189169, acc:  44%, op_acc: 90.00%] [G loss: 10.165061] time: 0:03:10.247171\n",
      "[Epoch 0/30] [Batch 80/816] [D loss: 1.333317, acc:  49%, op_acc: 90.00%] [G loss: 9.970359] time: 0:03:12.357190\n",
      "[Epoch 0/30] [Batch 81/816] [D loss: 1.542742, acc:  39%, op_acc: 90.00%] [G loss: 11.507214] time: 0:03:14.572655\n",
      "[Epoch 0/30] [Batch 82/816] [D loss: 1.424853, acc:  51%, op_acc: 80.00%] [G loss: 10.451142] time: 0:03:16.760787\n",
      "[Epoch 0/30] [Batch 83/816] [D loss: 1.561950, acc:  46%, op_acc: 85.00%] [G loss: 9.807925] time: 0:03:19.157239\n",
      "[Epoch 0/30] [Batch 84/816] [D loss: 0.622249, acc:  53%, op_acc: 95.00%] [G loss: 10.354374] time: 0:03:21.317819\n",
      "[Epoch 0/30] [Batch 85/816] [D loss: 1.127222, acc:  49%, op_acc: 85.00%] [G loss: 9.219841] time: 0:03:23.449795\n",
      "[Epoch 0/30] [Batch 86/816] [D loss: 0.385481, acc:  45%, op_acc: 100.00%] [G loss: 24.895727] time: 0:03:25.618698\n",
      "[Epoch 0/30] [Batch 87/816] [D loss: 3.304348, acc:  56%, op_acc: 80.00%] [G loss: 17.338133] time: 0:03:27.743315\n",
      "[Epoch 0/30] [Batch 88/816] [D loss: 3.139500, acc:  54%, op_acc: 70.00%] [G loss: 14.415360] time: 0:03:29.938050\n",
      "[Epoch 0/30] [Batch 89/816] [D loss: 4.086565, acc:  51%, op_acc: 80.00%] [G loss: 12.901220] time: 0:03:32.084269\n",
      "[Epoch 0/30] [Batch 90/816] [D loss: 0.943225, acc:  49%, op_acc: 85.00%] [G loss: 14.017330] time: 0:03:34.333624\n",
      "[Epoch 0/30] [Batch 91/816] [D loss: 5.696916, acc:  37%, op_acc: 75.00%] [G loss: 10.468209] time: 0:03:36.559352\n",
      "[Epoch 0/30] [Batch 92/816] [D loss: 0.418005, acc:  48%, op_acc: 100.00%] [G loss: 11.225114] time: 0:03:38.822742\n",
      "[Epoch 0/30] [Batch 93/816] [D loss: 1.088698, acc:  52%, op_acc: 75.00%] [G loss: 11.111682] time: 0:03:41.017101\n",
      "[Epoch 0/30] [Batch 94/816] [D loss: 0.511408, acc:  47%, op_acc: 95.00%] [G loss: 10.226674] time: 0:03:43.241093\n",
      "[Epoch 0/30] [Batch 95/816] [D loss: 0.337943, acc:  48%, op_acc: 100.00%] [G loss: 11.271338] time: 0:03:45.476730\n",
      "[Epoch 0/30] [Batch 96/816] [D loss: 0.356316, acc:  48%, op_acc: 100.00%] [G loss: 10.567507] time: 0:03:47.606104\n",
      "[Epoch 0/30] [Batch 97/816] [D loss: 0.367682, acc:  42%, op_acc: 100.00%] [G loss: 11.423073] time: 0:03:49.781514\n",
      "[Epoch 0/30] [Batch 98/816] [D loss: 0.332360, acc:  48%, op_acc: 100.00%] [G loss: 12.684796] time: 0:03:51.944699\n",
      "[Epoch 0/30] [Batch 99/816] [D loss: 0.825275, acc:  47%, op_acc: 95.00%] [G loss: 10.060644] time: 0:03:54.207022\n",
      "[Epoch 0/30] [Batch 100/816] [D loss: 0.561793, acc:  34%, op_acc: 95.00%] [G loss: 12.232608] time: 0:03:56.414602\n",
      "[Epoch 0/30] [Batch 101/816] [D loss: 2.544782, acc:  34%, op_acc: 70.00%] [G loss: 10.246909] time: 0:03:58.685098\n",
      "[Epoch 0/30] [Batch 102/816] [D loss: 3.926241, acc:  47%, op_acc: 80.00%] [G loss: 9.159545] time: 0:04:00.884274\n",
      "[Epoch 0/30] [Batch 103/816] [D loss: 1.699468, acc:  35%, op_acc: 85.00%] [G loss: 10.361683] time: 0:04:03.112556\n",
      "[Epoch 0/30] [Batch 104/816] [D loss: 0.475880, acc:  50%, op_acc: 95.00%] [G loss: 10.838717] time: 0:04:05.269966\n",
      "[Epoch 0/30] [Batch 105/816] [D loss: 1.201267, acc:  46%, op_acc: 80.00%] [G loss: 9.484344] time: 0:04:07.381377\n",
      "[Epoch 0/30] [Batch 106/816] [D loss: 1.055403, acc:  48%, op_acc: 90.00%] [G loss: 12.021065] time: 0:04:09.559607\n",
      "[Epoch 0/30] [Batch 107/816] [D loss: 1.375140, acc:  52%, op_acc: 90.00%] [G loss: 10.417055] time: 0:04:11.698026\n",
      "[Epoch 0/30] [Batch 108/816] [D loss: 0.352803, acc:  48%, op_acc: 100.00%] [G loss: 8.036819] time: 0:04:14.196225\n",
      "[Epoch 0/30] [Batch 109/816] [D loss: 0.386745, acc:  51%, op_acc: 95.00%] [G loss: 10.156098] time: 0:04:16.407634\n",
      "[Epoch 0/30] [Batch 110/816] [D loss: 1.642781, acc:  42%, op_acc: 85.00%] [G loss: 9.639811] time: 0:04:18.545844\n",
      "[Epoch 0/30] [Batch 111/816] [D loss: 0.308818, acc:  50%, op_acc: 100.00%] [G loss: 10.925056] time: 0:04:20.773669\n",
      "[Epoch 0/30] [Batch 112/816] [D loss: 0.511083, acc:  39%, op_acc: 95.00%] [G loss: 8.493546] time: 0:04:23.105147\n",
      "[Epoch 0/30] [Batch 113/816] [D loss: 1.083387, acc:  45%, op_acc: 95.00%] [G loss: 9.356149] time: 0:04:25.323523\n",
      "[Epoch 0/30] [Batch 114/816] [D loss: 0.701493, acc:  43%, op_acc: 90.00%] [G loss: 12.137950] time: 0:04:27.551395\n",
      "[Epoch 0/30] [Batch 115/816] [D loss: 1.975071, acc:  52%, op_acc: 85.00%] [G loss: 8.731822] time: 0:04:29.689536\n",
      "[Epoch 0/30] [Batch 116/816] [D loss: 0.324027, acc:  52%, op_acc: 100.00%] [G loss: 11.975712] time: 0:04:31.969234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/30] [Batch 117/816] [D loss: 0.392846, acc:  49%, op_acc: 95.00%] [G loss: 11.268739] time: 0:04:34.196901\n",
      "[Epoch 0/30] [Batch 118/816] [D loss: 2.541023, acc:  53%, op_acc: 85.00%] [G loss: 10.024487] time: 0:04:36.272342\n",
      "[Epoch 0/30] [Batch 119/816] [D loss: 4.239997, acc:  53%, op_acc: 85.00%] [G loss: 10.190257] time: 0:04:38.428789\n",
      "[Epoch 0/30] [Batch 120/816] [D loss: 1.250319, acc:  45%, op_acc: 90.00%] [G loss: 8.873822] time: 0:04:40.677705\n",
      "[Epoch 0/30] [Batch 121/816] [D loss: 0.821938, acc:  48%, op_acc: 90.00%] [G loss: 10.544367] time: 0:04:42.825397\n",
      "[Epoch 0/30] [Batch 122/816] [D loss: 3.707140, acc:  47%, op_acc: 80.00%] [G loss: 11.991008] time: 0:04:45.081185\n",
      "[Epoch 0/30] [Batch 123/816] [D loss: 1.066811, acc:  52%, op_acc: 90.00%] [G loss: 9.383269] time: 0:04:47.277067\n",
      "[Epoch 0/30] [Batch 124/816] [D loss: 2.341494, acc:  51%, op_acc: 95.00%] [G loss: 9.003647] time: 0:04:49.519693\n",
      "[Epoch 0/30] [Batch 125/816] [D loss: 0.767260, acc:  50%, op_acc: 80.00%] [G loss: 8.348503] time: 0:04:51.742644\n",
      "[Epoch 0/30] [Batch 126/816] [D loss: 0.566969, acc:  42%, op_acc: 95.00%] [G loss: 8.184570] time: 0:04:54.152325\n",
      "[Epoch 0/30] [Batch 127/816] [D loss: 0.328709, acc:  47%, op_acc: 100.00%] [G loss: 8.855751] time: 0:04:56.360976\n",
      "[Epoch 0/30] [Batch 128/816] [D loss: 0.546154, acc:  40%, op_acc: 90.00%] [G loss: 9.722116] time: 0:04:58.550288\n",
      "[Epoch 0/30] [Batch 129/816] [D loss: 0.315067, acc:  46%, op_acc: 100.00%] [G loss: 8.534495] time: 0:05:00.786498\n",
      "[Epoch 0/30] [Batch 130/816] [D loss: 0.750802, acc:  43%, op_acc: 90.00%] [G loss: 7.445954] time: 0:05:02.937708\n",
      "[Epoch 0/30] [Batch 131/816] [D loss: 0.391179, acc:  35%, op_acc: 100.00%] [G loss: 7.926393] time: 0:05:05.139119\n",
      "[Epoch 0/30] [Batch 132/816] [D loss: 3.513179, acc:  48%, op_acc: 85.00%] [G loss: 12.237168] time: 0:05:07.317528\n",
      "[Epoch 0/30] [Batch 133/816] [D loss: 2.574553, acc:  51%, op_acc: 65.00%] [G loss: 10.070868] time: 0:05:09.442845\n",
      "[Epoch 0/30] [Batch 134/816] [D loss: 2.215337, acc:  54%, op_acc: 95.00%] [G loss: 9.678684] time: 0:05:11.585277\n",
      "[Epoch 0/30] [Batch 135/816] [D loss: 0.304462, acc:  48%, op_acc: 100.00%] [G loss: 10.624763] time: 0:05:13.902184\n",
      "[Epoch 0/30] [Batch 136/816] [D loss: 0.826046, acc:  50%, op_acc: 90.00%] [G loss: 7.274694] time: 0:05:16.125207\n",
      "[Epoch 0/30] [Batch 137/816] [D loss: 1.067684, acc:  48%, op_acc: 95.00%] [G loss: 11.531180] time: 0:05:18.269852\n",
      "[Epoch 0/30] [Batch 138/816] [D loss: 0.625143, acc:  49%, op_acc: 90.00%] [G loss: 9.797056] time: 0:05:20.481406\n",
      "[Epoch 0/30] [Batch 139/816] [D loss: 0.308778, acc:  48%, op_acc: 100.00%] [G loss: 10.340110] time: 0:05:22.629210\n",
      "[Epoch 0/30] [Batch 140/816] [D loss: 0.662609, acc:  47%, op_acc: 95.00%] [G loss: 10.016666] time: 0:05:24.962446\n",
      "[Epoch 0/30] [Batch 141/816] [D loss: 0.579487, acc:  49%, op_acc: 95.00%] [G loss: 9.353330] time: 0:05:27.079224\n",
      "[Epoch 0/30] [Batch 142/816] [D loss: 0.326565, acc:  48%, op_acc: 100.00%] [G loss: 10.531755] time: 0:05:29.235324\n",
      "[Epoch 0/30] [Batch 143/816] [D loss: 0.326383, acc:  40%, op_acc: 100.00%] [G loss: 9.044068] time: 0:05:31.429500\n",
      "[Epoch 0/30] [Batch 144/816] [D loss: 1.312512, acc:  54%, op_acc: 90.00%] [G loss: 7.368602] time: 0:05:33.585005\n",
      "[Epoch 0/30] [Batch 145/816] [D loss: 0.613705, acc:  49%, op_acc: 90.00%] [G loss: 9.071714] time: 0:05:35.808283\n",
      "[Epoch 0/30] [Batch 146/816] [D loss: 0.506875, acc:  50%, op_acc: 95.00%] [G loss: 10.914830] time: 0:05:38.023550\n",
      "[Epoch 0/30] [Batch 147/816] [D loss: 0.526546, acc:  48%, op_acc: 85.00%] [G loss: 8.942926] time: 0:05:40.146408\n",
      "[Epoch 0/30] [Batch 148/816] [D loss: 0.701761, acc:  51%, op_acc: 95.00%] [G loss: 9.668056] time: 0:05:42.301237\n",
      "[Epoch 0/30] [Batch 149/816] [D loss: 0.511888, acc:  40%, op_acc: 90.00%] [G loss: 12.583459] time: 0:05:44.547077\n",
      "[Epoch 0/30] [Batch 150/816] [D loss: 1.123870, acc:  53%, op_acc: 85.00%] [G loss: 11.341455] time: 0:05:46.777764\n",
      "[Epoch 0/30] [Batch 151/816] [D loss: 1.955370, acc:  50%, op_acc: 85.00%] [G loss: 11.894037] time: 0:05:49.182552\n",
      "[Epoch 0/30] [Batch 152/816] [D loss: 0.856479, acc:  52%, op_acc: 95.00%] [G loss: 9.344196] time: 0:05:51.445815\n",
      "[Epoch 0/30] [Batch 153/816] [D loss: 0.547679, acc:  56%, op_acc: 95.00%] [G loss: 11.360806] time: 0:05:53.607437\n",
      "[Epoch 0/30] [Batch 154/816] [D loss: 0.528454, acc:  46%, op_acc: 85.00%] [G loss: 9.869429] time: 0:05:55.778035\n",
      "[Epoch 0/30] [Batch 155/816] [D loss: 2.137354, acc:  45%, op_acc: 85.00%] [G loss: 9.502738] time: 0:05:58.020867\n",
      "[Epoch 0/30] [Batch 156/816] [D loss: 0.423801, acc:  42%, op_acc: 90.00%] [G loss: 8.952541] time: 0:06:00.253806\n",
      "[Epoch 0/30] [Batch 157/816] [D loss: 0.983115, acc:  51%, op_acc: 80.00%] [G loss: 9.086999] time: 0:06:02.752434\n",
      "[Epoch 0/30] [Batch 158/816] [D loss: 0.339860, acc:  44%, op_acc: 100.00%] [G loss: 11.140018] time: 0:06:04.961627\n",
      "[Epoch 0/30] [Batch 159/816] [D loss: 0.350884, acc:  52%, op_acc: 95.00%] [G loss: 7.704391] time: 0:06:07.125942\n",
      "[Epoch 0/30] [Batch 160/816] [D loss: 0.553811, acc:  43%, op_acc: 95.00%] [G loss: 14.491382] time: 0:06:09.367591\n",
      "[Epoch 0/30] [Batch 161/816] [D loss: 7.246423, acc:  61%, op_acc: 65.00%] [G loss: 12.490698] time: 0:06:11.542734\n",
      "[Epoch 0/30] [Batch 162/816] [D loss: 0.557513, acc:  52%, op_acc: 90.00%] [G loss: 14.977073] time: 0:06:13.802578\n",
      "[Epoch 0/30] [Batch 163/816] [D loss: 2.983979, acc:  63%, op_acc: 85.00%] [G loss: 12.554859] time: 0:06:16.006448\n",
      "[Epoch 0/30] [Batch 164/816] [D loss: 3.573675, acc:  40%, op_acc: 75.00%] [G loss: 15.059289] time: 0:06:18.154263\n",
      "[Epoch 0/30] [Batch 165/816] [D loss: 3.924135, acc:  52%, op_acc: 85.00%] [G loss: 14.934259] time: 0:06:20.292329\n",
      "[Epoch 0/30] [Batch 166/816] [D loss: 8.463140, acc:  47%, op_acc: 75.00%] [G loss: 9.876842] time: 0:06:22.529071\n",
      "[Epoch 0/30] [Batch 167/816] [D loss: 1.430736, acc:  58%, op_acc: 85.00%] [G loss: 10.542366] time: 0:06:24.798336\n",
      "[Epoch 0/30] [Batch 168/816] [D loss: 3.463730, acc:  48%, op_acc: 75.00%] [G loss: 12.894701] time: 0:06:26.959025\n",
      "[Epoch 0/30] [Batch 169/816] [D loss: 8.995199, acc:  55%, op_acc: 60.00%] [G loss: 14.030764] time: 0:06:29.299679\n",
      "[Epoch 0/30] [Batch 170/816] [D loss: 0.547144, acc:  49%, op_acc: 95.00%] [G loss: 12.667191] time: 0:06:31.497564\n",
      "[Epoch 0/30] [Batch 171/816] [D loss: 4.153392, acc:  39%, op_acc: 85.00%] [G loss: 11.547717] time: 0:06:33.705849\n",
      "[Epoch 0/30] [Batch 172/816] [D loss: 1.032247, acc:  53%, op_acc: 90.00%] [G loss: 10.120218] time: 0:06:35.915206\n",
      "[Epoch 0/30] [Batch 173/816] [D loss: 0.304923, acc:  50%, op_acc: 100.00%] [G loss: 12.212278] time: 0:06:38.014165\n",
      "[Epoch 0/30] [Batch 174/816] [D loss: 0.352837, acc:  48%, op_acc: 95.00%] [G loss: 10.843819] time: 0:06:40.202239\n",
      "[Epoch 0/30] [Batch 175/816] [D loss: 3.917674, acc:  42%, op_acc: 85.00%] [G loss: 8.576738] time: 0:06:42.413584\n",
      "[Epoch 0/30] [Batch 176/816] [D loss: 0.351236, acc:  45%, op_acc: 100.00%] [G loss: 11.188026] time: 0:06:44.587652\n",
      "[Epoch 0/30] [Batch 177/816] [D loss: 0.378534, acc:  50%, op_acc: 100.00%] [G loss: 9.131321] time: 0:06:46.714206\n",
      "[Epoch 0/30] [Batch 178/816] [D loss: 0.921043, acc:  46%, op_acc: 85.00%] [G loss: 9.513288] time: 0:06:48.963519\n",
      "[Epoch 0/30] [Batch 179/816] [D loss: 1.465508, acc:  37%, op_acc: 90.00%] [G loss: 11.518485] time: 0:06:51.215071\n",
      "[Epoch 0/30] [Batch 180/816] [D loss: 0.658688, acc:  56%, op_acc: 95.00%] [G loss: 10.205616] time: 0:06:53.377344\n",
      "[Epoch 0/30] [Batch 181/816] [D loss: 0.407606, acc:  45%, op_acc: 95.00%] [G loss: 9.369757] time: 0:06:55.642287\n",
      "[Epoch 0/30] [Batch 182/816] [D loss: 3.028269, acc:  45%, op_acc: 90.00%] [G loss: 9.576412] time: 0:06:57.885194\n",
      "[Epoch 0/30] [Batch 183/816] [D loss: 0.968043, acc:  48%, op_acc: 85.00%] [G loss: 7.952426] time: 0:07:00.081952\n",
      "[Epoch 0/30] [Batch 184/816] [D loss: 0.344869, acc:  60%, op_acc: 95.00%] [G loss: 15.356407] time: 0:07:02.203724\n",
      "[Epoch 0/30] [Batch 185/816] [D loss: 3.354335, acc:  35%, op_acc: 85.00%] [G loss: 12.791177] time: 0:07:04.571316\n",
      "[Epoch 0/30] [Batch 186/816] [D loss: 2.158810, acc:  42%, op_acc: 90.00%] [G loss: 13.214869] time: 0:07:06.781817\n",
      "[Epoch 0/30] [Batch 187/816] [D loss: 1.147280, acc:  50%, op_acc: 90.00%] [G loss: 10.025949] time: 0:07:08.984135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/30] [Batch 188/816] [D loss: 1.229764, acc:  47%, op_acc: 85.00%] [G loss: 10.551724] time: 0:07:11.243905\n",
      "[Epoch 0/30] [Batch 189/816] [D loss: 1.067730, acc:  38%, op_acc: 95.00%] [G loss: 11.767858] time: 0:07:13.460252\n",
      "[Epoch 0/30] [Batch 190/816] [D loss: 0.908456, acc:  55%, op_acc: 90.00%] [G loss: 11.473761] time: 0:07:15.676164\n",
      "[Epoch 0/30] [Batch 191/816] [D loss: 0.305028, acc:  52%, op_acc: 100.00%] [G loss: 12.177772] time: 0:07:17.827966\n",
      "[Epoch 0/30] [Batch 192/816] [D loss: 0.316976, acc:  55%, op_acc: 100.00%] [G loss: 11.020803] time: 0:07:20.077530\n",
      "[Epoch 0/30] [Batch 193/816] [D loss: 0.737772, acc:  64%, op_acc: 90.00%] [G loss: 9.272228] time: 0:07:22.228046\n",
      "[Epoch 0/30] [Batch 194/816] [D loss: 0.502091, acc:  50%, op_acc: 95.00%] [G loss: 10.103774] time: 0:07:24.461305\n",
      "[Epoch 0/30] [Batch 195/816] [D loss: 0.340466, acc:  44%, op_acc: 100.00%] [G loss: 10.629846] time: 0:07:26.649367\n",
      "[Epoch 0/30] [Batch 196/816] [D loss: 0.657730, acc:  53%, op_acc: 95.00%] [G loss: 9.931087] time: 0:07:28.842232\n",
      "[Epoch 0/30] [Batch 197/816] [D loss: 2.507425, acc:  57%, op_acc: 85.00%] [G loss: 11.868338] time: 0:07:31.028001\n",
      "[Epoch 0/30] [Batch 198/816] [D loss: 1.345362, acc:  47%, op_acc: 90.00%] [G loss: 10.454515] time: 0:07:33.225478\n",
      "[Epoch 0/30] [Batch 199/816] [D loss: 2.410875, acc:  48%, op_acc: 75.00%] [G loss: 10.519485] time: 0:07:35.429329\n",
      "[Epoch 0/30] [Batch 200/816] [D loss: 1.663920, acc:  33%, op_acc: 90.00%] [G loss: 9.953354] time: 0:07:37.691079\n",
      "SAVE\n",
      "0_200-PD-T1.png\n",
      "[Epoch 0/30] [Batch 201/816] [D loss: 8.765117, acc:  47%, op_acc: 80.00%] [G loss: 11.843038] time: 0:07:40.106624\n",
      "[Epoch 0/30] [Batch 202/816] [D loss: 0.814064, acc:  50%, op_acc: 90.00%] [G loss: 11.252670] time: 0:07:42.294795\n",
      "[Epoch 0/30] [Batch 203/816] [D loss: 1.972266, acc:  46%, op_acc: 90.00%] [G loss: 12.815598] time: 0:07:44.792426\n",
      "[Epoch 0/30] [Batch 204/816] [D loss: 2.535678, acc:  45%, op_acc: 80.00%] [G loss: 9.021128] time: 0:07:47.091283\n",
      "[Epoch 0/30] [Batch 205/816] [D loss: 0.496963, acc:  41%, op_acc: 95.00%] [G loss: 10.221631] time: 0:07:49.324334\n",
      "[Epoch 0/30] [Batch 206/816] [D loss: 0.908286, acc:  61%, op_acc: 90.00%] [G loss: 15.194770] time: 0:07:51.535067\n",
      "[Epoch 0/30] [Batch 207/816] [D loss: 1.326537, acc:  56%, op_acc: 85.00%] [G loss: 11.655673] time: 0:07:53.730734\n",
      "[Epoch 0/30] [Batch 208/816] [D loss: 1.718569, acc:  60%, op_acc: 85.00%] [G loss: 10.143337] time: 0:07:55.909602\n",
      "[Epoch 0/30] [Batch 209/816] [D loss: 1.606530, acc:  53%, op_acc: 90.00%] [G loss: 9.900091] time: 0:07:58.095357\n",
      "[Epoch 0/30] [Batch 210/816] [D loss: 0.781454, acc:  53%, op_acc: 95.00%] [G loss: 10.694018] time: 0:08:00.262273\n",
      "[Epoch 0/30] [Batch 211/816] [D loss: 1.401861, acc:  55%, op_acc: 90.00%] [G loss: 8.519364] time: 0:08:02.659842\n",
      "[Epoch 0/30] [Batch 212/816] [D loss: 0.618436, acc:  43%, op_acc: 90.00%] [G loss: 8.061813] time: 0:08:04.911542\n",
      "[Epoch 0/30] [Batch 213/816] [D loss: 0.316324, acc:  50%, op_acc: 100.00%] [G loss: 11.518400] time: 0:08:07.076981\n",
      "[Epoch 0/30] [Batch 214/816] [D loss: 1.108428, acc:  51%, op_acc: 90.00%] [G loss: 9.523884] time: 0:08:09.251687\n",
      "[Epoch 0/30] [Batch 215/816] [D loss: 2.913051, acc:  46%, op_acc: 85.00%] [G loss: 9.025181] time: 0:08:11.436314\n",
      "[Epoch 0/30] [Batch 216/816] [D loss: 0.502459, acc:  51%, op_acc: 95.00%] [G loss: 9.288706] time: 0:08:13.634417\n",
      "[Epoch 0/30] [Batch 217/816] [D loss: 1.431605, acc:  51%, op_acc: 80.00%] [G loss: 8.731097] time: 0:08:15.837945\n",
      "[Epoch 0/30] [Batch 218/816] [D loss: 0.291962, acc:  53%, op_acc: 100.00%] [G loss: 10.054701] time: 0:08:18.011041\n",
      "[Epoch 0/30] [Batch 219/816] [D loss: 0.277606, acc:  56%, op_acc: 100.00%] [G loss: 14.258287] time: 0:08:20.268308\n",
      "[Epoch 0/30] [Batch 220/816] [D loss: 0.538141, acc:  38%, op_acc: 95.00%] [G loss: 15.515013] time: 0:08:22.524027\n",
      "[Epoch 0/30] [Batch 221/816] [D loss: 0.269594, acc:  55%, op_acc: 100.00%] [G loss: 12.644767] time: 0:08:24.771133\n",
      "[Epoch 0/30] [Batch 222/816] [D loss: 1.039916, acc:  61%, op_acc: 90.00%] [G loss: 12.329258] time: 0:08:27.001734\n",
      "[Epoch 0/30] [Batch 223/816] [D loss: 0.324944, acc:  50%, op_acc: 100.00%] [G loss: 11.143078] time: 0:08:29.151846\n",
      "[Epoch 0/30] [Batch 224/816] [D loss: 1.758648, acc:  51%, op_acc: 90.00%] [G loss: 11.840372] time: 0:08:31.280036\n",
      "[Epoch 0/30] [Batch 225/816] [D loss: 0.248820, acc:  64%, op_acc: 100.00%] [G loss: 10.849671] time: 0:08:33.514369\n",
      "[Epoch 0/30] [Batch 226/816] [D loss: 0.622898, acc:  57%, op_acc: 95.00%] [G loss: 9.497029] time: 0:08:35.739312\n",
      "[Epoch 0/30] [Batch 227/816] [D loss: 1.309723, acc:  58%, op_acc: 95.00%] [G loss: 9.664899] time: 0:08:37.877556\n",
      "[Epoch 0/30] [Batch 228/816] [D loss: 2.030117, acc:  49%, op_acc: 85.00%] [G loss: 10.493528] time: 0:08:40.129272\n",
      "[Epoch 0/30] [Batch 229/816] [D loss: 0.828391, acc:  49%, op_acc: 95.00%] [G loss: 11.188971] time: 0:08:42.307535\n",
      "[Epoch 0/30] [Batch 230/816] [D loss: 1.496694, acc:  53%, op_acc: 80.00%] [G loss: 10.361485] time: 0:08:44.542913\n",
      "[Epoch 0/30] [Batch 231/816] [D loss: 0.320588, acc:  49%, op_acc: 100.00%] [G loss: 10.615893] time: 0:08:46.817080\n",
      "[Epoch 0/30] [Batch 232/816] [D loss: 0.878415, acc:  48%, op_acc: 95.00%] [G loss: 8.504505] time: 0:08:49.160637\n",
      "[Epoch 0/30] [Batch 233/816] [D loss: 2.241591, acc:  51%, op_acc: 90.00%] [G loss: 10.374699] time: 0:08:51.489758\n",
      "[Epoch 0/30] [Batch 234/816] [D loss: 0.718856, acc:  49%, op_acc: 90.00%] [G loss: 10.071302] time: 0:08:53.853381\n",
      "[Epoch 0/30] [Batch 235/816] [D loss: 0.331839, acc:  57%, op_acc: 100.00%] [G loss: 10.024806] time: 0:08:56.052991\n",
      "[Epoch 0/30] [Batch 236/816] [D loss: 0.279572, acc:  48%, op_acc: 100.00%] [G loss: 11.210698] time: 0:08:58.455499\n",
      "[Epoch 0/30] [Batch 237/816] [D loss: 0.407369, acc:  46%, op_acc: 95.00%] [G loss: 8.511460] time: 0:09:00.663087\n",
      "[Epoch 0/30] [Batch 238/816] [D loss: 0.478899, acc:  52%, op_acc: 95.00%] [G loss: 8.589180] time: 0:09:02.844518\n",
      "[Epoch 0/30] [Batch 239/816] [D loss: 1.005094, acc:  50%, op_acc: 95.00%] [G loss: 9.795996] time: 0:09:05.073338\n",
      "[Epoch 0/30] [Batch 240/816] [D loss: 0.714481, acc:  51%, op_acc: 90.00%] [G loss: 7.256667] time: 0:09:07.243315\n",
      "[Epoch 0/30] [Batch 241/816] [D loss: 1.413606, acc:  41%, op_acc: 90.00%] [G loss: 7.528336] time: 0:09:09.713041\n",
      "[Epoch 0/30] [Batch 242/816] [D loss: 1.951814, acc:  53%, op_acc: 90.00%] [G loss: 11.347795] time: 0:09:11.898030\n",
      "[Epoch 0/30] [Batch 243/816] [D loss: 2.057853, acc:  60%, op_acc: 80.00%] [G loss: 7.496037] time: 0:09:14.332958\n",
      "[Epoch 0/30] [Batch 244/816] [D loss: 0.553882, acc:  42%, op_acc: 95.00%] [G loss: 8.434696] time: 0:09:16.573317\n",
      "[Epoch 0/30] [Batch 245/816] [D loss: 0.418518, acc:  49%, op_acc: 90.00%] [G loss: 8.606813] time: 0:09:18.786590\n",
      "[Epoch 0/30] [Batch 246/816] [D loss: 1.750818, acc:  36%, op_acc: 80.00%] [G loss: 9.280592] time: 0:09:21.029625\n",
      "[Epoch 0/30] [Batch 247/816] [D loss: 0.297398, acc:  46%, op_acc: 100.00%] [G loss: 10.052358] time: 0:09:23.193058\n",
      "[Epoch 0/30] [Batch 248/816] [D loss: 0.299328, acc:  49%, op_acc: 100.00%] [G loss: 8.735933] time: 0:09:25.408921\n",
      "[Epoch 0/30] [Batch 249/816] [D loss: 1.595891, acc:  52%, op_acc: 95.00%] [G loss: 7.321840] time: 0:09:27.597384\n",
      "[Epoch 0/30] [Batch 250/816] [D loss: 0.465795, acc:  52%, op_acc: 95.00%] [G loss: 9.310341] time: 0:09:30.001982\n",
      "[Epoch 0/30] [Batch 251/816] [D loss: 0.668615, acc:  55%, op_acc: 95.00%] [G loss: 9.935217] time: 0:09:32.204337\n",
      "[Epoch 0/30] [Batch 252/816] [D loss: 1.654893, acc:  50%, op_acc: 75.00%] [G loss: 6.719726] time: 0:09:34.451898\n",
      "[Epoch 0/30] [Batch 253/816] [D loss: 0.438456, acc:  47%, op_acc: 95.00%] [G loss: 9.286844] time: 0:09:36.593698\n",
      "[Epoch 0/30] [Batch 254/816] [D loss: 1.627052, acc:  51%, op_acc: 80.00%] [G loss: 7.159068] time: 0:09:38.946644\n",
      "[Epoch 0/30] [Batch 255/816] [D loss: 0.355422, acc:  44%, op_acc: 95.00%] [G loss: 11.894111] time: 0:09:41.194430\n",
      "[Epoch 0/30] [Batch 256/816] [D loss: 1.004064, acc:  39%, op_acc: 90.00%] [G loss: 10.480511] time: 0:09:43.401653\n",
      "[Epoch 0/30] [Batch 257/816] [D loss: 0.251644, acc:  56%, op_acc: 100.00%] [G loss: 14.458702] time: 0:09:45.635042\n",
      "[Epoch 0/30] [Batch 258/816] [D loss: 2.376845, acc:  43%, op_acc: 85.00%] [G loss: 14.076970] time: 0:09:47.752424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/30] [Batch 259/816] [D loss: 0.303228, acc:  49%, op_acc: 100.00%] [G loss: 11.819806] time: 0:09:49.898555\n",
      "[Epoch 0/30] [Batch 260/816] [D loss: 0.880428, acc:  51%, op_acc: 90.00%] [G loss: 8.528560] time: 0:09:52.115749\n",
      "[Epoch 0/30] [Batch 261/816] [D loss: 0.332600, acc:  57%, op_acc: 100.00%] [G loss: 10.637918] time: 0:09:54.318877\n",
      "[Epoch 0/30] [Batch 262/816] [D loss: 0.519047, acc:  50%, op_acc: 90.00%] [G loss: 9.897737] time: 0:09:56.461904\n",
      "[Epoch 0/30] [Batch 263/816] [D loss: 0.892625, acc:  40%, op_acc: 95.00%] [G loss: 10.297558] time: 0:09:58.686208\n",
      "[Epoch 0/30] [Batch 264/816] [D loss: 0.303179, acc:  49%, op_acc: 100.00%] [G loss: 11.953350] time: 0:10:00.905484\n",
      "[Epoch 0/30] [Batch 265/816] [D loss: 0.622355, acc:  49%, op_acc: 95.00%] [G loss: 9.272658] time: 0:10:03.312357\n",
      "[Epoch 0/30] [Batch 266/816] [D loss: 1.084054, acc:  44%, op_acc: 90.00%] [G loss: 8.175409] time: 0:10:05.628719\n",
      "[Epoch 0/30] [Batch 267/816] [D loss: 0.299902, acc:  47%, op_acc: 100.00%] [G loss: 9.778256] time: 0:10:07.749985\n",
      "[Epoch 0/30] [Batch 268/816] [D loss: 0.362435, acc:  53%, op_acc: 95.00%] [G loss: 10.905999] time: 0:10:09.988120\n",
      "[Epoch 0/30] [Batch 269/816] [D loss: 0.268625, acc:  53%, op_acc: 100.00%] [G loss: 10.327715] time: 0:10:12.106131\n",
      "[Epoch 0/30] [Batch 270/816] [D loss: 1.158090, acc:  51%, op_acc: 80.00%] [G loss: 9.532540] time: 0:10:14.353646\n",
      "[Epoch 0/30] [Batch 271/816] [D loss: 0.514335, acc:  42%, op_acc: 95.00%] [G loss: 8.575640] time: 0:10:16.554730\n",
      "[Epoch 0/30] [Batch 272/816] [D loss: 0.604566, acc:  54%, op_acc: 95.00%] [G loss: 8.372695] time: 0:10:18.733013\n",
      "[Epoch 0/30] [Batch 273/816] [D loss: 0.310457, acc:  46%, op_acc: 100.00%] [G loss: 12.327319] time: 0:10:20.894402\n",
      "[Epoch 0/30] [Batch 274/816] [D loss: 0.380273, acc:  59%, op_acc: 95.00%] [G loss: 8.933153] time: 0:10:23.134376\n",
      "[Epoch 0/30] [Batch 275/816] [D loss: 4.808589, acc:  50%, op_acc: 75.00%] [G loss: 17.414673] time: 0:10:25.382708\n",
      "[Epoch 0/30] [Batch 276/816] [D loss: 0.901265, acc:  41%, op_acc: 90.00%] [G loss: 10.756447] time: 0:10:27.625660\n",
      "[Epoch 0/30] [Batch 277/816] [D loss: 1.918834, acc:  50%, op_acc: 90.00%] [G loss: 10.476543] time: 0:10:29.845925\n",
      "[Epoch 0/30] [Batch 278/816] [D loss: 1.099471, acc:  55%, op_acc: 90.00%] [G loss: 9.518315] time: 0:10:32.066386\n",
      "[Epoch 0/30] [Batch 279/816] [D loss: 0.736036, acc:  48%, op_acc: 90.00%] [G loss: 8.902401] time: 0:10:34.269168\n",
      "[Epoch 0/30] [Batch 280/816] [D loss: 2.122969, acc:  40%, op_acc: 85.00%] [G loss: 9.636924] time: 0:10:36.481255\n",
      "[Epoch 0/30] [Batch 281/816] [D loss: 1.600976, acc:  47%, op_acc: 90.00%] [G loss: 11.502271] time: 0:10:38.584477\n",
      "[Epoch 0/30] [Batch 282/816] [D loss: 5.386452, acc:  59%, op_acc: 75.00%] [G loss: 13.496930] time: 0:10:40.735367\n",
      "[Epoch 0/30] [Batch 283/816] [D loss: 0.457026, acc:  52%, op_acc: 95.00%] [G loss: 12.375401] time: 0:10:42.925284\n",
      "[Epoch 0/30] [Batch 284/816] [D loss: 0.477161, acc:  49%, op_acc: 95.00%] [G loss: 9.704994] time: 0:10:45.148730\n",
      "[Epoch 0/30] [Batch 285/816] [D loss: 0.805501, acc:  55%, op_acc: 95.00%] [G loss: 9.649027] time: 0:10:47.308645\n",
      "[Epoch 0/30] [Batch 286/816] [D loss: 0.287427, acc:  52%, op_acc: 100.00%] [G loss: 9.373047] time: 0:10:49.540607\n",
      "[Epoch 0/30] [Batch 287/816] [D loss: 0.284633, acc:  53%, op_acc: 100.00%] [G loss: 10.878712] time: 0:10:51.725543\n",
      "[Epoch 0/30] [Batch 288/816] [D loss: 0.300836, acc:  54%, op_acc: 100.00%] [G loss: 10.127656] time: 0:10:54.083877\n",
      "[Epoch 0/30] [Batch 289/816] [D loss: 0.271171, acc:  53%, op_acc: 100.00%] [G loss: 9.808572] time: 0:10:56.256963\n",
      "[Epoch 0/30] [Batch 290/816] [D loss: 0.249836, acc:  59%, op_acc: 100.00%] [G loss: 9.066615] time: 0:10:58.348378\n",
      "[Epoch 0/30] [Batch 291/816] [D loss: 0.794178, acc:  51%, op_acc: 90.00%] [G loss: 8.271879] time: 0:11:00.688258\n",
      "[Epoch 0/30] [Batch 292/816] [D loss: 2.258645, acc:  56%, op_acc: 90.00%] [G loss: 9.518420] time: 0:11:02.910163\n",
      "[Epoch 0/30] [Batch 293/816] [D loss: 0.290563, acc:  51%, op_acc: 100.00%] [G loss: 8.576779] time: 0:11:05.134655\n",
      "[Epoch 0/30] [Batch 294/816] [D loss: 4.149973, acc:  56%, op_acc: 90.00%] [G loss: 9.628045] time: 0:11:07.274807\n",
      "[Epoch 0/30] [Batch 295/816] [D loss: 0.946842, acc:  53%, op_acc: 95.00%] [G loss: 8.855083] time: 0:11:09.510792\n",
      "[Epoch 0/30] [Batch 296/816] [D loss: 1.389906, acc:  53%, op_acc: 80.00%] [G loss: 8.409745] time: 0:11:11.713088\n",
      "[Epoch 0/30] [Batch 297/816] [D loss: 1.987697, acc:  46%, op_acc: 90.00%] [G loss: 10.143293] time: 0:11:13.929300\n",
      "[Epoch 0/30] [Batch 298/816] [D loss: 0.357301, acc:  62%, op_acc: 95.00%] [G loss: 8.893898] time: 0:11:16.164907\n",
      "[Epoch 0/30] [Batch 299/816] [D loss: 1.491382, acc:  55%, op_acc: 75.00%] [G loss: 6.328801] time: 0:11:18.369018\n",
      "[Epoch 0/30] [Batch 300/816] [D loss: 0.788431, acc:  59%, op_acc: 90.00%] [G loss: 9.791145] time: 0:11:20.544408\n",
      "[Epoch 0/30] [Batch 301/816] [D loss: 0.267885, acc:  53%, op_acc: 100.00%] [G loss: 10.374746] time: 0:11:22.765893\n",
      "[Epoch 0/30] [Batch 302/816] [D loss: 0.279379, acc:  53%, op_acc: 100.00%] [G loss: 9.400090] time: 0:11:25.022579\n",
      "[Epoch 0/30] [Batch 303/816] [D loss: 1.342496, acc:  56%, op_acc: 90.00%] [G loss: 8.282479] time: 0:11:27.175817\n",
      "[Epoch 0/30] [Batch 304/816] [D loss: 0.302194, acc:  46%, op_acc: 100.00%] [G loss: 9.684752] time: 0:11:29.377792\n",
      "[Epoch 0/30] [Batch 305/816] [D loss: 0.296321, acc:  56%, op_acc: 100.00%] [G loss: 9.853720] time: 0:11:31.597670\n",
      "[Epoch 0/30] [Batch 306/816] [D loss: 0.284572, acc:  57%, op_acc: 100.00%] [G loss: 8.785486] time: 0:11:33.915415\n",
      "[Epoch 0/30] [Batch 307/816] [D loss: 0.235843, acc:  60%, op_acc: 100.00%] [G loss: 9.562778] time: 0:11:36.176130\n",
      "[Epoch 0/30] [Batch 308/816] [D loss: 0.289292, acc:  45%, op_acc: 100.00%] [G loss: 10.977348] time: 0:11:38.467182\n",
      "[Epoch 0/30] [Batch 309/816] [D loss: 2.738856, acc:  50%, op_acc: 80.00%] [G loss: 8.549107] time: 0:11:40.659100\n",
      "[Epoch 0/30] [Batch 310/816] [D loss: 0.312436, acc:  48%, op_acc: 100.00%] [G loss: 8.344024] time: 0:11:42.899574\n",
      "[Epoch 0/30] [Batch 311/816] [D loss: 0.617156, acc:  47%, op_acc: 90.00%] [G loss: 10.341059] time: 0:11:45.134485\n",
      "[Epoch 0/30] [Batch 312/816] [D loss: 0.357469, acc:  55%, op_acc: 95.00%] [G loss: 8.704248] time: 0:11:47.299194\n",
      "[Epoch 0/30] [Batch 313/816] [D loss: 0.815591, acc:  52%, op_acc: 90.00%] [G loss: 9.810996] time: 0:11:49.517706\n",
      "[Epoch 0/30] [Batch 314/816] [D loss: 0.614014, acc:  52%, op_acc: 95.00%] [G loss: 9.246886] time: 0:11:51.764923\n",
      "[Epoch 0/30] [Batch 315/816] [D loss: 0.266233, acc:  53%, op_acc: 100.00%] [G loss: 10.261456] time: 0:11:54.135327\n",
      "[Epoch 0/30] [Batch 316/816] [D loss: 0.689049, acc:  52%, op_acc: 95.00%] [G loss: 9.972010] time: 0:11:56.402566\n",
      "[Epoch 0/30] [Batch 317/816] [D loss: 1.316543, acc:  48%, op_acc: 95.00%] [G loss: 8.572701] time: 0:11:58.633772\n",
      "[Epoch 0/30] [Batch 318/816] [D loss: 0.255214, acc:  59%, op_acc: 100.00%] [G loss: 8.491167] time: 0:12:01.033964\n",
      "[Epoch 0/30] [Batch 319/816] [D loss: 0.876811, acc:  59%, op_acc: 90.00%] [G loss: 9.104193] time: 0:12:03.240191\n",
      "[Epoch 0/30] [Batch 320/816] [D loss: 0.634678, acc:  55%, op_acc: 95.00%] [G loss: 7.829858] time: 0:12:05.429482\n",
      "[Epoch 0/30] [Batch 321/816] [D loss: 0.283463, acc:  54%, op_acc: 100.00%] [G loss: 9.606996] time: 0:12:07.635619\n",
      "[Epoch 0/30] [Batch 322/816] [D loss: 2.023504, acc:  57%, op_acc: 85.00%] [G loss: 9.111109] time: 0:12:09.830347\n",
      "[Epoch 0/30] [Batch 323/816] [D loss: 1.599711, acc:  49%, op_acc: 95.00%] [G loss: 9.109151] time: 0:12:11.991049\n",
      "[Epoch 0/30] [Batch 324/816] [D loss: 1.311429, acc:  56%, op_acc: 95.00%] [G loss: 10.268197] time: 0:12:14.260964\n",
      "[Epoch 0/30] [Batch 325/816] [D loss: 3.257368, acc:  58%, op_acc: 80.00%] [G loss: 9.384600] time: 0:12:16.488012\n",
      "[Epoch 0/30] [Batch 326/816] [D loss: 0.321980, acc:  46%, op_acc: 100.00%] [G loss: 9.881479] time: 0:12:18.646828\n",
      "[Epoch 0/30] [Batch 327/816] [D loss: 0.351163, acc:  48%, op_acc: 95.00%] [G loss: 7.930610] time: 0:12:20.757627\n",
      "[Epoch 0/30] [Batch 328/816] [D loss: 0.261885, acc:  58%, op_acc: 100.00%] [G loss: 9.797349] time: 0:12:23.049113\n",
      "[Epoch 0/30] [Batch 329/816] [D loss: 1.935534, acc:  50%, op_acc: 85.00%] [G loss: 9.262817] time: 0:12:25.273641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/30] [Batch 330/816] [D loss: 0.578856, acc:  52%, op_acc: 90.00%] [G loss: 8.177958] time: 0:12:27.724469\n",
      "[Epoch 0/30] [Batch 331/816] [D loss: 0.634607, acc:  40%, op_acc: 90.00%] [G loss: 7.824821] time: 0:12:29.942940\n",
      "[Epoch 0/30] [Batch 332/816] [D loss: 1.119513, acc:  52%, op_acc: 95.00%] [G loss: 8.149443] time: 0:12:32.064295\n",
      "[Epoch 0/30] [Batch 333/816] [D loss: 1.824474, acc:  52%, op_acc: 85.00%] [G loss: 9.476975] time: 0:12:34.296686\n",
      "[Epoch 0/30] [Batch 334/816] [D loss: 0.871922, acc:  53%, op_acc: 95.00%] [G loss: 9.729071] time: 0:12:36.436840\n",
      "[Epoch 0/30] [Batch 335/816] [D loss: 1.961192, acc:  60%, op_acc: 90.00%] [G loss: 10.075201] time: 0:12:38.571426\n",
      "[Epoch 0/30] [Batch 336/816] [D loss: 1.241844, acc:  42%, op_acc: 85.00%] [G loss: 17.439243] time: 0:12:40.771622\n",
      "[Epoch 0/30] [Batch 337/816] [D loss: 0.285735, acc:  61%, op_acc: 95.00%] [G loss: 14.762677] time: 0:12:42.922650\n",
      "[Epoch 0/30] [Batch 338/816] [D loss: 0.238868, acc:  68%, op_acc: 100.00%] [G loss: 12.051483] time: 0:12:45.144557\n",
      "[Epoch 0/30] [Batch 339/816] [D loss: 0.254542, acc:  61%, op_acc: 100.00%] [G loss: 9.728841] time: 0:12:47.334730\n",
      "[Epoch 0/30] [Batch 340/816] [D loss: 0.607635, acc:  55%, op_acc: 95.00%] [G loss: 10.297951] time: 0:12:49.495364\n",
      "[Epoch 0/30] [Batch 341/816] [D loss: 0.469874, acc:  54%, op_acc: 95.00%] [G loss: 9.195417] time: 0:12:51.836270\n",
      "[Epoch 0/30] [Batch 342/816] [D loss: 1.070261, acc:  58%, op_acc: 95.00%] [G loss: 10.882921] time: 0:12:54.144988\n",
      "[Epoch 0/30] [Batch 343/816] [D loss: 0.264921, acc:  54%, op_acc: 100.00%] [G loss: 14.752948] time: 0:12:56.298831\n",
      "[Epoch 0/30] [Batch 344/816] [D loss: 8.365305, acc:  48%, op_acc: 85.00%] [G loss: 17.266163] time: 0:12:58.518164\n",
      "[Epoch 0/30] [Batch 345/816] [D loss: 4.571122, acc:  59%, op_acc: 95.00%] [G loss: 14.876902] time: 0:13:00.739070\n",
      "[Epoch 0/30] [Batch 346/816] [D loss: 6.169142, acc:  34%, op_acc: 65.00%] [G loss: 13.434173] time: 0:13:02.987528\n",
      "[Epoch 0/30] [Batch 347/816] [D loss: 1.862728, acc:  45%, op_acc: 90.00%] [G loss: 15.182826] time: 0:13:05.151844\n",
      "[Epoch 0/30] [Batch 348/816] [D loss: 1.184505, acc:  65%, op_acc: 90.00%] [G loss: 12.484585] time: 0:13:07.344565\n",
      "[Epoch 0/30] [Batch 349/816] [D loss: 2.922933, acc:  64%, op_acc: 75.00%] [G loss: 13.720365] time: 0:13:09.521518\n",
      "[Epoch 0/30] [Batch 350/816] [D loss: 0.956130, acc:  50%, op_acc: 95.00%] [G loss: 12.090297] time: 0:13:11.641490\n",
      "[Epoch 0/30] [Batch 351/816] [D loss: 0.937595, acc:  47%, op_acc: 95.00%] [G loss: 10.645315] time: 0:13:13.868691\n",
      "[Epoch 0/30] [Batch 352/816] [D loss: 1.139501, acc:  56%, op_acc: 90.00%] [G loss: 10.989696] time: 0:13:15.998097\n",
      "[Epoch 0/30] [Batch 353/816] [D loss: 0.825929, acc:  47%, op_acc: 90.00%] [G loss: 12.051656] time: 0:13:18.117828\n",
      "[Epoch 0/30] [Batch 354/816] [D loss: 1.574636, acc:  55%, op_acc: 90.00%] [G loss: 9.857641] time: 0:13:20.337937\n",
      "[Epoch 0/30] [Batch 355/816] [D loss: 2.351341, acc:  51%, op_acc: 80.00%] [G loss: 9.558302] time: 0:13:22.559415\n",
      "[Epoch 0/30] [Batch 356/816] [D loss: 0.379212, acc:  54%, op_acc: 95.00%] [G loss: 10.719891] time: 0:13:24.768889\n",
      "[Epoch 0/30] [Batch 357/816] [D loss: 0.339675, acc:  55%, op_acc: 100.00%] [G loss: 10.920940] time: 0:13:26.991727\n",
      "[Epoch 0/30] [Batch 358/816] [D loss: 0.839472, acc:  52%, op_acc: 85.00%] [G loss: 9.835352] time: 0:13:29.215947\n",
      "[Epoch 0/30] [Batch 359/816] [D loss: 0.284497, acc:  54%, op_acc: 100.00%] [G loss: 8.473577] time: 0:13:31.450527\n",
      "[Epoch 0/30] [Batch 360/816] [D loss: 4.312449, acc:  46%, op_acc: 85.00%] [G loss: 10.026740] time: 0:13:33.778959\n",
      "[Epoch 0/30] [Batch 361/816] [D loss: 1.354184, acc:  53%, op_acc: 90.00%] [G loss: 9.033940] time: 0:13:35.979929\n",
      "[Epoch 0/30] [Batch 362/816] [D loss: 0.279589, acc:  61%, op_acc: 100.00%] [G loss: 11.228502] time: 0:13:38.159046\n",
      "[Epoch 0/30] [Batch 363/816] [D loss: 0.421757, acc:  54%, op_acc: 95.00%] [G loss: 12.552921] time: 0:13:40.372303\n",
      "[Epoch 0/30] [Batch 364/816] [D loss: 0.407606, acc:  64%, op_acc: 95.00%] [G loss: 10.282844] time: 0:13:42.788930\n",
      "[Epoch 0/30] [Batch 365/816] [D loss: 0.701136, acc:  45%, op_acc: 90.00%] [G loss: 10.719987] time: 0:13:44.999922\n",
      "[Epoch 0/30] [Batch 366/816] [D loss: 0.463507, acc:  55%, op_acc: 95.00%] [G loss: 9.143598] time: 0:13:47.208386\n",
      "[Epoch 0/30] [Batch 367/816] [D loss: 1.106883, acc:  60%, op_acc: 85.00%] [G loss: 10.877048] time: 0:13:49.429307\n",
      "[Epoch 0/30] [Batch 368/816] [D loss: 0.287515, acc:  47%, op_acc: 100.00%] [G loss: 8.016846] time: 0:13:51.690105\n",
      "[Epoch 0/30] [Batch 369/816] [D loss: 1.062364, acc:  49%, op_acc: 90.00%] [G loss: 9.971832] time: 0:13:53.901495\n",
      "[Epoch 0/30] [Batch 370/816] [D loss: 0.260939, acc:  61%, op_acc: 100.00%] [G loss: 9.249663] time: 0:13:56.145782\n",
      "[Epoch 0/30] [Batch 371/816] [D loss: 0.280080, acc:  50%, op_acc: 100.00%] [G loss: 8.213317] time: 0:13:58.374990\n",
      "[Epoch 0/30] [Batch 372/816] [D loss: 0.572904, acc:  54%, op_acc: 95.00%] [G loss: 8.132325] time: 0:14:00.539009\n",
      "[Epoch 0/30] [Batch 373/816] [D loss: 1.268386, acc:  49%, op_acc: 85.00%] [G loss: 8.832436] time: 0:14:02.844003\n",
      "[Epoch 0/30] [Batch 374/816] [D loss: 0.282982, acc:  51%, op_acc: 100.00%] [G loss: 9.485267] time: 0:14:05.041494\n",
      "[Epoch 0/30] [Batch 375/816] [D loss: 0.230827, acc:  62%, op_acc: 100.00%] [G loss: 9.344116] time: 0:14:07.368726\n",
      "[Epoch 0/30] [Batch 376/816] [D loss: 0.336777, acc:  58%, op_acc: 100.00%] [G loss: 10.575902] time: 0:14:09.606969\n",
      "[Epoch 0/30] [Batch 377/816] [D loss: 0.387590, acc:  49%, op_acc: 90.00%] [G loss: 8.293417] time: 0:14:11.769983\n",
      "[Epoch 0/30] [Batch 378/816] [D loss: 0.983428, acc:  43%, op_acc: 90.00%] [G loss: 8.502990] time: 0:14:14.111128\n",
      "[Epoch 0/30] [Batch 379/816] [D loss: 0.867240, acc:  45%, op_acc: 90.00%] [G loss: 8.879914] time: 0:14:16.357315\n",
      "[Epoch 0/30] [Batch 380/816] [D loss: 3.212605, acc:  54%, op_acc: 90.00%] [G loss: 9.457824] time: 0:14:18.505190\n",
      "[Epoch 0/30] [Batch 381/816] [D loss: 1.423557, acc:  59%, op_acc: 90.00%] [G loss: 8.675701] time: 0:14:20.668664\n",
      "[Epoch 0/30] [Batch 382/816] [D loss: 0.270547, acc:  57%, op_acc: 100.00%] [G loss: 9.384183] time: 0:14:22.926540\n",
      "[Epoch 0/30] [Batch 383/816] [D loss: 0.249113, acc:  60%, op_acc: 100.00%] [G loss: 10.133956] time: 0:14:25.183369\n",
      "[Epoch 0/30] [Batch 384/816] [D loss: 0.679224, acc:  52%, op_acc: 95.00%] [G loss: 8.197680] time: 0:14:27.273237\n",
      "[Epoch 0/30] [Batch 385/816] [D loss: 2.165264, acc:  55%, op_acc: 80.00%] [G loss: 8.579604] time: 0:14:29.492435\n",
      "[Epoch 0/30] [Batch 386/816] [D loss: 1.447770, acc:  60%, op_acc: 90.00%] [G loss: 10.673866] time: 0:14:31.612659\n",
      "[Epoch 0/30] [Batch 387/816] [D loss: 1.574384, acc:  57%, op_acc: 95.00%] [G loss: 9.025427] time: 0:14:33.994004\n",
      "[Epoch 0/30] [Batch 388/816] [D loss: 0.471726, acc:  47%, op_acc: 95.00%] [G loss: 8.657373] time: 0:14:36.233352\n",
      "[Epoch 0/30] [Batch 389/816] [D loss: 0.357177, acc:  49%, op_acc: 95.00%] [G loss: 9.664678] time: 0:14:38.410847\n",
      "[Epoch 0/30] [Batch 390/816] [D loss: 1.262509, acc:  51%, op_acc: 85.00%] [G loss: 7.908766] time: 0:14:40.580042\n",
      "[Epoch 0/30] [Batch 391/816] [D loss: 0.753987, acc:  55%, op_acc: 85.00%] [G loss: 9.483931] time: 0:14:42.876361\n",
      "[Epoch 0/30] [Batch 392/816] [D loss: 1.039708, acc:  59%, op_acc: 95.00%] [G loss: 10.300337] time: 0:14:45.113902\n",
      "[Epoch 0/30] [Batch 393/816] [D loss: 0.725309, acc:  54%, op_acc: 95.00%] [G loss: 9.835328] time: 0:14:47.443859\n",
      "[Epoch 0/30] [Batch 394/816] [D loss: 0.263311, acc:  57%, op_acc: 100.00%] [G loss: 9.962893] time: 0:14:49.668684\n",
      "[Epoch 0/30] [Batch 395/816] [D loss: 0.271824, acc:  54%, op_acc: 100.00%] [G loss: 8.006344] time: 0:14:51.839810\n",
      "[Epoch 0/30] [Batch 396/816] [D loss: 0.462004, acc:  62%, op_acc: 95.00%] [G loss: 10.256910] time: 0:14:54.153530\n",
      "[Epoch 0/30] [Batch 397/816] [D loss: 0.289884, acc:  47%, op_acc: 100.00%] [G loss: 9.324545] time: 0:14:56.381810\n",
      "[Epoch 0/30] [Batch 398/816] [D loss: 0.290543, acc:  50%, op_acc: 100.00%] [G loss: 9.669606] time: 0:14:58.620059\n",
      "[Epoch 0/30] [Batch 399/816] [D loss: 0.257521, acc:  64%, op_acc: 100.00%] [G loss: 9.850202] time: 0:15:00.844256\n",
      "[Epoch 0/30] [Batch 400/816] [D loss: 0.572058, acc:  36%, op_acc: 95.00%] [G loss: 9.552576] time: 0:15:03.077902\n",
      "SAVE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_400-T2-T1.png\n",
      "[Epoch 0/30] [Batch 401/816] [D loss: 0.616015, acc:  56%, op_acc: 95.00%] [G loss: 10.476714] time: 0:15:05.540525\n",
      "[Epoch 0/30] [Batch 402/816] [D loss: 0.942360, acc:  38%, op_acc: 85.00%] [G loss: 8.056991] time: 0:15:07.664667\n",
      "[Epoch 0/30] [Batch 403/816] [D loss: 1.360147, acc:  54%, op_acc: 95.00%] [G loss: 7.967783] time: 0:15:09.886654\n",
      "[Epoch 0/30] [Batch 404/816] [D loss: 0.801146, acc:  41%, op_acc: 90.00%] [G loss: 8.305198] time: 0:15:12.049913\n",
      "[Epoch 0/30] [Batch 405/816] [D loss: 0.255894, acc:  61%, op_acc: 100.00%] [G loss: 9.796309] time: 0:15:14.259115\n",
      "[Epoch 0/30] [Batch 406/816] [D loss: 0.643545, acc:  51%, op_acc: 90.00%] [G loss: 8.210447] time: 0:15:16.490843\n",
      "[Epoch 0/30] [Batch 407/816] [D loss: 0.296104, acc:  53%, op_acc: 100.00%] [G loss: 9.694576] time: 0:15:18.757458\n",
      "[Epoch 0/30] [Batch 408/816] [D loss: 0.371054, acc:  48%, op_acc: 95.00%] [G loss: 8.035576] time: 0:15:21.039378\n",
      "[Epoch 0/30] [Batch 409/816] [D loss: 0.582247, acc:  55%, op_acc: 90.00%] [G loss: 9.600965] time: 0:15:23.262128\n",
      "[Epoch 0/30] [Batch 410/816] [D loss: 0.272141, acc:  53%, op_acc: 100.00%] [G loss: 11.452724] time: 0:15:25.487381\n",
      "[Epoch 0/30] [Batch 411/816] [D loss: 0.767817, acc:  44%, op_acc: 90.00%] [G loss: 8.115959] time: 0:15:27.717450\n",
      "[Epoch 0/30] [Batch 412/816] [D loss: 0.263806, acc:  51%, op_acc: 100.00%] [G loss: 8.265312] time: 0:15:29.946322\n",
      "[Epoch 0/30] [Batch 413/816] [D loss: 1.400899, acc:  48%, op_acc: 90.00%] [G loss: 11.626575] time: 0:15:32.110435\n",
      "[Epoch 0/30] [Batch 414/816] [D loss: 0.260988, acc:  61%, op_acc: 100.00%] [G loss: 10.225884] time: 0:15:34.314779\n",
      "[Epoch 0/30] [Batch 415/816] [D loss: 0.473262, acc:  46%, op_acc: 90.00%] [G loss: 7.706312] time: 0:15:36.545980\n",
      "[Epoch 0/30] [Batch 416/816] [D loss: 0.430382, acc:  59%, op_acc: 95.00%] [G loss: 9.719924] time: 0:15:38.681705\n",
      "[Epoch 0/30] [Batch 417/816] [D loss: 1.473348, acc:  48%, op_acc: 90.00%] [G loss: 9.456779] time: 0:15:40.875234\n",
      "[Epoch 0/30] [Batch 418/816] [D loss: 1.137063, acc:  52%, op_acc: 95.00%] [G loss: 9.272044] time: 0:15:43.092360\n",
      "[Epoch 0/30] [Batch 419/816] [D loss: 0.269476, acc:  50%, op_acc: 100.00%] [G loss: 9.345569] time: 0:15:45.323269\n",
      "[Epoch 0/30] [Batch 420/816] [D loss: 2.008502, acc:  50%, op_acc: 75.00%] [G loss: 10.213598] time: 0:15:47.470740\n",
      "[Epoch 0/30] [Batch 421/816] [D loss: 6.427061, acc:  51%, op_acc: 80.00%] [G loss: 9.880917] time: 0:15:49.725037\n",
      "[Epoch 0/30] [Batch 422/816] [D loss: 2.370299, acc:  55%, op_acc: 85.00%] [G loss: 11.512613] time: 0:15:51.968170\n",
      "[Epoch 0/30] [Batch 423/816] [D loss: 0.615701, acc:  56%, op_acc: 95.00%] [G loss: 9.664222] time: 0:15:54.248086\n",
      "[Epoch 0/30] [Batch 424/816] [D loss: 1.142771, acc:  57%, op_acc: 95.00%] [G loss: 8.595879] time: 0:15:56.463728\n",
      "[Epoch 0/30] [Batch 425/816] [D loss: 3.112243, acc:  51%, op_acc: 75.00%] [G loss: 7.482817] time: 0:15:58.576346\n",
      "[Epoch 0/30] [Batch 426/816] [D loss: 1.843241, acc:  60%, op_acc: 85.00%] [G loss: 10.127363] time: 0:16:00.872605\n",
      "[Epoch 0/30] [Batch 427/816] [D loss: 1.829754, acc:  42%, op_acc: 90.00%] [G loss: 7.771491] time: 0:16:03.144934\n",
      "[Epoch 0/30] [Batch 428/816] [D loss: 1.104938, acc:  54%, op_acc: 95.00%] [G loss: 10.433628] time: 0:16:05.510405\n",
      "[Epoch 0/30] [Batch 429/816] [D loss: 0.270329, acc:  54%, op_acc: 100.00%] [G loss: 9.833259] time: 0:16:07.691576\n",
      "[Epoch 0/30] [Batch 430/816] [D loss: 0.500820, acc:  55%, op_acc: 95.00%] [G loss: 9.920715] time: 0:16:09.795354\n",
      "[Epoch 0/30] [Batch 431/816] [D loss: 0.480274, acc:  54%, op_acc: 95.00%] [G loss: 6.612728] time: 0:16:11.993993\n",
      "[Epoch 0/30] [Batch 432/816] [D loss: 0.395073, acc:  58%, op_acc: 95.00%] [G loss: 9.785458] time: 0:16:14.308260\n",
      "[Epoch 0/30] [Batch 433/816] [D loss: 0.286319, acc:  38%, op_acc: 100.00%] [G loss: 7.950092] time: 0:16:16.568972\n",
      "[Epoch 0/30] [Batch 434/816] [D loss: 0.269983, acc:  52%, op_acc: 100.00%] [G loss: 7.986714] time: 0:16:18.797169\n",
      "[Epoch 0/30] [Batch 435/816] [D loss: 1.256202, acc:  49%, op_acc: 90.00%] [G loss: 8.379652] time: 0:16:21.043134\n",
      "[Epoch 0/30] [Batch 436/816] [D loss: 1.529395, acc:  51%, op_acc: 95.00%] [G loss: 10.347578] time: 0:16:23.202618\n",
      "[Epoch 0/30] [Batch 437/816] [D loss: 0.293870, acc:  49%, op_acc: 100.00%] [G loss: 7.589154] time: 0:16:25.463816\n",
      "[Epoch 0/30] [Batch 438/816] [D loss: 1.799409, acc:  53%, op_acc: 85.00%] [G loss: 8.719575] time: 0:16:27.604073\n",
      "[Epoch 0/30] [Batch 439/816] [D loss: 0.762178, acc:  53%, op_acc: 95.00%] [G loss: 8.621659] time: 0:16:29.925306\n",
      "[Epoch 0/30] [Batch 440/816] [D loss: 1.245541, acc:  59%, op_acc: 95.00%] [G loss: 8.102042] time: 0:16:32.134501\n",
      "[Epoch 0/30] [Batch 441/816] [D loss: 2.587762, acc:  54%, op_acc: 90.00%] [G loss: 8.322807] time: 0:16:34.364048\n",
      "[Epoch 0/30] [Batch 442/816] [D loss: 0.415121, acc:  38%, op_acc: 90.00%] [G loss: 9.269904] time: 0:16:36.573069\n",
      "[Epoch 0/30] [Batch 443/816] [D loss: 1.036092, acc:  46%, op_acc: 95.00%] [G loss: 9.645541] time: 0:16:38.783229\n",
      "[Epoch 0/30] [Batch 444/816] [D loss: 2.105824, acc:  42%, op_acc: 85.00%] [G loss: 9.171263] time: 0:16:40.918615\n",
      "[Epoch 0/30] [Batch 445/816] [D loss: 1.339341, acc:  50%, op_acc: 90.00%] [G loss: 8.443254] time: 0:16:43.158135\n",
      "[Epoch 0/30] [Batch 446/816] [D loss: 1.844138, acc:  62%, op_acc: 90.00%] [G loss: 10.049750] time: 0:16:45.432683\n",
      "[Epoch 0/30] [Batch 447/816] [D loss: 0.304841, acc:  51%, op_acc: 100.00%] [G loss: 9.770439] time: 0:16:47.677572\n",
      "[Epoch 0/30] [Batch 448/816] [D loss: 2.108145, acc:  50%, op_acc: 95.00%] [G loss: 8.700192] time: 0:16:49.916640\n",
      "[Epoch 0/30] [Batch 449/816] [D loss: 0.284374, acc:  56%, op_acc: 100.00%] [G loss: 9.192243] time: 0:16:52.062324\n",
      "[Epoch 0/30] [Batch 450/816] [D loss: 1.159455, acc:  44%, op_acc: 95.00%] [G loss: 8.570008] time: 0:16:54.256018\n",
      "[Epoch 0/30] [Batch 451/816] [D loss: 0.490187, acc:  45%, op_acc: 95.00%] [G loss: 8.837044] time: 0:16:56.446515\n",
      "[Epoch 0/30] [Batch 452/816] [D loss: 0.280822, acc:  46%, op_acc: 100.00%] [G loss: 7.371957] time: 0:16:58.616117\n",
      "[Epoch 0/30] [Batch 453/816] [D loss: 0.308786, acc:  55%, op_acc: 100.00%] [G loss: 7.993625] time: 0:17:00.751902\n",
      "[Epoch 0/30] [Batch 454/816] [D loss: 1.561439, acc:  65%, op_acc: 85.00%] [G loss: 8.153402] time: 0:17:02.905483\n",
      "[Epoch 0/30] [Batch 455/816] [D loss: 0.266724, acc:  51%, op_acc: 100.00%] [G loss: 7.252801] time: 0:17:05.074074\n",
      "[Epoch 0/30] [Batch 456/816] [D loss: 0.288176, acc:  50%, op_acc: 100.00%] [G loss: 11.960748] time: 0:17:07.192087\n",
      "[Epoch 0/30] [Batch 457/816] [D loss: 1.054468, acc:  53%, op_acc: 90.00%] [G loss: 12.678718] time: 0:17:09.396923\n",
      "[Epoch 0/30] [Batch 458/816] [D loss: 0.282049, acc:  52%, op_acc: 100.00%] [G loss: 10.380813] time: 0:17:11.580410\n",
      "[Epoch 0/30] [Batch 459/816] [D loss: 0.277631, acc:  50%, op_acc: 100.00%] [G loss: 8.854964] time: 0:17:13.801718\n",
      "[Epoch 0/30] [Batch 460/816] [D loss: 0.436462, acc:  54%, op_acc: 95.00%] [G loss: 12.503739] time: 0:17:16.035258\n",
      "[Epoch 0/30] [Batch 461/816] [D loss: 0.479054, acc:  65%, op_acc: 95.00%] [G loss: 10.196044] time: 0:17:18.291309\n",
      "[Epoch 0/30] [Batch 462/816] [D loss: 0.268626, acc:  60%, op_acc: 100.00%] [G loss: 9.979101] time: 0:17:20.428846\n",
      "[Epoch 0/30] [Batch 463/816] [D loss: 0.270649, acc:  50%, op_acc: 100.00%] [G loss: 9.351039] time: 0:17:22.621710\n",
      "[Epoch 0/30] [Batch 464/816] [D loss: 2.040616, acc:  60%, op_acc: 90.00%] [G loss: 9.128603] time: 0:17:24.842270\n",
      "[Epoch 0/30] [Batch 465/816] [D loss: 0.535927, acc:  57%, op_acc: 90.00%] [G loss: 10.502853] time: 0:17:27.027414\n",
      "[Epoch 0/30] [Batch 466/816] [D loss: 0.923365, acc:  60%, op_acc: 95.00%] [G loss: 10.420971] time: 0:17:29.238554\n",
      "[Epoch 0/30] [Batch 467/816] [D loss: 0.794954, acc:  53%, op_acc: 95.00%] [G loss: 10.146425] time: 0:17:31.519133\n",
      "[Epoch 0/30] [Batch 468/816] [D loss: 0.338541, acc:  64%, op_acc: 95.00%] [G loss: 11.311988] time: 0:17:33.620379\n",
      "[Epoch 0/30] [Batch 469/816] [D loss: 0.698358, acc:  43%, op_acc: 90.00%] [G loss: 8.595682] time: 0:17:35.852320\n",
      "[Epoch 0/30] [Batch 470/816] [D loss: 0.254876, acc:  57%, op_acc: 100.00%] [G loss: 9.315945] time: 0:17:38.007186\n",
      "[Epoch 0/30] [Batch 471/816] [D loss: 0.259269, acc:  51%, op_acc: 100.00%] [G loss: 9.843342] time: 0:17:40.230599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/30] [Batch 472/816] [D loss: 0.253755, acc:  58%, op_acc: 100.00%] [G loss: 10.472657] time: 0:17:42.448140\n",
      "[Epoch 0/30] [Batch 473/816] [D loss: 1.208827, acc:  50%, op_acc: 90.00%] [G loss: 11.047375] time: 0:17:44.653624\n",
      "[Epoch 0/30] [Batch 474/816] [D loss: 2.565995, acc:  64%, op_acc: 85.00%] [G loss: 10.696592] time: 0:17:46.837622\n",
      "[Epoch 0/30] [Batch 475/816] [D loss: 0.259197, acc:  61%, op_acc: 100.00%] [G loss: 10.515176] time: 0:17:49.108535\n",
      "[Epoch 0/30] [Batch 476/816] [D loss: 0.288995, acc:  59%, op_acc: 100.00%] [G loss: 9.898726] time: 0:17:51.325070\n",
      "[Epoch 0/30] [Batch 477/816] [D loss: 0.272010, acc:  62%, op_acc: 100.00%] [G loss: 10.190892] time: 0:17:53.562434\n",
      "[Epoch 0/30] [Batch 478/816] [D loss: 0.568673, acc:  58%, op_acc: 85.00%] [G loss: 6.509377] time: 0:17:55.785997\n",
      "[Epoch 0/30] [Batch 479/816] [D loss: 2.058367, acc:  65%, op_acc: 80.00%] [G loss: 9.404808] time: 0:17:57.935540\n",
      "[Epoch 0/30] [Batch 480/816] [D loss: 0.422117, acc:  67%, op_acc: 90.00%] [G loss: 23.845022] time: 0:18:00.083846\n",
      "[Epoch 0/30] [Batch 481/816] [D loss: 5.437539, acc:  35%, op_acc: 75.00%] [G loss: 17.507233] time: 0:18:02.297681\n",
      "[Epoch 0/30] [Batch 482/816] [D loss: 1.820448, acc:  45%, op_acc: 90.00%] [G loss: 15.688434] time: 0:18:04.498130\n",
      "[Epoch 0/30] [Batch 483/816] [D loss: 3.312397, acc:  57%, op_acc: 90.00%] [G loss: 12.105884] time: 0:18:06.683522\n",
      "[Epoch 0/30] [Batch 484/816] [D loss: 5.628983, acc:  65%, op_acc: 85.00%] [G loss: 8.786845] time: 0:18:08.899636\n",
      "[Epoch 0/30] [Batch 485/816] [D loss: 6.696272, acc:  70%, op_acc: 80.00%] [G loss: 9.172873] time: 0:18:11.139430\n",
      "[Epoch 0/30] [Batch 486/816] [D loss: 0.360566, acc:  50%, op_acc: 100.00%] [G loss: 21.497940] time: 0:18:13.331334\n",
      "[Epoch 0/30] [Batch 487/816] [D loss: 4.854109, acc:  48%, op_acc: 80.00%] [G loss: 16.791544] time: 0:18:15.558085\n",
      "[Epoch 0/30] [Batch 488/816] [D loss: 3.632263, acc:  51%, op_acc: 80.00%] [G loss: 10.629958] time: 0:18:17.708443\n",
      "[Epoch 0/30] [Batch 489/816] [D loss: 1.045136, acc:  43%, op_acc: 95.00%] [G loss: 10.079114] time: 0:18:20.051424\n",
      "[Epoch 0/30] [Batch 490/816] [D loss: 3.347374, acc:  55%, op_acc: 70.00%] [G loss: 16.453981] time: 0:18:22.179271\n",
      "[Epoch 0/30] [Batch 491/816] [D loss: 2.220894, acc:  60%, op_acc: 95.00%] [G loss: 13.226662] time: 0:18:24.562800\n",
      "[Epoch 0/30] [Batch 492/816] [D loss: 0.789707, acc:  39%, op_acc: 100.00%] [G loss: 11.624982] time: 0:18:26.796179\n",
      "[Epoch 0/30] [Batch 493/816] [D loss: 1.602442, acc:  38%, op_acc: 85.00%] [G loss: 10.816126] time: 0:18:29.240080\n",
      "[Epoch 0/30] [Batch 494/816] [D loss: 0.487719, acc:  41%, op_acc: 100.00%] [G loss: 12.493000] time: 0:18:31.384251\n",
      "[Epoch 0/30] [Batch 495/816] [D loss: 0.470019, acc:  60%, op_acc: 100.00%] [G loss: 10.447750] time: 0:18:33.497642\n",
      "[Epoch 0/30] [Batch 496/816] [D loss: 4.444300, acc:  61%, op_acc: 90.00%] [G loss: 13.639302] time: 0:18:35.722397\n",
      "[Epoch 0/30] [Batch 497/816] [D loss: 3.154983, acc:  57%, op_acc: 90.00%] [G loss: 11.251535] time: 0:18:37.945875\n",
      "[Epoch 0/30] [Batch 498/816] [D loss: 1.210580, acc:  54%, op_acc: 95.00%] [G loss: 10.804121] time: 0:18:40.091155\n",
      "[Epoch 0/30] [Batch 499/816] [D loss: 1.406461, acc:  60%, op_acc: 90.00%] [G loss: 12.630586] time: 0:18:42.345379\n",
      "[Epoch 0/30] [Batch 500/816] [D loss: 0.280588, acc:  53%, op_acc: 100.00%] [G loss: 11.211356] time: 0:18:44.533190\n",
      "[Epoch 0/30] [Batch 501/816] [D loss: 2.394555, acc:  52%, op_acc: 85.00%] [G loss: 9.401871] time: 0:18:46.710719\n",
      "[Epoch 0/30] [Batch 502/816] [D loss: 2.777395, acc:  58%, op_acc: 90.00%] [G loss: 9.502980] time: 0:18:48.963617\n",
      "[Epoch 0/30] [Batch 503/816] [D loss: 2.672081, acc:  59%, op_acc: 90.00%] [G loss: 10.558420] time: 0:18:51.205435\n",
      "[Epoch 0/30] [Batch 504/816] [D loss: 0.264536, acc:  53%, op_acc: 100.00%] [G loss: 8.816895] time: 0:18:53.400841\n",
      "[Epoch 0/30] [Batch 505/816] [D loss: 0.453612, acc:  60%, op_acc: 95.00%] [G loss: 11.103409] time: 0:18:55.617567\n",
      "[Epoch 0/30] [Batch 506/816] [D loss: 3.426433, acc:  50%, op_acc: 95.00%] [G loss: 8.319701] time: 0:18:57.859322\n",
      "[Epoch 0/30] [Batch 507/816] [D loss: 0.283548, acc:  44%, op_acc: 100.00%] [G loss: 8.679768] time: 0:19:00.168398\n",
      "[Epoch 0/30] [Batch 508/816] [D loss: 4.292569, acc:  58%, op_acc: 85.00%] [G loss: 10.996325] time: 0:19:02.297682\n",
      "[Epoch 0/30] [Batch 509/816] [D loss: 0.627618, acc:  62%, op_acc: 95.00%] [G loss: 9.814409] time: 0:19:04.482471\n",
      "[Epoch 0/30] [Batch 510/816] [D loss: 1.534646, acc:  63%, op_acc: 90.00%] [G loss: 10.128191] time: 0:19:06.717036\n",
      "[Epoch 0/30] [Batch 511/816] [D loss: 1.372563, acc:  62%, op_acc: 95.00%] [G loss: 11.126606] time: 0:19:08.881840\n",
      "[Epoch 0/30] [Batch 512/816] [D loss: 1.802085, acc:  58%, op_acc: 90.00%] [G loss: 9.419657] time: 0:19:11.147744\n",
      "[Epoch 0/30] [Batch 513/816] [D loss: 0.286868, acc:  59%, op_acc: 100.00%] [G loss: 10.433300] time: 0:19:13.394993\n",
      "[Epoch 0/30] [Batch 514/816] [D loss: 3.612846, acc:  49%, op_acc: 90.00%] [G loss: 11.395244] time: 0:19:15.709496\n",
      "[Epoch 0/30] [Batch 515/816] [D loss: 4.358502, acc:  62%, op_acc: 85.00%] [G loss: 8.400858] time: 0:19:17.846349\n",
      "[Epoch 0/30] [Batch 516/816] [D loss: 1.336324, acc:  56%, op_acc: 90.00%] [G loss: 11.538935] time: 0:19:20.128071\n",
      "[Epoch 0/30] [Batch 517/816] [D loss: 1.578989, acc:  64%, op_acc: 90.00%] [G loss: 8.923754] time: 0:19:22.353596\n",
      "[Epoch 0/30] [Batch 518/816] [D loss: 0.762999, acc:  56%, op_acc: 90.00%] [G loss: 9.043589] time: 0:19:24.552912\n",
      "[Epoch 0/30] [Batch 519/816] [D loss: 2.345586, acc:  54%, op_acc: 90.00%] [G loss: 7.807644] time: 0:19:26.739881\n",
      "[Epoch 0/30] [Batch 520/816] [D loss: 0.305445, acc:  47%, op_acc: 100.00%] [G loss: 7.950228] time: 0:19:28.913415\n",
      "[Epoch 0/30] [Batch 521/816] [D loss: 0.462220, acc:  50%, op_acc: 95.00%] [G loss: 9.357186] time: 0:19:31.104357\n",
      "[Epoch 0/30] [Batch 522/816] [D loss: 0.465042, acc:  54%, op_acc: 90.00%] [G loss: 9.123123] time: 0:19:33.358756\n",
      "[Epoch 0/30] [Batch 523/816] [D loss: 0.374067, acc:  60%, op_acc: 95.00%] [G loss: 9.047592] time: 0:19:35.550964\n",
      "[Epoch 0/30] [Batch 524/816] [D loss: 0.944523, acc:  47%, op_acc: 90.00%] [G loss: 9.464421] time: 0:19:37.709201\n",
      "[Epoch 0/30] [Batch 525/816] [D loss: 0.520457, acc:  56%, op_acc: 95.00%] [G loss: 10.308107] time: 0:19:39.920439\n",
      "[Epoch 0/30] [Batch 526/816] [D loss: 0.280402, acc:  48%, op_acc: 100.00%] [G loss: 10.442534] time: 0:19:42.053808\n",
      "[Epoch 0/30] [Batch 527/816] [D loss: 1.123809, acc:  57%, op_acc: 95.00%] [G loss: 11.063811] time: 0:19:44.279837\n",
      "[Epoch 0/30] [Batch 528/816] [D loss: 0.712501, acc:  49%, op_acc: 95.00%] [G loss: 9.406298] time: 0:19:46.505236\n",
      "[Epoch 0/30] [Batch 529/816] [D loss: 2.319829, acc:  65%, op_acc: 95.00%] [G loss: 7.425888] time: 0:19:48.620077\n",
      "[Epoch 0/30] [Batch 530/816] [D loss: 0.281376, acc:  55%, op_acc: 100.00%] [G loss: 11.297752] time: 0:19:50.707812\n",
      "[Epoch 0/30] [Batch 531/816] [D loss: 0.271724, acc:  61%, op_acc: 100.00%] [G loss: 13.114717] time: 0:19:52.972000\n",
      "[Epoch 0/30] [Batch 532/816] [D loss: 0.225722, acc:  66%, op_acc: 100.00%] [G loss: 11.431563] time: 0:19:55.160600\n",
      "[Epoch 0/30] [Batch 533/816] [D loss: 0.254115, acc:  58%, op_acc: 100.00%] [G loss: 10.405672] time: 0:19:57.287090\n",
      "[Epoch 0/30] [Batch 534/816] [D loss: 0.362924, acc:  58%, op_acc: 95.00%] [G loss: 9.617883] time: 0:19:59.500999\n",
      "[Epoch 0/30] [Batch 535/816] [D loss: 0.567098, acc:  54%, op_acc: 90.00%] [G loss: 11.936721] time: 0:20:01.770211\n",
      "[Epoch 0/30] [Batch 536/816] [D loss: 1.115211, acc:  50%, op_acc: 90.00%] [G loss: 8.695177] time: 0:20:04.133938\n",
      "[Epoch 0/30] [Batch 537/816] [D loss: 4.318470, acc:  57%, op_acc: 85.00%] [G loss: 10.828544] time: 0:20:06.345011\n",
      "[Epoch 0/30] [Batch 538/816] [D loss: 0.288121, acc:  52%, op_acc: 100.00%] [G loss: 9.982644] time: 0:20:08.482019\n",
      "[Epoch 0/30] [Batch 539/816] [D loss: 1.259436, acc:  52%, op_acc: 90.00%] [G loss: 9.958944] time: 0:20:10.844625\n",
      "[Epoch 0/30] [Batch 540/816] [D loss: 0.772437, acc:  56%, op_acc: 95.00%] [G loss: 9.331459] time: 0:20:13.011850\n",
      "[Epoch 0/30] [Batch 541/816] [D loss: 1.298985, acc:  58%, op_acc: 95.00%] [G loss: 11.142085] time: 0:20:15.232423\n",
      "[Epoch 0/30] [Batch 542/816] [D loss: 0.570281, acc:  55%, op_acc: 90.00%] [G loss: 12.887138] time: 0:20:17.469868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/30] [Batch 543/816] [D loss: 4.281349, acc:  55%, op_acc: 80.00%] [G loss: 9.921275] time: 0:20:19.617678\n",
      "[Epoch 0/30] [Batch 544/816] [D loss: 2.118432, acc:  45%, op_acc: 95.00%] [G loss: 9.033844] time: 0:20:21.796169\n",
      "[Epoch 0/30] [Batch 545/816] [D loss: 0.367212, acc:  59%, op_acc: 95.00%] [G loss: 9.035091] time: 0:20:24.151537\n",
      "[Epoch 0/30] [Batch 546/816] [D loss: 1.068109, acc:  54%, op_acc: 90.00%] [G loss: 9.416935] time: 0:20:26.258580\n",
      "[Epoch 0/30] [Batch 547/816] [D loss: 0.289604, acc:  52%, op_acc: 100.00%] [G loss: 15.082640] time: 0:20:28.397425\n",
      "[Epoch 0/30] [Batch 548/816] [D loss: 0.237582, acc:  63%, op_acc: 100.00%] [G loss: 12.716364] time: 0:20:30.626508\n",
      "[Epoch 0/30] [Batch 549/816] [D loss: 1.381597, acc:  58%, op_acc: 90.00%] [G loss: 14.284721] time: 0:20:32.795234\n",
      "[Epoch 0/30] [Batch 550/816] [D loss: 0.343884, acc:  58%, op_acc: 95.00%] [G loss: 10.867332] time: 0:20:35.173329\n",
      "[Epoch 0/30] [Batch 551/816] [D loss: 2.218168, acc:  57%, op_acc: 90.00%] [G loss: 9.678343] time: 0:20:37.392652\n",
      "[Epoch 0/30] [Batch 552/816] [D loss: 0.342050, acc:  48%, op_acc: 100.00%] [G loss: 10.954995] time: 0:20:39.686859\n",
      "[Epoch 0/30] [Batch 553/816] [D loss: 1.677556, acc:  55%, op_acc: 95.00%] [G loss: 13.330317] time: 0:20:41.945564\n",
      "[Epoch 0/30] [Batch 554/816] [D loss: 1.852834, acc:  63%, op_acc: 90.00%] [G loss: 9.842748] time: 0:20:44.174881\n",
      "[Epoch 0/30] [Batch 555/816] [D loss: 0.547036, acc:  60%, op_acc: 95.00%] [G loss: 11.099094] time: 0:20:46.476450\n",
      "[Epoch 0/30] [Batch 556/816] [D loss: 0.273127, acc:  58%, op_acc: 100.00%] [G loss: 9.305120] time: 0:20:48.954493\n",
      "[Epoch 0/30] [Batch 557/816] [D loss: 0.253068, acc:  62%, op_acc: 100.00%] [G loss: 8.676844] time: 0:20:51.123284\n",
      "[Epoch 0/30] [Batch 558/816] [D loss: 0.246503, acc:  61%, op_acc: 100.00%] [G loss: 10.305637] time: 0:20:53.267648\n",
      "[Epoch 0/30] [Batch 559/816] [D loss: 1.576538, acc:  59%, op_acc: 85.00%] [G loss: 8.838715] time: 0:20:55.490929\n",
      "[Epoch 0/30] [Batch 560/816] [D loss: 1.601119, acc:  58%, op_acc: 85.00%] [G loss: 8.164537] time: 0:20:57.744763\n",
      "[Epoch 0/30] [Batch 561/816] [D loss: 0.250875, acc:  61%, op_acc: 100.00%] [G loss: 11.469314] time: 0:21:00.009170\n",
      "[Epoch 0/30] [Batch 562/816] [D loss: 1.049202, acc:  49%, op_acc: 90.00%] [G loss: 11.238122] time: 0:21:02.224888\n",
      "[Epoch 0/30] [Batch 563/816] [D loss: 0.255542, acc:  59%, op_acc: 100.00%] [G loss: 11.388313] time: 0:21:04.458770\n",
      "[Epoch 0/30] [Batch 564/816] [D loss: 0.273450, acc:  57%, op_acc: 100.00%] [G loss: 9.079132] time: 0:21:06.661468\n",
      "[Epoch 0/30] [Batch 565/816] [D loss: 0.280544, acc:  54%, op_acc: 100.00%] [G loss: 9.610563] time: 0:21:08.897829\n",
      "[Epoch 0/30] [Batch 566/816] [D loss: 0.905693, acc:  53%, op_acc: 95.00%] [G loss: 10.021631] time: 0:21:11.088794\n",
      "[Epoch 0/30] [Batch 567/816] [D loss: 0.544836, acc:  54%, op_acc: 95.00%] [G loss: 9.214735] time: 0:21:13.220918\n",
      "[Epoch 0/30] [Batch 568/816] [D loss: 0.453816, acc:  55%, op_acc: 95.00%] [G loss: 9.893590] time: 0:21:15.660964\n",
      "[Epoch 0/30] [Batch 569/816] [D loss: 0.357490, acc:  54%, op_acc: 95.00%] [G loss: 11.845785] time: 0:21:17.882192\n",
      "[Epoch 0/30] [Batch 570/816] [D loss: 0.294218, acc:  54%, op_acc: 100.00%] [G loss: 13.546775] time: 0:21:20.277746\n",
      "[Epoch 0/30] [Batch 571/816] [D loss: 0.250563, acc:  64%, op_acc: 100.00%] [G loss: 12.925063] time: 0:21:22.451396\n",
      "[Epoch 0/30] [Batch 572/816] [D loss: 0.523115, acc:  63%, op_acc: 95.00%] [G loss: 10.379410] time: 0:21:24.672438\n",
      "[Epoch 0/30] [Batch 573/816] [D loss: 0.265869, acc:  49%, op_acc: 100.00%] [G loss: 11.284574] time: 0:21:26.947252\n",
      "[Epoch 0/30] [Batch 574/816] [D loss: 0.260916, acc:  57%, op_acc: 100.00%] [G loss: 9.704798] time: 0:21:29.091692\n",
      "[Epoch 0/30] [Batch 575/816] [D loss: 0.836160, acc:  56%, op_acc: 95.00%] [G loss: 10.586980] time: 0:21:31.268335\n",
      "[Epoch 0/30] [Batch 576/816] [D loss: 0.296631, acc:  62%, op_acc: 100.00%] [G loss: 10.485740] time: 0:21:33.447709\n",
      "[Epoch 0/30] [Batch 577/816] [D loss: 0.250902, acc:  60%, op_acc: 100.00%] [G loss: 11.116571] time: 0:21:35.687542\n",
      "[Epoch 0/30] [Batch 578/816] [D loss: 0.272092, acc:  55%, op_acc: 100.00%] [G loss: 9.167546] time: 0:21:37.847799\n",
      "[Epoch 0/30] [Batch 579/816] [D loss: 0.248911, acc:  64%, op_acc: 100.00%] [G loss: 8.431260] time: 0:21:40.063149\n",
      "[Epoch 0/30] [Batch 580/816] [D loss: 0.533762, acc:  66%, op_acc: 95.00%] [G loss: 9.020000] time: 0:21:42.272471\n",
      "[Epoch 0/30] [Batch 581/816] [D loss: 0.272876, acc:  54%, op_acc: 100.00%] [G loss: 10.305475] time: 0:21:44.504797\n",
      "[Epoch 0/30] [Batch 582/816] [D loss: 0.348564, acc:  62%, op_acc: 95.00%] [G loss: 10.964276] time: 0:21:46.713587\n",
      "[Epoch 0/30] [Batch 583/816] [D loss: 0.885236, acc:  65%, op_acc: 90.00%] [G loss: 10.047879] time: 0:21:48.887000\n",
      "[Epoch 0/30] [Batch 584/816] [D loss: 0.276536, acc:  49%, op_acc: 100.00%] [G loss: 8.834105] time: 0:21:51.089076\n",
      "[Epoch 0/30] [Batch 585/816] [D loss: 2.798143, acc:  53%, op_acc: 95.00%] [G loss: 9.025583] time: 0:21:53.327469\n",
      "[Epoch 0/30] [Batch 586/816] [D loss: 0.399060, acc:  45%, op_acc: 95.00%] [G loss: 7.890044] time: 0:21:55.553458\n",
      "[Epoch 0/30] [Batch 587/816] [D loss: 0.310197, acc:  58%, op_acc: 100.00%] [G loss: 11.430125] time: 0:21:57.832568\n",
      "[Epoch 0/30] [Batch 588/816] [D loss: 0.232981, acc:  61%, op_acc: 100.00%] [G loss: 9.940551] time: 0:22:00.025134\n",
      "[Epoch 0/30] [Batch 589/816] [D loss: 0.251571, acc:  62%, op_acc: 100.00%] [G loss: 8.932035] time: 0:22:02.160849\n",
      "[Epoch 0/30] [Batch 590/816] [D loss: 0.492103, acc:  50%, op_acc: 95.00%] [G loss: 7.756707] time: 0:22:04.417987\n",
      "[Epoch 0/30] [Batch 591/816] [D loss: 0.253242, acc:  60%, op_acc: 100.00%] [G loss: 8.651588] time: 0:22:06.674022\n",
      "[Epoch 0/30] [Batch 592/816] [D loss: 0.287547, acc:  53%, op_acc: 100.00%] [G loss: 10.379904] time: 0:22:09.253449\n",
      "[Epoch 0/30] [Batch 593/816] [D loss: 0.269230, acc:  49%, op_acc: 100.00%] [G loss: 10.901981] time: 0:22:11.474369\n",
      "[Epoch 0/30] [Batch 594/816] [D loss: 1.334230, acc:  39%, op_acc: 85.00%] [G loss: 18.084017] time: 0:22:13.577445\n",
      "[Epoch 0/30] [Batch 595/816] [D loss: 3.292377, acc:  53%, op_acc: 90.00%] [G loss: 13.866616] time: 0:22:15.769952\n",
      "[Epoch 0/30] [Batch 596/816] [D loss: 0.316181, acc:  62%, op_acc: 100.00%] [G loss: 11.271779] time: 0:22:17.953544\n",
      "[Epoch 0/30] [Batch 597/816] [D loss: 0.262366, acc:  58%, op_acc: 100.00%] [G loss: 9.224343] time: 0:22:20.150791\n",
      "[Epoch 0/30] [Batch 598/816] [D loss: 0.270038, acc:  44%, op_acc: 100.00%] [G loss: 10.162512] time: 0:22:22.309960\n",
      "[Epoch 0/30] [Batch 599/816] [D loss: 0.636714, acc:  61%, op_acc: 95.00%] [G loss: 11.549212] time: 0:22:24.549706\n",
      "[Epoch 0/30] [Batch 600/816] [D loss: 0.695935, acc:  58%, op_acc: 90.00%] [G loss: 10.515218] time: 0:22:26.780313\n",
      "SAVE\n",
      "0_600-T1-PD.png\n",
      "[Epoch 0/30] [Batch 601/816] [D loss: 0.262607, acc:  52%, op_acc: 100.00%] [G loss: 9.029902] time: 0:22:29.110561\n",
      "[Epoch 0/30] [Batch 602/816] [D loss: 1.415550, acc:  49%, op_acc: 85.00%] [G loss: 8.629413] time: 0:22:31.549148\n",
      "[Epoch 0/30] [Batch 603/816] [D loss: 0.325854, acc:  53%, op_acc: 95.00%] [G loss: 9.997308] time: 0:22:33.841644\n",
      "[Epoch 0/30] [Batch 604/816] [D loss: 0.269226, acc:  57%, op_acc: 100.00%] [G loss: 10.591935] time: 0:22:36.051119\n",
      "[Epoch 0/30] [Batch 605/816] [D loss: 0.259387, acc:  54%, op_acc: 100.00%] [G loss: 8.083741] time: 0:22:38.307839\n",
      "[Epoch 0/30] [Batch 606/816] [D loss: 0.362029, acc:  58%, op_acc: 95.00%] [G loss: 10.481697] time: 0:22:40.502442\n",
      "[Epoch 0/30] [Batch 607/816] [D loss: 0.254434, acc:  56%, op_acc: 100.00%] [G loss: 8.923552] time: 0:22:42.826277\n",
      "[Epoch 0/30] [Batch 608/816] [D loss: 0.363996, acc:  50%, op_acc: 95.00%] [G loss: 9.872206] time: 0:22:45.054214\n",
      "[Epoch 0/30] [Batch 609/816] [D loss: 1.706693, acc:  56%, op_acc: 90.00%] [G loss: 9.700644] time: 0:22:47.170807\n",
      "[Epoch 0/30] [Batch 610/816] [D loss: 1.866019, acc:  53%, op_acc: 85.00%] [G loss: 7.472207] time: 0:22:49.632861\n",
      "[Epoch 0/30] [Batch 611/816] [D loss: 0.373770, acc:  46%, op_acc: 95.00%] [G loss: 11.056993] time: 0:22:51.774365\n",
      "[Epoch 0/30] [Batch 612/816] [D loss: 0.771153, acc:  51%, op_acc: 90.00%] [G loss: 10.602956] time: 0:22:54.376818\n",
      "[Epoch 0/30] [Batch 613/816] [D loss: 0.527622, acc:  60%, op_acc: 95.00%] [G loss: 8.315216] time: 0:22:56.602213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/30] [Batch 614/816] [D loss: 0.273736, acc:  54%, op_acc: 100.00%] [G loss: 8.713895] time: 0:22:58.761828\n",
      "[Epoch 0/30] [Batch 615/816] [D loss: 0.429960, acc:  58%, op_acc: 95.00%] [G loss: 8.166546] time: 0:23:01.166685\n",
      "[Epoch 0/30] [Batch 616/816] [D loss: 0.931251, acc:  65%, op_acc: 95.00%] [G loss: 11.419222] time: 0:23:03.280744\n",
      "[Epoch 0/30] [Batch 617/816] [D loss: 0.265453, acc:  54%, op_acc: 100.00%] [G loss: 9.164151] time: 0:23:05.896893\n",
      "[Epoch 0/30] [Batch 618/816] [D loss: 0.342124, acc:  68%, op_acc: 95.00%] [G loss: 10.356473] time: 0:23:08.067348\n",
      "[Epoch 0/30] [Batch 619/816] [D loss: 0.355004, acc:  52%, op_acc: 100.00%] [G loss: 10.140491] time: 0:23:10.203059\n",
      "[Epoch 0/30] [Batch 620/816] [D loss: 0.695666, acc:  65%, op_acc: 90.00%] [G loss: 12.930887] time: 0:23:12.418780\n",
      "[Epoch 0/30] [Batch 621/816] [D loss: 0.404334, acc:  36%, op_acc: 95.00%] [G loss: 11.272079] time: 0:23:14.529359\n",
      "[Epoch 0/30] [Batch 622/816] [D loss: 0.289875, acc:  53%, op_acc: 100.00%] [G loss: 11.995400] time: 0:23:16.648350\n",
      "[Epoch 0/30] [Batch 623/816] [D loss: 1.658831, acc:  52%, op_acc: 90.00%] [G loss: 11.178434] time: 0:23:18.810932\n",
      "[Epoch 0/30] [Batch 624/816] [D loss: 0.286977, acc:  57%, op_acc: 100.00%] [G loss: 10.149455] time: 0:23:20.992786\n",
      "[Epoch 0/30] [Batch 625/816] [D loss: 3.370561, acc:  56%, op_acc: 80.00%] [G loss: 8.714976] time: 0:23:23.327035\n",
      "[Epoch 0/30] [Batch 626/816] [D loss: 3.436653, acc:  46%, op_acc: 90.00%] [G loss: 11.261571] time: 0:23:25.530322\n",
      "[Epoch 0/30] [Batch 627/816] [D loss: 0.246612, acc:  60%, op_acc: 100.00%] [G loss: 9.952230] time: 0:23:27.701624\n",
      "[Epoch 0/30] [Batch 628/816] [D loss: 0.383680, acc:  53%, op_acc: 95.00%] [G loss: 9.596343] time: 0:23:29.875652\n",
      "[Epoch 0/30] [Batch 629/816] [D loss: 0.316880, acc:  59%, op_acc: 95.00%] [G loss: 9.286769] time: 0:23:32.101247\n",
      "[Epoch 0/30] [Batch 630/816] [D loss: 0.253777, acc:  60%, op_acc: 100.00%] [G loss: 8.905495] time: 0:23:34.419895\n",
      "[Epoch 0/30] [Batch 631/816] [D loss: 0.538813, acc:  51%, op_acc: 95.00%] [G loss: 9.844872] time: 0:23:36.562146\n",
      "[Epoch 0/30] [Batch 632/816] [D loss: 1.501793, acc:  59%, op_acc: 90.00%] [G loss: 9.533048] time: 0:23:38.776051\n",
      "[Epoch 0/30] [Batch 633/816] [D loss: 1.407780, acc:  59%, op_acc: 95.00%] [G loss: 10.164580] time: 0:23:41.005436\n",
      "[Epoch 0/30] [Batch 634/816] [D loss: 0.477321, acc:  55%, op_acc: 95.00%] [G loss: 12.193528] time: 0:23:43.110132\n",
      "[Epoch 0/30] [Batch 635/816] [D loss: 0.289017, acc:  52%, op_acc: 100.00%] [G loss: 8.350275] time: 0:23:45.309611\n",
      "[Epoch 0/30] [Batch 636/816] [D loss: 0.556492, acc:  64%, op_acc: 95.00%] [G loss: 9.321593] time: 0:23:47.519267\n",
      "[Epoch 0/30] [Batch 637/816] [D loss: 2.668998, acc:  56%, op_acc: 90.00%] [G loss: 8.190703] time: 0:23:49.674084\n",
      "[Epoch 0/30] [Batch 638/816] [D loss: 0.284877, acc:  53%, op_acc: 100.00%] [G loss: 13.884226] time: 0:23:51.821978\n",
      "[Epoch 0/30] [Batch 639/816] [D loss: 0.511080, acc:  59%, op_acc: 95.00%] [G loss: 9.267114] time: 0:23:54.206925\n",
      "[Epoch 0/30] [Batch 640/816] [D loss: 2.447544, acc:  60%, op_acc: 90.00%] [G loss: 12.114701] time: 0:23:56.374971\n",
      "[Epoch 0/30] [Batch 641/816] [D loss: 0.282206, acc:  53%, op_acc: 100.00%] [G loss: 10.399776] time: 0:23:58.529295\n",
      "[Epoch 0/30] [Batch 642/816] [D loss: 0.651715, acc:  51%, op_acc: 95.00%] [G loss: 9.654072] time: 0:24:00.754270\n",
      "[Epoch 0/30] [Batch 643/816] [D loss: 1.376930, acc:  52%, op_acc: 90.00%] [G loss: 8.873898] time: 0:24:02.993970\n",
      "[Epoch 0/30] [Batch 644/816] [D loss: 0.624345, acc:  62%, op_acc: 95.00%] [G loss: 9.966334] time: 0:24:05.222059\n",
      "[Epoch 0/30] [Batch 645/816] [D loss: 0.717300, acc:  50%, op_acc: 95.00%] [G loss: 9.334144] time: 0:24:07.446643\n",
      "[Epoch 0/30] [Batch 646/816] [D loss: 0.289781, acc:  49%, op_acc: 100.00%] [G loss: 10.102698] time: 0:24:09.593318\n",
      "[Epoch 0/30] [Batch 647/816] [D loss: 0.245311, acc:  59%, op_acc: 100.00%] [G loss: 10.322433] time: 0:24:11.768967\n",
      "[Epoch 0/30] [Batch 648/816] [D loss: 0.287329, acc:  54%, op_acc: 100.00%] [G loss: 10.152544] time: 0:24:14.103139\n",
      "[Epoch 0/30] [Batch 649/816] [D loss: 0.740309, acc:  62%, op_acc: 95.00%] [G loss: 10.074692] time: 0:24:16.327154\n",
      "[Epoch 0/30] [Batch 650/816] [D loss: 0.456776, acc:  50%, op_acc: 95.00%] [G loss: 9.579968] time: 0:24:18.492973\n",
      "[Epoch 0/30] [Batch 651/816] [D loss: 1.676908, acc:  59%, op_acc: 95.00%] [G loss: 11.415549] time: 0:24:20.710666\n",
      "[Epoch 0/30] [Batch 652/816] [D loss: 0.986761, acc:  54%, op_acc: 90.00%] [G loss: 7.859512] time: 0:24:22.916594\n",
      "[Epoch 0/30] [Batch 653/816] [D loss: 1.770660, acc:  55%, op_acc: 90.00%] [G loss: 11.556616] time: 0:24:25.136048\n",
      "[Epoch 0/30] [Batch 654/816] [D loss: 0.675840, acc:  53%, op_acc: 95.00%] [G loss: 8.596451] time: 0:24:27.367963\n",
      "[Epoch 0/30] [Batch 655/816] [D loss: 1.139261, acc:  41%, op_acc: 95.00%] [G loss: 9.847948] time: 0:24:29.563497\n",
      "[Epoch 0/30] [Batch 656/816] [D loss: 0.283721, acc:  59%, op_acc: 100.00%] [G loss: 10.562344] time: 0:24:31.834031\n",
      "[Epoch 0/30] [Batch 657/816] [D loss: 1.392117, acc:  52%, op_acc: 95.00%] [G loss: 9.765346] time: 0:24:34.164247\n",
      "[Epoch 0/30] [Batch 658/816] [D loss: 1.014958, acc:  60%, op_acc: 90.00%] [G loss: 8.212750] time: 0:24:36.385123\n",
      "[Epoch 0/30] [Batch 659/816] [D loss: 0.279555, acc:  65%, op_acc: 100.00%] [G loss: 9.867751] time: 0:24:38.623266\n",
      "[Epoch 0/30] [Batch 660/816] [D loss: 0.256001, acc:  60%, op_acc: 100.00%] [G loss: 9.810591] time: 0:24:40.886078\n",
      "[Epoch 0/30] [Batch 661/816] [D loss: 4.479584, acc:  54%, op_acc: 80.00%] [G loss: 10.576793] time: 0:24:43.045818\n",
      "[Epoch 0/30] [Batch 662/816] [D loss: 4.266631, acc:  49%, op_acc: 85.00%] [G loss: 10.943125] time: 0:24:45.239888\n",
      "[Epoch 0/30] [Batch 663/816] [D loss: 0.382425, acc:  60%, op_acc: 95.00%] [G loss: 10.343156] time: 0:24:47.457456\n",
      "[Epoch 0/30] [Batch 664/816] [D loss: 0.297738, acc:  50%, op_acc: 100.00%] [G loss: 9.174776] time: 0:24:49.599769\n",
      "[Epoch 0/30] [Batch 665/816] [D loss: 0.297692, acc:  62%, op_acc: 95.00%] [G loss: 8.782944] time: 0:24:51.951634\n",
      "[Epoch 0/30] [Batch 666/816] [D loss: 0.233636, acc:  61%, op_acc: 100.00%] [G loss: 10.113339] time: 0:24:54.283583\n",
      "[Epoch 0/30] [Batch 667/816] [D loss: 0.400771, acc:  55%, op_acc: 95.00%] [G loss: 8.100871] time: 0:24:56.545107\n",
      "[Epoch 0/30] [Batch 668/816] [D loss: 0.256072, acc:  58%, op_acc: 100.00%] [G loss: 8.805624] time: 0:24:59.008161\n",
      "[Epoch 0/30] [Batch 669/816] [D loss: 1.475535, acc:  55%, op_acc: 95.00%] [G loss: 7.287256] time: 0:25:01.377332\n",
      "[Epoch 0/30] [Batch 670/816] [D loss: 0.280742, acc:  55%, op_acc: 100.00%] [G loss: 7.820533] time: 0:25:03.634949\n",
      "[Epoch 0/30] [Batch 671/816] [D loss: 0.255183, acc:  60%, op_acc: 100.00%] [G loss: 10.190630] time: 0:25:05.889294\n",
      "[Epoch 0/30] [Batch 672/816] [D loss: 0.246958, acc:  58%, op_acc: 100.00%] [G loss: 8.477807] time: 0:25:07.993135\n",
      "[Epoch 0/30] [Batch 673/816] [D loss: 0.750409, acc:  58%, op_acc: 90.00%] [G loss: 8.214694] time: 0:25:10.200904\n",
      "[Epoch 0/30] [Batch 674/816] [D loss: 0.883245, acc:  57%, op_acc: 90.00%] [G loss: 9.396334] time: 0:25:12.351367\n",
      "[Epoch 0/30] [Batch 675/816] [D loss: 1.329177, acc:  44%, op_acc: 90.00%] [G loss: 9.686842] time: 0:25:14.746237\n",
      "[Epoch 0/30] [Batch 676/816] [D loss: 0.854517, acc:  60%, op_acc: 95.00%] [G loss: 9.962007] time: 0:25:16.966532\n",
      "[Epoch 0/30] [Batch 677/816] [D loss: 0.842586, acc:  52%, op_acc: 95.00%] [G loss: 10.169835] time: 0:25:19.234007\n",
      "[Epoch 0/30] [Batch 678/816] [D loss: 0.256391, acc:  57%, op_acc: 100.00%] [G loss: 11.021878] time: 0:25:21.418808\n",
      "[Epoch 0/30] [Batch 679/816] [D loss: 0.256355, acc:  57%, op_acc: 100.00%] [G loss: 10.355830] time: 0:25:23.580966\n",
      "[Epoch 0/30] [Batch 680/816] [D loss: 0.251315, acc:  53%, op_acc: 100.00%] [G loss: 7.168648] time: 0:25:25.850636\n",
      "[Epoch 0/30] [Batch 681/816] [D loss: 0.421437, acc:  55%, op_acc: 95.00%] [G loss: 9.991025] time: 0:25:28.025741\n",
      "[Epoch 0/30] [Batch 682/816] [D loss: 5.857157, acc:  61%, op_acc: 85.00%] [G loss: 8.919979] time: 0:25:30.204474\n",
      "[Epoch 0/30] [Batch 683/816] [D loss: 1.974056, acc:  52%, op_acc: 75.00%] [G loss: 10.977562] time: 0:25:32.368186\n",
      "[Epoch 0/30] [Batch 684/816] [D loss: 0.273045, acc:  55%, op_acc: 100.00%] [G loss: 9.083952] time: 0:25:34.562537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/30] [Batch 685/816] [D loss: 0.255666, acc:  59%, op_acc: 100.00%] [G loss: 9.436607] time: 0:25:36.782226\n",
      "[Epoch 0/30] [Batch 686/816] [D loss: 0.250316, acc:  57%, op_acc: 100.00%] [G loss: 8.826730] time: 0:25:39.050661\n",
      "[Epoch 0/30] [Batch 687/816] [D loss: 0.530981, acc:  60%, op_acc: 95.00%] [G loss: 8.517569] time: 0:25:41.311118\n",
      "[Epoch 0/30] [Batch 688/816] [D loss: 0.261176, acc:  62%, op_acc: 100.00%] [G loss: 9.929855] time: 0:25:43.502840\n",
      "[Epoch 0/30] [Batch 689/816] [D loss: 0.291071, acc:  60%, op_acc: 95.00%] [G loss: 8.826122] time: 0:25:45.717268\n",
      "[Epoch 0/30] [Batch 690/816] [D loss: 0.578220, acc:  61%, op_acc: 95.00%] [G loss: 8.791721] time: 0:25:47.977350\n",
      "[Epoch 0/30] [Batch 691/816] [D loss: 0.643013, acc:  57%, op_acc: 95.00%] [G loss: 9.383553] time: 0:25:50.205551\n",
      "[Epoch 0/30] [Batch 692/816] [D loss: 0.252480, acc:  56%, op_acc: 100.00%] [G loss: 8.827311] time: 0:25:52.395304\n",
      "[Epoch 0/30] [Batch 693/816] [D loss: 0.256251, acc:  56%, op_acc: 100.00%] [G loss: 8.343645] time: 0:25:54.639585\n",
      "[Epoch 0/30] [Batch 694/816] [D loss: 0.259558, acc:  55%, op_acc: 100.00%] [G loss: 8.347265] time: 0:25:56.875234\n",
      "[Epoch 0/30] [Batch 695/816] [D loss: 0.234096, acc:  65%, op_acc: 100.00%] [G loss: 9.398476] time: 0:25:59.113981\n",
      "[Epoch 0/30] [Batch 696/816] [D loss: 0.271201, acc:  46%, op_acc: 100.00%] [G loss: 7.589089] time: 0:26:01.293454\n",
      "[Epoch 0/30] [Batch 697/816] [D loss: 0.244170, acc:  58%, op_acc: 100.00%] [G loss: 9.196100] time: 0:26:03.513930\n",
      "[Epoch 0/30] [Batch 698/816] [D loss: 0.237031, acc:  59%, op_acc: 100.00%] [G loss: 8.071383] time: 0:26:05.756020\n",
      "[Epoch 0/30] [Batch 699/816] [D loss: 0.232433, acc:  60%, op_acc: 100.00%] [G loss: 8.480006] time: 0:26:08.137085\n",
      "[Epoch 0/30] [Batch 700/816] [D loss: 0.610905, acc:  50%, op_acc: 90.00%] [G loss: 6.342519] time: 0:26:10.364779\n",
      "[Epoch 0/30] [Batch 701/816] [D loss: 0.260852, acc:  60%, op_acc: 100.00%] [G loss: 8.196919] time: 0:26:12.535088\n",
      "[Epoch 0/30] [Batch 702/816] [D loss: 0.243093, acc:  63%, op_acc: 100.00%] [G loss: 8.594696] time: 0:26:14.762192\n",
      "[Epoch 0/30] [Batch 703/816] [D loss: 0.241308, acc:  60%, op_acc: 100.00%] [G loss: 9.632339] time: 0:26:16.931618\n",
      "[Epoch 0/30] [Batch 704/816] [D loss: 0.249482, acc:  56%, op_acc: 100.00%] [G loss: 9.919664] time: 0:26:19.375033\n",
      "[Epoch 0/30] [Batch 705/816] [D loss: 1.070020, acc:  44%, op_acc: 90.00%] [G loss: 10.810205] time: 0:26:21.538862\n",
      "[Epoch 0/30] [Batch 706/816] [D loss: 1.739784, acc:  63%, op_acc: 95.00%] [G loss: 9.201395] time: 0:26:23.718013\n",
      "[Epoch 0/30] [Batch 707/816] [D loss: 0.260502, acc:  66%, op_acc: 100.00%] [G loss: 9.767412] time: 0:26:25.927126\n",
      "[Epoch 0/30] [Batch 708/816] [D loss: 0.480942, acc:  55%, op_acc: 95.00%] [G loss: 9.242834] time: 0:26:28.074893\n",
      "[Epoch 0/30] [Batch 709/816] [D loss: 0.515437, acc:  57%, op_acc: 95.00%] [G loss: 8.617212] time: 0:26:30.361510\n",
      "[Epoch 0/30] [Batch 710/816] [D loss: 0.268220, acc:  50%, op_acc: 100.00%] [G loss: 7.494606] time: 0:26:32.569358\n",
      "[Epoch 0/30] [Batch 711/816] [D loss: 0.261078, acc:  46%, op_acc: 100.00%] [G loss: 7.431077] time: 0:26:34.747320\n",
      "[Epoch 0/30] [Batch 712/816] [D loss: 1.478464, acc:  55%, op_acc: 90.00%] [G loss: 7.743759] time: 0:26:36.896569\n",
      "[Epoch 0/30] [Batch 713/816] [D loss: 1.235531, acc:  61%, op_acc: 90.00%] [G loss: 13.959426] time: 0:26:39.134164\n",
      "[Epoch 0/30] [Batch 714/816] [D loss: 0.285421, acc:  56%, op_acc: 100.00%] [G loss: 10.217838] time: 0:26:41.349595\n",
      "[Epoch 0/30] [Batch 715/816] [D loss: 0.289422, acc:  60%, op_acc: 100.00%] [G loss: 10.001294] time: 0:26:43.585367\n",
      "[Epoch 0/30] [Batch 716/816] [D loss: 0.278827, acc:  52%, op_acc: 100.00%] [G loss: 8.866311] time: 0:26:45.839858\n",
      "[Epoch 0/30] [Batch 717/816] [D loss: 0.925594, acc:  55%, op_acc: 95.00%] [G loss: 8.857389] time: 0:26:47.933926\n",
      "[Epoch 0/30] [Batch 718/816] [D loss: 0.850365, acc:  60%, op_acc: 95.00%] [G loss: 7.099244] time: 0:26:50.154406\n",
      "[Epoch 0/30] [Batch 719/816] [D loss: 0.247979, acc:  58%, op_acc: 100.00%] [G loss: 8.678058] time: 0:26:52.401896\n",
      "[Epoch 0/30] [Batch 720/816] [D loss: 0.592716, acc:  49%, op_acc: 95.00%] [G loss: 8.346942] time: 0:26:54.625763\n",
      "[Epoch 0/30] [Batch 721/816] [D loss: 1.595870, acc:  58%, op_acc: 90.00%] [G loss: 8.410451] time: 0:26:56.774714\n",
      "[Epoch 0/30] [Batch 722/816] [D loss: 1.387246, acc:  56%, op_acc: 90.00%] [G loss: 8.882782] time: 0:26:59.013419\n",
      "[Epoch 0/30] [Batch 723/816] [D loss: 1.396635, acc:  53%, op_acc: 95.00%] [G loss: 7.472197] time: 0:27:01.422740\n",
      "[Epoch 0/30] [Batch 724/816] [D loss: 0.251231, acc:  64%, op_acc: 100.00%] [G loss: 8.673407] time: 0:27:03.634960\n",
      "[Epoch 0/30] [Batch 725/816] [D loss: 1.187899, acc:  56%, op_acc: 90.00%] [G loss: 7.918869] time: 0:27:05.836825\n",
      "[Epoch 0/30] [Batch 726/816] [D loss: 1.577797, acc:  62%, op_acc: 95.00%] [G loss: 8.380581] time: 0:27:08.077177\n",
      "[Epoch 0/30] [Batch 727/816] [D loss: 0.245750, acc:  62%, op_acc: 100.00%] [G loss: 8.050187] time: 0:27:10.305980\n",
      "[Epoch 0/30] [Batch 728/816] [D loss: 0.780836, acc:  47%, op_acc: 80.00%] [G loss: 9.081885] time: 0:27:12.546857\n",
      "[Epoch 0/30] [Batch 729/816] [D loss: 0.378238, acc:  52%, op_acc: 95.00%] [G loss: 9.920897] time: 0:27:14.804790\n",
      "[Epoch 0/30] [Batch 730/816] [D loss: 0.241835, acc:  67%, op_acc: 100.00%] [G loss: 10.771178] time: 0:27:17.044159\n",
      "[Epoch 0/30] [Batch 731/816] [D loss: 0.638825, acc:  60%, op_acc: 90.00%] [G loss: 8.589602] time: 0:27:19.160190\n",
      "[Epoch 0/30] [Batch 732/816] [D loss: 0.243981, acc:  55%, op_acc: 100.00%] [G loss: 8.657301] time: 0:27:21.315529\n",
      "[Epoch 0/30] [Batch 733/816] [D loss: 0.248798, acc:  55%, op_acc: 100.00%] [G loss: 9.123439] time: 0:27:23.531597\n",
      "[Epoch 0/30] [Batch 734/816] [D loss: 1.905281, acc:  65%, op_acc: 95.00%] [G loss: 9.717797] time: 0:27:25.741652\n",
      "[Epoch 0/30] [Batch 735/816] [D loss: 0.229157, acc:  66%, op_acc: 100.00%] [G loss: 7.450881] time: 0:27:27.856257\n",
      "[Epoch 0/30] [Batch 736/816] [D loss: 1.682902, acc:  66%, op_acc: 95.00%] [G loss: 8.829735] time: 0:27:30.044432\n",
      "[Epoch 0/30] [Batch 737/816] [D loss: 0.214790, acc:  68%, op_acc: 100.00%] [G loss: 8.880389] time: 0:27:32.176380\n",
      "[Epoch 0/30] [Batch 738/816] [D loss: 0.502380, acc:  55%, op_acc: 95.00%] [G loss: 6.015314] time: 0:27:34.397016\n",
      "[Epoch 0/30] [Batch 739/816] [D loss: 0.283611, acc:  51%, op_acc: 100.00%] [G loss: 7.849775] time: 0:27:36.602227\n",
      "[Epoch 0/30] [Batch 740/816] [D loss: 0.593028, acc:  56%, op_acc: 95.00%] [G loss: 9.657182] time: 0:27:38.751035\n",
      "[Epoch 0/30] [Batch 741/816] [D loss: 0.247825, acc:  59%, op_acc: 100.00%] [G loss: 8.886380] time: 0:27:40.978744\n",
      "[Epoch 0/30] [Batch 742/816] [D loss: 0.872982, acc:  55%, op_acc: 95.00%] [G loss: 7.890757] time: 0:27:43.193484\n",
      "[Epoch 0/30] [Batch 743/816] [D loss: 0.488581, acc:  56%, op_acc: 95.00%] [G loss: 9.432896] time: 0:27:45.429372\n",
      "[Epoch 0/30] [Batch 744/816] [D loss: 0.257594, acc:  52%, op_acc: 100.00%] [G loss: 8.006358] time: 0:27:47.812706\n",
      "[Epoch 0/30] [Batch 745/816] [D loss: 0.227843, acc:  65%, op_acc: 100.00%] [G loss: 8.701152] time: 0:27:49.926594\n",
      "[Epoch 0/30] [Batch 746/816] [D loss: 1.324349, acc:  43%, op_acc: 85.00%] [G loss: 8.760204] time: 0:27:52.279899\n",
      "[Epoch 0/30] [Batch 747/816] [D loss: 0.239522, acc:  64%, op_acc: 100.00%] [G loss: 8.962686] time: 0:27:54.613303\n",
      "[Epoch 0/30] [Batch 748/816] [D loss: 1.114354, acc:  60%, op_acc: 95.00%] [G loss: 7.832988] time: 0:27:56.743913\n",
      "[Epoch 0/30] [Batch 749/816] [D loss: 0.513650, acc:  69%, op_acc: 90.00%] [G loss: 9.306946] time: 0:27:59.187707\n",
      "[Epoch 0/30] [Batch 750/816] [D loss: 0.293493, acc:  56%, op_acc: 100.00%] [G loss: 8.842861] time: 0:28:01.357964\n",
      "[Epoch 0/30] [Batch 751/816] [D loss: 0.328679, acc:  70%, op_acc: 100.00%] [G loss: 7.512705] time: 0:28:03.584439\n",
      "[Epoch 0/30] [Batch 752/816] [D loss: 0.441733, acc:  65%, op_acc: 100.00%] [G loss: 8.431235] time: 0:28:05.809597\n",
      "[Epoch 0/30] [Batch 753/816] [D loss: 1.633390, acc:  65%, op_acc: 95.00%] [G loss: 7.819792] time: 0:28:08.040983\n",
      "[Epoch 0/30] [Batch 754/816] [D loss: 2.176711, acc:  47%, op_acc: 90.00%] [G loss: 9.202961] time: 0:28:10.184894\n",
      "[Epoch 0/30] [Batch 755/816] [D loss: 0.588020, acc:  51%, op_acc: 95.00%] [G loss: 7.927398] time: 0:28:12.396508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/30] [Batch 756/816] [D loss: 0.277745, acc:  57%, op_acc: 100.00%] [G loss: 8.278872] time: 0:28:14.653839\n",
      "[Epoch 0/30] [Batch 757/816] [D loss: 0.299304, acc:  53%, op_acc: 95.00%] [G loss: 8.248427] time: 0:28:16.770775\n",
      "[Epoch 0/30] [Batch 758/816] [D loss: 0.271393, acc:  59%, op_acc: 100.00%] [G loss: 7.020219] time: 0:28:19.090701\n",
      "[Epoch 0/30] [Batch 759/816] [D loss: 0.250163, acc:  56%, op_acc: 100.00%] [G loss: 8.325518] time: 0:28:21.418032\n",
      "[Epoch 0/30] [Batch 760/816] [D loss: 0.256681, acc:  55%, op_acc: 100.00%] [G loss: 8.470747] time: 0:28:23.609575\n",
      "[Epoch 0/30] [Batch 761/816] [D loss: 0.260264, acc:  60%, op_acc: 100.00%] [G loss: 8.310280] time: 0:28:25.825166\n",
      "[Epoch 0/30] [Batch 762/816] [D loss: 0.221191, acc:  65%, op_acc: 100.00%] [G loss: 8.679920] time: 0:28:27.982947\n",
      "[Epoch 0/30] [Batch 763/816] [D loss: 0.550548, acc:  56%, op_acc: 95.00%] [G loss: 6.969862] time: 0:28:30.186088\n",
      "[Epoch 0/30] [Batch 764/816] [D loss: 0.256050, acc:  53%, op_acc: 100.00%] [G loss: 8.575525] time: 0:28:32.378363\n",
      "[Epoch 0/30] [Batch 765/816] [D loss: 0.256803, acc:  57%, op_acc: 100.00%] [G loss: 10.336948] time: 0:28:34.617646\n",
      "[Epoch 0/30] [Batch 766/816] [D loss: 0.225157, acc:  59%, op_acc: 100.00%] [G loss: 7.857983] time: 0:28:36.855778\n",
      "[Epoch 0/30] [Batch 767/816] [D loss: 0.259139, acc:  58%, op_acc: 100.00%] [G loss: 7.367816] time: 0:28:39.005959\n",
      "[Epoch 0/30] [Batch 768/816] [D loss: 0.273636, acc:  51%, op_acc: 100.00%] [G loss: 8.488378] time: 0:28:41.138322\n",
      "[Epoch 0/30] [Batch 769/816] [D loss: 0.254325, acc:  53%, op_acc: 100.00%] [G loss: 8.585030] time: 0:28:43.412423\n",
      "[Epoch 0/30] [Batch 770/816] [D loss: 0.311845, acc:  51%, op_acc: 95.00%] [G loss: 8.759540] time: 0:28:45.657603\n",
      "[Epoch 0/30] [Batch 771/816] [D loss: 0.277649, acc:  66%, op_acc: 95.00%] [G loss: 8.651684] time: 0:28:47.910595\n",
      "[Epoch 0/30] [Batch 772/816] [D loss: 0.251154, acc:  56%, op_acc: 100.00%] [G loss: 7.917478] time: 0:28:50.157592\n",
      "[Epoch 0/30] [Batch 773/816] [D loss: 0.426274, acc:  69%, op_acc: 95.00%] [G loss: 8.847085] time: 0:28:52.330300\n",
      "[Epoch 0/30] [Batch 774/816] [D loss: 0.238656, acc:  58%, op_acc: 100.00%] [G loss: 9.731920] time: 0:28:54.623230\n",
      "[Epoch 0/30] [Batch 775/816] [D loss: 0.234182, acc:  64%, op_acc: 100.00%] [G loss: 10.307889] time: 0:28:56.877937\n",
      "[Epoch 0/30] [Batch 776/816] [D loss: 0.224811, acc:  62%, op_acc: 100.00%] [G loss: 7.573373] time: 0:28:59.303472\n",
      "[Epoch 0/30] [Batch 777/816] [D loss: 0.238080, acc:  60%, op_acc: 100.00%] [G loss: 7.716788] time: 0:29:01.508761\n",
      "[Epoch 0/30] [Batch 778/816] [D loss: 0.308606, acc:  67%, op_acc: 100.00%] [G loss: 8.532957] time: 0:29:03.788075\n",
      "[Epoch 0/30] [Batch 779/816] [D loss: 0.279478, acc:  71%, op_acc: 100.00%] [G loss: 8.053768] time: 0:29:05.980493\n",
      "[Epoch 0/30] [Batch 780/816] [D loss: 0.908048, acc:  60%, op_acc: 95.00%] [G loss: 8.834087] time: 0:29:08.158108\n",
      "[Epoch 0/30] [Batch 781/816] [D loss: 0.255832, acc:  52%, op_acc: 100.00%] [G loss: 7.154019] time: 0:29:10.311797\n",
      "[Epoch 0/30] [Batch 782/816] [D loss: 0.275594, acc:  54%, op_acc: 100.00%] [G loss: 8.067749] time: 0:29:12.526485\n",
      "[Epoch 0/30] [Batch 783/816] [D loss: 0.236983, acc:  69%, op_acc: 100.00%] [G loss: 9.514714] time: 0:29:14.724061\n",
      "[Epoch 0/30] [Batch 784/816] [D loss: 0.463216, acc:  66%, op_acc: 95.00%] [G loss: 9.292707] time: 0:29:16.867396\n",
      "[Epoch 0/30] [Batch 785/816] [D loss: 0.326941, acc:  69%, op_acc: 95.00%] [G loss: 8.562987] time: 0:29:19.092995\n",
      "[Epoch 0/30] [Batch 786/816] [D loss: 0.258102, acc:  64%, op_acc: 95.00%] [G loss: 8.850103] time: 0:29:21.319149\n",
      "[Epoch 0/30] [Batch 787/816] [D loss: 0.237749, acc:  56%, op_acc: 100.00%] [G loss: 8.202116] time: 0:29:23.625856\n",
      "[Epoch 0/30] [Batch 788/816] [D loss: 0.253316, acc:  57%, op_acc: 100.00%] [G loss: 7.857179] time: 0:29:25.882615\n",
      "[Epoch 0/30] [Batch 789/816] [D loss: 0.849409, acc:  73%, op_acc: 95.00%] [G loss: 9.571185] time: 0:29:28.148855\n",
      "[Epoch 0/30] [Batch 790/816] [D loss: 0.304996, acc:  51%, op_acc: 100.00%] [G loss: 8.833479] time: 0:29:30.390105\n",
      "[Epoch 0/30] [Batch 791/816] [D loss: 0.245788, acc:  60%, op_acc: 100.00%] [G loss: 8.908755] time: 0:29:32.601438\n",
      "[Epoch 0/30] [Batch 792/816] [D loss: 0.253536, acc:  49%, op_acc: 100.00%] [G loss: 8.006283] time: 0:29:34.811343\n",
      "[Epoch 0/30] [Batch 793/816] [D loss: 0.260095, acc:  62%, op_acc: 100.00%] [G loss: 8.588284] time: 0:29:36.950115\n",
      "[Epoch 0/30] [Batch 794/816] [D loss: 0.261853, acc:  46%, op_acc: 100.00%] [G loss: 7.381083] time: 0:29:39.211314\n",
      "[Epoch 0/30] [Batch 795/816] [D loss: 0.247004, acc:  62%, op_acc: 100.00%] [G loss: 9.151796] time: 0:29:41.479215\n",
      "[Epoch 0/30] [Batch 796/816] [D loss: 0.261703, acc:  69%, op_acc: 100.00%] [G loss: 8.767448] time: 0:29:44.037667\n",
      "[Epoch 0/30] [Batch 797/816] [D loss: 0.283472, acc:  42%, op_acc: 100.00%] [G loss: 8.783813] time: 0:29:46.262964\n",
      "[Epoch 0/30] [Batch 798/816] [D loss: 0.938980, acc:  57%, op_acc: 95.00%] [G loss: 7.985504] time: 0:29:48.379162\n",
      "[Epoch 0/30] [Batch 799/816] [D loss: 0.689132, acc:  70%, op_acc: 100.00%] [G loss: 9.555248] time: 0:29:50.610400\n",
      "[Epoch 0/30] [Batch 800/816] [D loss: 1.586361, acc:  56%, op_acc: 95.00%] [G loss: 9.650690] time: 0:29:52.857696\n",
      "SAVE\n",
      "0_800-T2-T1.png\n",
      "[Epoch 0/30] [Batch 801/816] [D loss: 3.521528, acc:  51%, op_acc: 95.00%] [G loss: 8.427325] time: 0:29:55.509455\n",
      "[Epoch 0/30] [Batch 802/816] [D loss: 0.388980, acc:  64%, op_acc: 100.00%] [G loss: 9.837072] time: 0:29:57.695575\n",
      "[Epoch 0/30] [Batch 803/816] [D loss: 0.213413, acc:  73%, op_acc: 100.00%] [G loss: 10.651409] time: 0:29:59.799507\n",
      "[Epoch 0/30] [Batch 804/816] [D loss: 0.270549, acc:  62%, op_acc: 100.00%] [G loss: 6.957737] time: 0:30:02.021901\n",
      "[Epoch 0/30] [Batch 805/816] [D loss: 0.237340, acc:  60%, op_acc: 100.00%] [G loss: 8.404949] time: 0:30:04.275280\n",
      "[Epoch 0/30] [Batch 806/816] [D loss: 0.211390, acc:  67%, op_acc: 100.00%] [G loss: 8.901305] time: 0:30:06.504397\n",
      "save generator\n",
      "[Epoch 0/30] [Batch 807/816] [D loss: 0.222452, acc:  62%, op_acc: 100.00%] [G loss: 9.372853] time: 0:30:13.603954\n",
      "[Epoch 0/30] [Batch 808/816] [D loss: 0.268750, acc:  68%, op_acc: 100.00%] [G loss: 7.844580] time: 0:30:15.756197\n",
      "[Epoch 0/30] [Batch 809/816] [D loss: 0.207257, acc:  71%, op_acc: 100.00%] [G loss: 8.349353] time: 0:30:17.888352\n",
      "[Epoch 0/30] [Batch 810/816] [D loss: 0.214454, acc:  68%, op_acc: 100.00%] [G loss: 8.772648] time: 0:30:20.099463\n",
      "[Epoch 0/30] [Batch 811/816] [D loss: 0.401784, acc:  58%, op_acc: 95.00%] [G loss: 10.134713] time: 0:30:22.405195\n",
      "[Epoch 0/30] [Batch 812/816] [D loss: 0.784489, acc:  60%, op_acc: 90.00%] [G loss: 8.581964] time: 0:30:24.623982\n",
      "[Epoch 0/30] [Batch 813/816] [D loss: 0.437704, acc:  62%, op_acc: 95.00%] [G loss: 6.750453] time: 0:30:26.831274\n",
      "[Epoch 0/30] [Batch 814/816] [D loss: 0.308530, acc:  63%, op_acc: 100.00%] [G loss: 9.183426] time: 0:30:29.012846\n",
      "[Epoch 0/30] [Batch 815/816] [D loss: 0.249333, acc:  57%, op_acc: 100.00%] [G loss: 7.444939] time: 0:30:31.245183\n",
      "[Epoch 1/30] [Batch 0/816] [D loss: 0.248143, acc:  57%, op_acc: 100.00%] [G loss: 8.473489] time: 0:30:33.223071\n",
      "SAVE\n",
      "1_0-PD-T1.png\n",
      "[Epoch 1/30] [Batch 1/816] [D loss: 0.241240, acc:  63%, op_acc: 100.00%] [G loss: 9.189650] time: 0:30:35.473524\n",
      "[Epoch 1/30] [Batch 2/816] [D loss: 0.204583, acc:  68%, op_acc: 100.00%] [G loss: 9.381729] time: 0:30:37.410631\n",
      "save generator\n",
      "[Epoch 1/30] [Batch 3/816] [D loss: 0.266579, acc:  51%, op_acc: 100.00%] [G loss: 7.708394] time: 0:30:41.085991\n",
      "[Epoch 1/30] [Batch 4/816] [D loss: 4.210772, acc:  67%, op_acc: 90.00%] [G loss: 9.130329] time: 0:30:43.030480\n",
      "[Epoch 1/30] [Batch 5/816] [D loss: 0.194009, acc:  73%, op_acc: 100.00%] [G loss: 9.417686] time: 0:30:44.973242\n",
      "[Epoch 1/30] [Batch 6/816] [D loss: 0.210737, acc:  68%, op_acc: 100.00%] [G loss: 11.517478] time: 0:30:46.908572\n",
      "[Epoch 1/30] [Batch 7/816] [D loss: 0.206963, acc:  67%, op_acc: 100.00%] [G loss: 11.040706] time: 0:30:48.851190\n",
      "[Epoch 1/30] [Batch 8/816] [D loss: 0.221269, acc:  63%, op_acc: 100.00%] [G loss: 11.076074] time: 0:30:50.789050\n",
      "[Epoch 1/30] [Batch 9/816] [D loss: 0.234937, acc:  61%, op_acc: 100.00%] [G loss: 9.890997] time: 0:30:52.719452\n",
      "[Epoch 1/30] [Batch 10/816] [D loss: 0.247179, acc:  63%, op_acc: 100.00%] [G loss: 9.943755] time: 0:30:54.658457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/30] [Batch 11/816] [D loss: 0.293895, acc:  62%, op_acc: 100.00%] [G loss: 11.046664] time: 0:30:56.598936\n",
      "[Epoch 1/30] [Batch 12/816] [D loss: 0.243620, acc:  67%, op_acc: 100.00%] [G loss: 8.440166] time: 0:30:58.537716\n",
      "[Epoch 1/30] [Batch 13/816] [D loss: 0.317477, acc:  64%, op_acc: 95.00%] [G loss: 8.420203] time: 0:31:00.478481\n",
      "[Epoch 1/30] [Batch 14/816] [D loss: 2.832398, acc:  68%, op_acc: 85.00%] [G loss: 9.610538] time: 0:31:02.410005\n",
      "[Epoch 1/30] [Batch 15/816] [D loss: 0.210705, acc:  66%, op_acc: 100.00%] [G loss: 8.583465] time: 0:31:04.343551\n",
      "[Epoch 1/30] [Batch 16/816] [D loss: 0.254763, acc:  50%, op_acc: 100.00%] [G loss: 9.318838] time: 0:31:06.281692\n",
      "[Epoch 1/30] [Batch 17/816] [D loss: 2.245307, acc:  67%, op_acc: 95.00%] [G loss: 8.391222] time: 0:31:08.217448\n",
      "[Epoch 1/30] [Batch 18/816] [D loss: 0.236239, acc:  60%, op_acc: 100.00%] [G loss: 8.040698] time: 0:31:10.154375\n",
      "[Epoch 1/30] [Batch 19/816] [D loss: 0.352962, acc:  71%, op_acc: 95.00%] [G loss: 8.557754] time: 0:31:12.089349\n",
      "[Epoch 1/30] [Batch 20/816] [D loss: 0.882894, acc:  74%, op_acc: 95.00%] [G loss: 8.012438] time: 0:31:14.019554\n",
      "[Epoch 1/30] [Batch 21/816] [D loss: 1.097939, acc:  63%, op_acc: 90.00%] [G loss: 10.434289] time: 0:31:15.958531\n",
      "[Epoch 1/30] [Batch 22/816] [D loss: 0.270490, acc:  59%, op_acc: 100.00%] [G loss: 12.227984] time: 0:31:17.891199\n",
      "[Epoch 1/30] [Batch 23/816] [D loss: 0.200926, acc:  76%, op_acc: 100.00%] [G loss: 10.749350] time: 0:31:19.825550\n",
      "[Epoch 1/30] [Batch 24/816] [D loss: 1.980739, acc:  55%, op_acc: 95.00%] [G loss: 9.268633] time: 0:31:21.760407\n",
      "[Epoch 1/30] [Batch 25/816] [D loss: 0.468402, acc:  62%, op_acc: 95.00%] [G loss: 8.330770] time: 0:31:23.708000\n",
      "[Epoch 1/30] [Batch 26/816] [D loss: 1.019155, acc:  61%, op_acc: 85.00%] [G loss: 9.360197] time: 0:31:25.648201\n",
      "[Epoch 1/30] [Batch 27/816] [D loss: 0.281881, acc:  64%, op_acc: 100.00%] [G loss: 8.642241] time: 0:31:27.581598\n",
      "[Epoch 1/30] [Batch 28/816] [D loss: 0.244391, acc:  72%, op_acc: 100.00%] [G loss: 10.416277] time: 0:31:29.521059\n",
      "[Epoch 1/30] [Batch 29/816] [D loss: 0.645864, acc:  67%, op_acc: 95.00%] [G loss: 8.061213] time: 0:31:31.454342\n",
      "[Epoch 1/30] [Batch 30/816] [D loss: 0.326617, acc:  74%, op_acc: 95.00%] [G loss: 12.072905] time: 0:31:33.393213\n",
      "[Epoch 1/30] [Batch 31/816] [D loss: 0.299802, acc:  45%, op_acc: 100.00%] [G loss: 8.866961] time: 0:31:35.331874\n",
      "[Epoch 1/30] [Batch 32/816] [D loss: 1.167575, acc:  47%, op_acc: 90.00%] [G loss: 10.103668] time: 0:31:37.266415\n",
      "[Epoch 1/30] [Batch 33/816] [D loss: 0.315784, acc:  54%, op_acc: 100.00%] [G loss: 11.202694] time: 0:31:39.213710\n",
      "[Epoch 1/30] [Batch 34/816] [D loss: 0.263646, acc:  57%, op_acc: 100.00%] [G loss: 9.080293] time: 0:31:41.153599\n",
      "[Epoch 1/30] [Batch 35/816] [D loss: 1.022323, acc:  61%, op_acc: 90.00%] [G loss: 8.573341] time: 0:31:43.086147\n",
      "[Epoch 1/30] [Batch 36/816] [D loss: 0.254335, acc:  60%, op_acc: 100.00%] [G loss: 10.620984] time: 0:31:45.022743\n",
      "[Epoch 1/30] [Batch 37/816] [D loss: 1.676994, acc:  70%, op_acc: 90.00%] [G loss: 8.404924] time: 0:31:46.953521\n",
      "[Epoch 1/30] [Batch 38/816] [D loss: 0.266834, acc:  57%, op_acc: 100.00%] [G loss: 8.392859] time: 0:31:48.892168\n",
      "[Epoch 1/30] [Batch 39/816] [D loss: 0.224141, acc:  69%, op_acc: 100.00%] [G loss: 8.206953] time: 0:31:50.828860\n",
      "[Epoch 1/30] [Batch 40/816] [D loss: 0.219350, acc:  66%, op_acc: 100.00%] [G loss: 8.606519] time: 0:31:52.761772\n",
      "[Epoch 1/30] [Batch 41/816] [D loss: 0.224075, acc:  69%, op_acc: 100.00%] [G loss: 8.497272] time: 0:31:54.711819\n",
      "[Epoch 1/30] [Batch 42/816] [D loss: 0.707891, acc:  73%, op_acc: 90.00%] [G loss: 7.688080] time: 0:31:56.651699\n",
      "[Epoch 1/30] [Batch 43/816] [D loss: 0.234239, acc:  61%, op_acc: 100.00%] [G loss: 9.466382] time: 0:31:58.592887\n",
      "[Epoch 1/30] [Batch 44/816] [D loss: 1.380772, acc:  60%, op_acc: 95.00%] [G loss: 9.413502] time: 0:32:00.533569\n",
      "[Epoch 1/30] [Batch 45/816] [D loss: 0.421821, acc:  74%, op_acc: 95.00%] [G loss: 7.905645] time: 0:32:02.468682\n",
      "[Epoch 1/30] [Batch 46/816] [D loss: 0.190477, acc:  73%, op_acc: 100.00%] [G loss: 8.997667] time: 0:32:04.406634\n",
      "[Epoch 1/30] [Batch 47/816] [D loss: 0.210407, acc:  63%, op_acc: 100.00%] [G loss: 8.019098] time: 0:32:06.346883\n",
      "[Epoch 1/30] [Batch 48/816] [D loss: 0.248908, acc:  66%, op_acc: 100.00%] [G loss: 9.436475] time: 0:32:08.286400\n",
      "[Epoch 1/30] [Batch 49/816] [D loss: 0.274751, acc:  58%, op_acc: 100.00%] [G loss: 7.135569] time: 0:32:10.224936\n",
      "[Epoch 1/30] [Batch 50/816] [D loss: 0.598231, acc:  69%, op_acc: 95.00%] [G loss: 7.790248] time: 0:32:12.159785\n",
      "[Epoch 1/30] [Batch 51/816] [D loss: 0.241740, acc:  59%, op_acc: 100.00%] [G loss: 7.519330] time: 0:32:14.097679\n",
      "[Epoch 1/30] [Batch 52/816] [D loss: 0.192006, acc:  74%, op_acc: 100.00%] [G loss: 10.217727] time: 0:32:16.034913\n",
      "[Epoch 1/30] [Batch 53/816] [D loss: 0.366785, acc:  74%, op_acc: 95.00%] [G loss: 7.483739] time: 0:32:17.971519\n",
      "[Epoch 1/30] [Batch 54/816] [D loss: 0.188411, acc:  76%, op_acc: 100.00%] [G loss: 10.208885] time: 0:32:19.915780\n",
      "[Epoch 1/30] [Batch 55/816] [D loss: 0.300123, acc:  49%, op_acc: 100.00%] [G loss: 7.481037] time: 0:32:21.851772\n",
      "[Epoch 1/30] [Batch 56/816] [D loss: 0.213271, acc:  70%, op_acc: 100.00%] [G loss: 14.336607] time: 0:32:23.793883\n",
      "[Epoch 1/30] [Batch 57/816] [D loss: 0.547461, acc:  63%, op_acc: 90.00%] [G loss: 11.882128] time: 0:32:25.729655\n",
      "[Epoch 1/30] [Batch 58/816] [D loss: 1.481076, acc:  65%, op_acc: 85.00%] [G loss: 10.491119] time: 0:32:27.662307\n",
      "[Epoch 1/30] [Batch 59/816] [D loss: 1.785561, acc:  28%, op_acc: 95.00%] [G loss: 11.714722] time: 0:32:29.599167\n",
      "[Epoch 1/30] [Batch 60/816] [D loss: 0.671432, acc:  62%, op_acc: 100.00%] [G loss: 10.788004] time: 0:32:31.547899\n",
      "[Epoch 1/30] [Batch 61/816] [D loss: 0.326422, acc:  55%, op_acc: 100.00%] [G loss: 9.743057] time: 0:32:33.487136\n",
      "[Epoch 1/30] [Batch 62/816] [D loss: 0.505888, acc:  55%, op_acc: 100.00%] [G loss: 9.452961] time: 0:32:35.421948\n",
      "[Epoch 1/30] [Batch 63/816] [D loss: 0.632311, acc:  66%, op_acc: 100.00%] [G loss: 10.353286] time: 0:32:37.355762\n",
      "[Epoch 1/30] [Batch 64/816] [D loss: 0.552393, acc:  67%, op_acc: 100.00%] [G loss: 9.120830] time: 0:32:39.295362\n",
      "[Epoch 1/30] [Batch 65/816] [D loss: 0.288252, acc:  47%, op_acc: 100.00%] [G loss: 10.929246] time: 0:32:41.231725\n",
      "[Epoch 1/30] [Batch 66/816] [D loss: 0.222850, acc:  72%, op_acc: 100.00%] [G loss: 8.947701] time: 0:32:43.173858\n",
      "[Epoch 1/30] [Batch 67/816] [D loss: 0.407410, acc:  55%, op_acc: 95.00%] [G loss: 8.298627] time: 0:32:45.111057\n",
      "[Epoch 1/30] [Batch 68/816] [D loss: 0.200065, acc:  68%, op_acc: 100.00%] [G loss: 9.262743] time: 0:32:47.041886\n",
      "[Epoch 1/30] [Batch 69/816] [D loss: 0.209268, acc:  71%, op_acc: 100.00%] [G loss: 12.212270] time: 0:32:48.986232\n",
      "[Epoch 1/30] [Batch 70/816] [D loss: 0.272230, acc:  65%, op_acc: 100.00%] [G loss: 8.123806] time: 0:32:50.921596\n",
      "[Epoch 1/30] [Batch 71/816] [D loss: 0.252718, acc:  60%, op_acc: 100.00%] [G loss: 9.041237] time: 0:32:52.864831\n",
      "[Epoch 1/30] [Batch 72/816] [D loss: 0.188105, acc:  74%, op_acc: 100.00%] [G loss: 10.717346] time: 0:32:54.804980\n",
      "[Epoch 1/30] [Batch 73/816] [D loss: 1.256560, acc:  64%, op_acc: 95.00%] [G loss: 8.614710] time: 0:32:56.733425\n",
      "[Epoch 1/30] [Batch 74/816] [D loss: 0.262396, acc:  59%, op_acc: 100.00%] [G loss: 9.273265] time: 0:32:58.681059\n",
      "[Epoch 1/30] [Batch 75/816] [D loss: 1.702851, acc:  74%, op_acc: 90.00%] [G loss: 8.868084] time: 0:33:00.624398\n",
      "[Epoch 1/30] [Batch 76/816] [D loss: 0.208604, acc:  66%, op_acc: 100.00%] [G loss: 8.533195] time: 0:33:02.561887\n",
      "[Epoch 1/30] [Batch 77/816] [D loss: 0.265727, acc:  53%, op_acc: 100.00%] [G loss: 7.567877] time: 0:33:04.500050\n",
      "[Epoch 1/30] [Batch 78/816] [D loss: 0.181282, acc:  79%, op_acc: 100.00%] [G loss: 9.238937] time: 0:33:06.443216\n",
      "[Epoch 1/30] [Batch 79/816] [D loss: 0.241111, acc:  62%, op_acc: 100.00%] [G loss: 8.170810] time: 0:33:08.374057\n",
      "[Epoch 1/30] [Batch 80/816] [D loss: 1.124756, acc:  70%, op_acc: 95.00%] [G loss: 7.677940] time: 0:33:10.313934\n",
      "[Epoch 1/30] [Batch 81/816] [D loss: 1.177760, acc:  69%, op_acc: 95.00%] [G loss: 10.368666] time: 0:33:12.248983\n",
      "[Epoch 1/30] [Batch 82/816] [D loss: 0.259884, acc:  56%, op_acc: 100.00%] [G loss: 9.647274] time: 0:33:14.183494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/30] [Batch 83/816] [D loss: 0.573052, acc:  72%, op_acc: 95.00%] [G loss: 9.280678] time: 0:33:16.120321\n",
      "[Epoch 1/30] [Batch 84/816] [D loss: 0.182573, acc:  78%, op_acc: 100.00%] [G loss: 8.876796] time: 0:33:18.061770\n",
      "[Epoch 1/30] [Batch 85/816] [D loss: 0.235894, acc:  63%, op_acc: 100.00%] [G loss: 8.853634] time: 0:33:19.995361\n",
      "[Epoch 1/30] [Batch 86/816] [D loss: 0.322152, acc:  59%, op_acc: 100.00%] [G loss: 8.771433] time: 0:33:21.936452\n",
      "[Epoch 1/30] [Batch 87/816] [D loss: 0.305697, acc:  70%, op_acc: 100.00%] [G loss: 8.848093] time: 0:33:23.880065\n",
      "[Epoch 1/30] [Batch 88/816] [D loss: 0.185961, acc:  75%, op_acc: 100.00%] [G loss: 8.762422] time: 0:33:25.817445\n",
      "[Epoch 1/30] [Batch 89/816] [D loss: 0.259026, acc:  56%, op_acc: 100.00%] [G loss: 7.711346] time: 0:33:27.753731\n",
      "[Epoch 1/30] [Batch 90/816] [D loss: 0.226931, acc:  63%, op_acc: 100.00%] [G loss: 7.957553] time: 0:33:29.692792\n",
      "[Epoch 1/30] [Batch 91/816] [D loss: 0.182526, acc:  79%, op_acc: 100.00%] [G loss: 9.456224] time: 0:33:31.623795\n",
      "[Epoch 1/30] [Batch 92/816] [D loss: 0.277735, acc:  56%, op_acc: 100.00%] [G loss: 8.052412] time: 0:33:33.562072\n",
      "[Epoch 1/30] [Batch 93/816] [D loss: 0.214168, acc:  69%, op_acc: 100.00%] [G loss: 7.741695] time: 0:33:35.503434\n",
      "[Epoch 1/30] [Batch 94/816] [D loss: 0.210139, acc:  77%, op_acc: 100.00%] [G loss: 10.443232] time: 0:33:37.440168\n",
      "[Epoch 1/30] [Batch 95/816] [D loss: 0.296075, acc:  49%, op_acc: 100.00%] [G loss: 7.711582] time: 0:33:39.374756\n",
      "[Epoch 1/30] [Batch 96/816] [D loss: 0.281171, acc:  78%, op_acc: 95.00%] [G loss: 8.908885] time: 0:33:41.323717\n",
      "[Epoch 1/30] [Batch 97/816] [D loss: 0.173780, acc:  79%, op_acc: 100.00%] [G loss: 8.634267] time: 0:33:43.259640\n",
      "[Epoch 1/30] [Batch 98/816] [D loss: 0.208380, acc:  76%, op_acc: 100.00%] [G loss: 8.378963] time: 0:33:45.192699\n",
      "[Epoch 1/30] [Batch 99/816] [D loss: 0.187047, acc:  78%, op_acc: 100.00%] [G loss: 7.966611] time: 0:33:47.121059\n",
      "[Epoch 1/30] [Batch 100/816] [D loss: 0.166881, acc:  83%, op_acc: 100.00%] [G loss: 9.957330] time: 0:33:49.056980\n",
      "[Epoch 1/30] [Batch 101/816] [D loss: 0.207708, acc:  73%, op_acc: 100.00%] [G loss: 9.760325] time: 0:33:50.995084\n",
      "[Epoch 1/30] [Batch 102/816] [D loss: 0.573253, acc:  73%, op_acc: 95.00%] [G loss: 10.139906] time: 0:33:52.932690\n",
      "[Epoch 1/30] [Batch 103/816] [D loss: 0.278406, acc:  56%, op_acc: 100.00%] [G loss: 11.254745] time: 0:33:54.871210\n",
      "[Epoch 1/30] [Batch 104/816] [D loss: 0.259541, acc:  64%, op_acc: 100.00%] [G loss: 9.884699] time: 0:33:56.800526\n",
      "[Epoch 1/30] [Batch 105/816] [D loss: 0.669309, acc:  67%, op_acc: 95.00%] [G loss: 7.642537] time: 0:33:58.745026\n",
      "[Epoch 1/30] [Batch 106/816] [D loss: 0.247872, acc:  71%, op_acc: 100.00%] [G loss: 9.684405] time: 0:34:00.684490\n",
      "[Epoch 1/30] [Batch 107/816] [D loss: 0.219024, acc:  74%, op_acc: 100.00%] [G loss: 7.663908] time: 0:34:02.618290\n",
      "[Epoch 1/30] [Batch 108/816] [D loss: 0.247555, acc:  55%, op_acc: 100.00%] [G loss: 8.417748] time: 0:34:04.555466\n",
      "[Epoch 1/30] [Batch 109/816] [D loss: 0.254756, acc:  56%, op_acc: 100.00%] [G loss: 6.654127] time: 0:34:06.490175\n",
      "[Epoch 1/30] [Batch 110/816] [D loss: 0.177145, acc:  82%, op_acc: 100.00%] [G loss: 8.574362] time: 0:34:08.424846\n",
      "[Epoch 1/30] [Batch 111/816] [D loss: 0.141901, acc:  84%, op_acc: 100.00%] [G loss: 11.115126] time: 0:34:10.361673\n",
      "[Epoch 1/30] [Batch 112/816] [D loss: 0.275201, acc:  51%, op_acc: 100.00%] [G loss: 9.669548] time: 0:34:12.297298\n",
      "[Epoch 1/30] [Batch 113/816] [D loss: 0.778554, acc:  54%, op_acc: 95.00%] [G loss: 10.913666] time: 0:34:14.236665\n",
      "[Epoch 1/30] [Batch 114/816] [D loss: 0.274624, acc:  52%, op_acc: 100.00%] [G loss: 8.923130] time: 0:34:16.179346\n",
      "[Epoch 1/30] [Batch 115/816] [D loss: 0.176934, acc:  82%, op_acc: 100.00%] [G loss: 9.622123] time: 0:34:18.112629\n",
      "[Epoch 1/30] [Batch 116/816] [D loss: 0.214984, acc:  71%, op_acc: 100.00%] [G loss: 9.940759] time: 0:34:20.046570\n",
      "[Epoch 1/30] [Batch 117/816] [D loss: 0.182761, acc:  78%, op_acc: 100.00%] [G loss: 8.834231] time: 0:34:21.987114\n",
      "[Epoch 1/30] [Batch 118/816] [D loss: 0.210024, acc:  75%, op_acc: 100.00%] [G loss: 8.729215] time: 0:34:23.920320\n",
      "[Epoch 1/30] [Batch 119/816] [D loss: 0.502557, acc:  75%, op_acc: 95.00%] [G loss: 7.139388] time: 0:34:25.845978\n",
      "[Epoch 1/30] [Batch 120/816] [D loss: 0.352156, acc:  69%, op_acc: 95.00%] [G loss: 12.049850] time: 0:34:27.783720\n",
      "[Epoch 1/30] [Batch 121/816] [D loss: 1.353481, acc:  45%, op_acc: 90.00%] [G loss: 17.952835] time: 0:34:29.717921\n",
      "[Epoch 1/30] [Batch 122/816] [D loss: 1.536589, acc:  65%, op_acc: 85.00%] [G loss: 17.996426] time: 0:34:31.660304\n",
      "[Epoch 1/30] [Batch 123/816] [D loss: 0.482338, acc:  30%, op_acc: 100.00%] [G loss: 13.151697] time: 0:34:33.601115\n",
      "[Epoch 1/30] [Batch 124/816] [D loss: 0.375023, acc:  43%, op_acc: 100.00%] [G loss: 12.545974] time: 0:34:35.537176\n",
      "[Epoch 1/30] [Batch 125/816] [D loss: 0.295428, acc:  44%, op_acc: 100.00%] [G loss: 10.228828] time: 0:34:37.479331\n",
      "[Epoch 1/30] [Batch 126/816] [D loss: 0.534090, acc:  47%, op_acc: 95.00%] [G loss: 11.049801] time: 0:34:39.417900\n",
      "[Epoch 1/30] [Batch 127/816] [D loss: 0.229503, acc:  62%, op_acc: 100.00%] [G loss: 9.987058] time: 0:34:41.357115\n",
      "[Epoch 1/30] [Batch 128/816] [D loss: 0.852660, acc:  54%, op_acc: 95.00%] [G loss: 8.976606] time: 0:34:43.290697\n",
      "[Epoch 1/30] [Batch 129/816] [D loss: 3.467756, acc:  63%, op_acc: 90.00%] [G loss: 12.891147] time: 0:34:45.228269\n",
      "[Epoch 1/30] [Batch 130/816] [D loss: 1.421586, acc:  63%, op_acc: 95.00%] [G loss: 10.340392] time: 0:34:47.166104\n",
      "[Epoch 1/30] [Batch 131/816] [D loss: 0.925153, acc:  43%, op_acc: 95.00%] [G loss: 11.099976] time: 0:34:49.102918\n",
      "[Epoch 1/30] [Batch 132/816] [D loss: 0.253271, acc:  68%, op_acc: 100.00%] [G loss: 9.812375] time: 0:34:51.051680\n",
      "[Epoch 1/30] [Batch 133/816] [D loss: 0.698527, acc:  68%, op_acc: 95.00%] [G loss: 10.774862] time: 0:34:52.983235\n",
      "[Epoch 1/30] [Batch 134/816] [D loss: 2.016779, acc:  51%, op_acc: 95.00%] [G loss: 10.506620] time: 0:34:54.916069\n",
      "[Epoch 1/30] [Batch 135/816] [D loss: 0.815600, acc:  69%, op_acc: 95.00%] [G loss: 10.079493] time: 0:34:56.849550\n",
      "[Epoch 1/30] [Batch 136/816] [D loss: 0.194080, acc:  72%, op_acc: 100.00%] [G loss: 10.040846] time: 0:34:58.779793\n",
      "[Epoch 1/30] [Batch 137/816] [D loss: 0.194997, acc:  76%, op_acc: 100.00%] [G loss: 10.769026] time: 0:35:00.712754\n",
      "[Epoch 1/30] [Batch 138/816] [D loss: 1.552951, acc:  70%, op_acc: 95.00%] [G loss: 8.948620] time: 0:35:02.643165\n",
      "[Epoch 1/30] [Batch 139/816] [D loss: 0.241734, acc:  67%, op_acc: 100.00%] [G loss: 8.680848] time: 0:35:04.578806\n",
      "[Epoch 1/30] [Batch 140/816] [D loss: 0.332955, acc:  49%, op_acc: 100.00%] [G loss: 9.392595] time: 0:35:06.516005\n",
      "[Epoch 1/30] [Batch 141/816] [D loss: 1.704275, acc:  55%, op_acc: 90.00%] [G loss: 8.607059] time: 0:35:08.449763\n",
      "[Epoch 1/30] [Batch 142/816] [D loss: 1.802110, acc:  57%, op_acc: 90.00%] [G loss: 11.099471] time: 0:35:10.363072\n",
      "[Epoch 1/30] [Batch 143/816] [D loss: 0.277688, acc:  49%, op_acc: 100.00%] [G loss: 8.769406] time: 0:35:12.298378\n",
      "[Epoch 1/30] [Batch 144/816] [D loss: 1.143302, acc:  65%, op_acc: 95.00%] [G loss: 11.330644] time: 0:35:14.239134\n",
      "[Epoch 1/30] [Batch 145/816] [D loss: 0.220422, acc:  70%, op_acc: 100.00%] [G loss: 10.755526] time: 0:35:16.178807\n",
      "[Epoch 1/30] [Batch 146/816] [D loss: 0.250257, acc:  69%, op_acc: 100.00%] [G loss: 8.349007] time: 0:35:18.114503\n",
      "[Epoch 1/30] [Batch 147/816] [D loss: 0.238130, acc:  71%, op_acc: 100.00%] [G loss: 11.899646] time: 0:35:20.049438\n",
      "[Epoch 1/30] [Batch 148/816] [D loss: 2.223493, acc:  75%, op_acc: 80.00%] [G loss: 8.668788] time: 0:35:21.983133\n",
      "[Epoch 1/30] [Batch 149/816] [D loss: 0.236202, acc:  65%, op_acc: 100.00%] [G loss: 8.435285] time: 0:35:23.916785\n",
      "[Epoch 1/30] [Batch 150/816] [D loss: 0.314034, acc:  77%, op_acc: 95.00%] [G loss: 11.128178] time: 0:35:25.848743\n",
      "[Epoch 1/30] [Batch 151/816] [D loss: 0.253365, acc:  55%, op_acc: 100.00%] [G loss: 7.381825] time: 0:35:27.777756\n",
      "[Epoch 1/30] [Batch 152/816] [D loss: 0.202620, acc:  71%, op_acc: 100.00%] [G loss: 9.868654] time: 0:35:29.705647\n",
      "[Epoch 1/30] [Batch 153/816] [D loss: 0.227111, acc:  70%, op_acc: 100.00%] [G loss: 12.783137] time: 0:35:31.637194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/30] [Batch 154/816] [D loss: 7.042614, acc:  50%, op_acc: 85.00%] [G loss: 13.750874] time: 0:35:33.569287\n",
      "[Epoch 1/30] [Batch 155/816] [D loss: 1.054323, acc:  63%, op_acc: 95.00%] [G loss: 11.650348] time: 0:35:35.508054\n",
      "[Epoch 1/30] [Batch 156/816] [D loss: 1.051809, acc:  59%, op_acc: 95.00%] [G loss: 13.283585] time: 0:35:37.449755\n",
      "[Epoch 1/30] [Batch 157/816] [D loss: 3.097394, acc:  68%, op_acc: 85.00%] [G loss: 10.867459] time: 0:35:39.385720\n",
      "[Epoch 1/30] [Batch 158/816] [D loss: 1.525469, acc:  62%, op_acc: 95.00%] [G loss: 11.565251] time: 0:35:41.302931\n",
      "[Epoch 1/30] [Batch 159/816] [D loss: 0.621110, acc:  45%, op_acc: 95.00%] [G loss: 12.276587] time: 0:35:43.240998\n",
      "[Epoch 1/30] [Batch 160/816] [D loss: 0.288661, acc:  48%, op_acc: 100.00%] [G loss: 10.193601] time: 0:35:45.170731\n",
      "[Epoch 1/30] [Batch 161/816] [D loss: 0.240879, acc:  63%, op_acc: 100.00%] [G loss: 9.190242] time: 0:35:47.105929\n",
      "[Epoch 1/30] [Batch 162/816] [D loss: 1.411242, acc:  73%, op_acc: 95.00%] [G loss: 12.231750] time: 0:35:49.043370\n",
      "[Epoch 1/30] [Batch 163/816] [D loss: 0.271448, acc:  66%, op_acc: 100.00%] [G loss: 9.887564] time: 0:35:50.976403\n",
      "[Epoch 1/30] [Batch 164/816] [D loss: 0.788999, acc:  64%, op_acc: 90.00%] [G loss: 9.549616] time: 0:35:52.905469\n",
      "[Epoch 1/30] [Batch 165/816] [D loss: 0.288046, acc:  64%, op_acc: 100.00%] [G loss: 8.552231] time: 0:35:54.843280\n",
      "[Epoch 1/30] [Batch 166/816] [D loss: 2.637118, acc:  63%, op_acc: 80.00%] [G loss: 7.092186] time: 0:35:56.781549\n",
      "[Epoch 1/30] [Batch 167/816] [D loss: 0.217309, acc:  73%, op_acc: 100.00%] [G loss: 10.568805] time: 0:35:58.701575\n",
      "[Epoch 1/30] [Batch 168/816] [D loss: 0.265577, acc:  58%, op_acc: 100.00%] [G loss: 9.404588] time: 0:36:00.640688\n",
      "[Epoch 1/30] [Batch 169/816] [D loss: 0.248533, acc:  73%, op_acc: 100.00%] [G loss: 8.532489] time: 0:36:02.559351\n",
      "[Epoch 1/30] [Batch 170/816] [D loss: 0.255795, acc:  57%, op_acc: 100.00%] [G loss: 8.668052] time: 0:36:04.490128\n",
      "[Epoch 1/30] [Batch 171/816] [D loss: 0.224106, acc:  73%, op_acc: 100.00%] [G loss: 9.593408] time: 0:36:06.426722\n",
      "[Epoch 1/30] [Batch 172/816] [D loss: 0.589362, acc:  70%, op_acc: 95.00%] [G loss: 8.303753] time: 0:36:08.359900\n",
      "[Epoch 1/30] [Batch 173/816] [D loss: 1.661916, acc:  64%, op_acc: 90.00%] [G loss: 9.173541] time: 0:36:10.295983\n",
      "[Epoch 1/30] [Batch 174/816] [D loss: 0.461897, acc:  65%, op_acc: 90.00%] [G loss: 8.868258] time: 0:36:12.235854\n",
      "[Epoch 1/30] [Batch 175/816] [D loss: 1.477263, acc:  72%, op_acc: 95.00%] [G loss: 9.783743] time: 0:36:14.170111\n",
      "[Epoch 1/30] [Batch 176/816] [D loss: 0.232286, acc:  67%, op_acc: 100.00%] [G loss: 9.350148] time: 0:36:16.109805\n",
      "[Epoch 1/30] [Batch 177/816] [D loss: 1.556315, acc:  70%, op_acc: 95.00%] [G loss: 8.884518] time: 0:36:18.062051\n",
      "[Epoch 1/30] [Batch 178/816] [D loss: 0.256596, acc:  63%, op_acc: 100.00%] [G loss: 9.810088] time: 0:36:19.995570\n",
      "[Epoch 1/30] [Batch 179/816] [D loss: 1.885330, acc:  62%, op_acc: 95.00%] [G loss: 8.852488] time: 0:36:21.934811\n",
      "[Epoch 1/30] [Batch 180/816] [D loss: 0.224025, acc:  65%, op_acc: 100.00%] [G loss: 10.194245] time: 0:36:23.866409\n",
      "[Epoch 1/30] [Batch 181/816] [D loss: 0.193744, acc:  80%, op_acc: 100.00%] [G loss: 7.717434] time: 0:36:25.795630\n",
      "[Epoch 1/30] [Batch 182/816] [D loss: 1.007373, acc:  61%, op_acc: 95.00%] [G loss: 9.520548] time: 0:36:27.735017\n",
      "[Epoch 1/30] [Batch 183/816] [D loss: 0.417007, acc:  64%, op_acc: 95.00%] [G loss: 13.100256] time: 0:36:29.656112\n",
      "[Epoch 1/30] [Batch 184/816] [D loss: 0.257134, acc:  69%, op_acc: 100.00%] [G loss: 10.700255] time: 0:36:31.586699\n",
      "[Epoch 1/30] [Batch 185/816] [D loss: 0.247632, acc:  74%, op_acc: 100.00%] [G loss: 8.890578] time: 0:36:33.521326\n",
      "[Epoch 1/30] [Batch 186/816] [D loss: 0.244850, acc:  59%, op_acc: 100.00%] [G loss: 7.863511] time: 0:36:35.454485\n",
      "[Epoch 1/30] [Batch 187/816] [D loss: 0.468892, acc:  71%, op_acc: 95.00%] [G loss: 10.430534] time: 0:36:37.383599\n",
      "[Epoch 1/30] [Batch 188/816] [D loss: 0.202709, acc:  73%, op_acc: 100.00%] [G loss: 9.910453] time: 0:36:39.323215\n",
      "[Epoch 1/30] [Batch 189/816] [D loss: 0.305834, acc:  75%, op_acc: 95.00%] [G loss: 9.083554] time: 0:36:41.249987\n",
      "[Epoch 1/30] [Batch 190/816] [D loss: 0.418418, acc:  70%, op_acc: 95.00%] [G loss: 9.426683] time: 0:36:43.185672\n",
      "[Epoch 1/30] [Batch 191/816] [D loss: 0.257093, acc:  66%, op_acc: 100.00%] [G loss: 8.504203] time: 0:36:45.121432\n",
      "[Epoch 1/30] [Batch 192/816] [D loss: 1.025502, acc:  79%, op_acc: 95.00%] [G loss: 9.501591] time: 0:36:47.061885\n",
      "[Epoch 1/30] [Batch 193/816] [D loss: 1.789726, acc:  66%, op_acc: 90.00%] [G loss: 10.162524] time: 0:36:48.992993\n",
      "[Epoch 1/30] [Batch 194/816] [D loss: 0.304299, acc:  51%, op_acc: 100.00%] [G loss: 8.255303] time: 0:36:50.919088\n",
      "[Epoch 1/30] [Batch 195/816] [D loss: 0.514403, acc:  61%, op_acc: 90.00%] [G loss: 9.671821] time: 0:36:52.862329\n",
      "[Epoch 1/30] [Batch 196/816] [D loss: 0.300578, acc:  69%, op_acc: 100.00%] [G loss: 7.526460] time: 0:36:54.802918\n",
      "[Epoch 1/30] [Batch 197/816] [D loss: 1.538048, acc:  68%, op_acc: 85.00%] [G loss: 9.412547] time: 0:36:56.734484\n",
      "[Epoch 1/30] [Batch 198/816] [D loss: 0.232307, acc:  69%, op_acc: 100.00%] [G loss: 8.226847] time: 0:36:58.671278\n",
      "[Epoch 1/30] [Batch 199/816] [D loss: 4.196965, acc:  60%, op_acc: 80.00%] [G loss: 11.032171] time: 0:37:00.606012\n",
      "[Epoch 1/30] [Batch 200/816] [D loss: 0.292663, acc:  49%, op_acc: 100.00%] [G loss: 7.944396] time: 0:37:02.539900\n",
      "SAVE\n",
      "1_200-T1-T2.png\n",
      "[Epoch 1/30] [Batch 201/816] [D loss: 1.501547, acc:  55%, op_acc: 95.00%] [G loss: 8.231747] time: 0:37:04.678085\n",
      "[Epoch 1/30] [Batch 202/816] [D loss: 0.709373, acc:  67%, op_acc: 95.00%] [G loss: 8.645637] time: 0:37:06.619821\n",
      "[Epoch 1/30] [Batch 203/816] [D loss: 0.260395, acc:  68%, op_acc: 100.00%] [G loss: 8.539223] time: 0:37:08.559357\n",
      "[Epoch 1/30] [Batch 204/816] [D loss: 0.278259, acc:  65%, op_acc: 100.00%] [G loss: 7.969834] time: 0:37:10.477603\n",
      "[Epoch 1/30] [Batch 205/816] [D loss: 0.724894, acc:  67%, op_acc: 95.00%] [G loss: 8.637115] time: 0:37:12.413217\n",
      "[Epoch 1/30] [Batch 206/816] [D loss: 0.236181, acc:  69%, op_acc: 100.00%] [G loss: 8.389233] time: 0:37:14.349075\n",
      "[Epoch 1/30] [Batch 207/816] [D loss: 0.240714, acc:  61%, op_acc: 100.00%] [G loss: 8.054190] time: 0:37:16.285394\n",
      "[Epoch 1/30] [Batch 208/816] [D loss: 0.231182, acc:  67%, op_acc: 100.00%] [G loss: 8.317819] time: 0:37:18.231189\n",
      "[Epoch 1/30] [Batch 209/816] [D loss: 0.614635, acc:  70%, op_acc: 95.00%] [G loss: 8.938892] time: 0:37:20.147908\n",
      "[Epoch 1/30] [Batch 210/816] [D loss: 2.189758, acc:  54%, op_acc: 90.00%] [G loss: 18.035036] time: 0:37:22.071636\n",
      "[Epoch 1/30] [Batch 211/816] [D loss: 3.361382, acc:  55%, op_acc: 85.00%] [G loss: 14.106299] time: 0:37:24.015969\n",
      "[Epoch 1/30] [Batch 212/816] [D loss: 0.470761, acc:  71%, op_acc: 95.00%] [G loss: 12.959713] time: 0:37:25.956054\n",
      "[Epoch 1/30] [Batch 213/816] [D loss: 1.875738, acc:  51%, op_acc: 95.00%] [G loss: 10.196023] time: 0:37:27.892545\n",
      "[Epoch 1/30] [Batch 214/816] [D loss: 0.663536, acc:  72%, op_acc: 95.00%] [G loss: 10.351604] time: 0:37:29.828455\n",
      "[Epoch 1/30] [Batch 215/816] [D loss: 0.229335, acc:  63%, op_acc: 100.00%] [G loss: 7.468280] time: 0:37:31.760485\n",
      "[Epoch 1/30] [Batch 216/816] [D loss: 1.704255, acc:  64%, op_acc: 95.00%] [G loss: 10.753901] time: 0:37:33.691810\n",
      "[Epoch 1/30] [Batch 217/816] [D loss: 0.258032, acc:  74%, op_acc: 100.00%] [G loss: 8.853832] time: 0:37:35.630027\n",
      "[Epoch 1/30] [Batch 218/816] [D loss: 0.304239, acc:  57%, op_acc: 100.00%] [G loss: 9.480964] time: 0:37:37.567877\n",
      "[Epoch 1/30] [Batch 219/816] [D loss: 0.787726, acc:  62%, op_acc: 95.00%] [G loss: 7.373670] time: 0:37:39.502510\n",
      "[Epoch 1/30] [Batch 220/816] [D loss: 0.255003, acc:  67%, op_acc: 100.00%] [G loss: 8.153561] time: 0:37:41.438427\n",
      "[Epoch 1/30] [Batch 221/816] [D loss: 0.239461, acc:  57%, op_acc: 100.00%] [G loss: 7.848783] time: 0:37:43.382720\n",
      "[Epoch 1/30] [Batch 222/816] [D loss: 0.177867, acc:  82%, op_acc: 100.00%] [G loss: 8.239479] time: 0:37:45.321395\n",
      "[Epoch 1/30] [Batch 223/816] [D loss: 0.184220, acc:  78%, op_acc: 100.00%] [G loss: 10.655782] time: 0:37:47.262221\n",
      "[Epoch 1/30] [Batch 224/816] [D loss: 0.216291, acc:  74%, op_acc: 100.00%] [G loss: 7.975493] time: 0:37:49.201319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/30] [Batch 225/816] [D loss: 0.216770, acc:  72%, op_acc: 100.00%] [G loss: 8.195406] time: 0:37:51.140492\n",
      "[Epoch 1/30] [Batch 226/816] [D loss: 1.062746, acc:  71%, op_acc: 85.00%] [G loss: 8.969174] time: 0:37:53.071944\n",
      "[Epoch 1/30] [Batch 227/816] [D loss: 1.004376, acc:  68%, op_acc: 95.00%] [G loss: 8.402278] time: 0:37:54.994236\n",
      "[Epoch 1/30] [Batch 228/816] [D loss: 0.296583, acc:  75%, op_acc: 95.00%] [G loss: 8.947510] time: 0:37:56.929898\n",
      "[Epoch 1/30] [Batch 229/816] [D loss: 0.655010, acc:  69%, op_acc: 95.00%] [G loss: 7.828227] time: 0:37:58.869284\n",
      "[Epoch 1/30] [Batch 230/816] [D loss: 0.512020, acc:  64%, op_acc: 95.00%] [G loss: 8.175549] time: 0:38:00.800054\n",
      "[Epoch 1/30] [Batch 231/816] [D loss: 0.191720, acc:  73%, op_acc: 100.00%] [G loss: 8.944428] time: 0:38:02.741617\n",
      "[Epoch 1/30] [Batch 232/816] [D loss: 1.478938, acc:  77%, op_acc: 95.00%] [G loss: 8.943254] time: 0:38:04.678808\n",
      "[Epoch 1/30] [Batch 233/816] [D loss: 2.860786, acc:  59%, op_acc: 90.00%] [G loss: 8.336141] time: 0:38:06.612288\n",
      "[Epoch 1/30] [Batch 234/816] [D loss: 0.154565, acc:  82%, op_acc: 100.00%] [G loss: 10.421378] time: 0:38:08.547467\n",
      "[Epoch 1/30] [Batch 235/816] [D loss: 0.247674, acc:  62%, op_acc: 100.00%] [G loss: 10.591442] time: 0:38:10.487535\n",
      "[Epoch 1/30] [Batch 236/816] [D loss: 0.209706, acc:  76%, op_acc: 100.00%] [G loss: 9.076747] time: 0:38:12.424812\n",
      "[Epoch 1/30] [Batch 237/816] [D loss: 0.498523, acc:  76%, op_acc: 95.00%] [G loss: 8.406797] time: 0:38:14.367037\n",
      "[Epoch 1/30] [Batch 238/816] [D loss: 0.518474, acc:  76%, op_acc: 95.00%] [G loss: 8.468954] time: 0:38:16.311397\n",
      "[Epoch 1/30] [Batch 239/816] [D loss: 0.204134, acc:  71%, op_acc: 100.00%] [G loss: 9.110430] time: 0:38:18.249149\n",
      "[Epoch 1/30] [Batch 240/816] [D loss: 0.203415, acc:  81%, op_acc: 100.00%] [G loss: 7.353802] time: 0:38:20.176321\n",
      "[Epoch 1/30] [Batch 241/816] [D loss: 0.226371, acc:  82%, op_acc: 95.00%] [G loss: 9.191974] time: 0:38:22.105161\n",
      "[Epoch 1/30] [Batch 242/816] [D loss: 0.196711, acc:  72%, op_acc: 100.00%] [G loss: 9.370243] time: 0:38:24.044331\n",
      "[Epoch 1/30] [Batch 243/816] [D loss: 0.183556, acc:  80%, op_acc: 100.00%] [G loss: 8.893345] time: 0:38:25.982215\n",
      "[Epoch 1/30] [Batch 244/816] [D loss: 0.338083, acc:  68%, op_acc: 95.00%] [G loss: 9.697194] time: 0:38:27.923502\n",
      "[Epoch 1/30] [Batch 245/816] [D loss: 0.256028, acc:  64%, op_acc: 100.00%] [G loss: 8.752530] time: 0:38:29.874184\n",
      "[Epoch 1/30] [Batch 246/816] [D loss: 0.170368, acc:  80%, op_acc: 100.00%] [G loss: 9.691968] time: 0:38:31.817168\n",
      "[Epoch 1/30] [Batch 247/816] [D loss: 0.170002, acc:  70%, op_acc: 100.00%] [G loss: 8.250150] time: 0:38:33.758499\n",
      "[Epoch 1/30] [Batch 248/816] [D loss: 2.299856, acc:  82%, op_acc: 90.00%] [G loss: 9.328455] time: 0:38:35.695782\n",
      "[Epoch 1/30] [Batch 249/816] [D loss: 0.234331, acc:  72%, op_acc: 100.00%] [G loss: 9.079796] time: 0:38:37.633767\n",
      "[Epoch 1/30] [Batch 250/816] [D loss: 0.196142, acc:  71%, op_acc: 100.00%] [G loss: 8.913967] time: 0:38:39.570877\n",
      "[Epoch 1/30] [Batch 251/816] [D loss: 0.156519, acc:  80%, op_acc: 100.00%] [G loss: 9.475181] time: 0:38:41.510542\n",
      "[Epoch 1/30] [Batch 252/816] [D loss: 0.284211, acc:  69%, op_acc: 100.00%] [G loss: 7.521925] time: 0:38:43.436408\n",
      "[Epoch 1/30] [Batch 253/816] [D loss: 1.191885, acc:  68%, op_acc: 95.00%] [G loss: 9.735061] time: 0:38:45.368576\n",
      "[Epoch 1/30] [Batch 254/816] [D loss: 0.188900, acc:  80%, op_acc: 100.00%] [G loss: 9.056404] time: 0:38:47.300129\n",
      "[Epoch 1/30] [Batch 255/816] [D loss: 0.205678, acc:  65%, op_acc: 100.00%] [G loss: 7.441956] time: 0:38:49.238992\n",
      "[Epoch 1/30] [Batch 256/816] [D loss: 0.223800, acc:  65%, op_acc: 100.00%] [G loss: 9.328216] time: 0:38:51.185663\n",
      "[Epoch 1/30] [Batch 257/816] [D loss: 0.185779, acc:  82%, op_acc: 100.00%] [G loss: 8.367197] time: 0:38:53.131148\n",
      "[Epoch 1/30] [Batch 258/816] [D loss: 0.194840, acc:  74%, op_acc: 100.00%] [G loss: 8.944809] time: 0:38:55.069634\n",
      "[Epoch 1/30] [Batch 259/816] [D loss: 0.192046, acc:  78%, op_acc: 100.00%] [G loss: 9.399526] time: 0:38:57.011591\n",
      "[Epoch 1/30] [Batch 260/816] [D loss: 0.167038, acc:  79%, op_acc: 100.00%] [G loss: 9.095771] time: 0:38:58.947907\n",
      "[Epoch 1/30] [Batch 261/816] [D loss: 0.272976, acc:  84%, op_acc: 95.00%] [G loss: 12.332725] time: 0:39:00.880846\n",
      "[Epoch 1/30] [Batch 262/816] [D loss: 0.201142, acc:  72%, op_acc: 100.00%] [G loss: 9.692513] time: 0:39:02.809282\n",
      "[Epoch 1/30] [Batch 263/816] [D loss: 0.361831, acc:  57%, op_acc: 100.00%] [G loss: 7.063822] time: 0:39:04.747097\n",
      "[Epoch 1/30] [Batch 264/816] [D loss: 0.190312, acc:  78%, op_acc: 100.00%] [G loss: 8.658704] time: 0:39:06.687827\n",
      "[Epoch 1/30] [Batch 265/816] [D loss: 0.164878, acc:  82%, op_acc: 100.00%] [G loss: 10.920681] time: 0:39:08.626627\n",
      "[Epoch 1/30] [Batch 266/816] [D loss: 0.225448, acc:  80%, op_acc: 95.00%] [G loss: 10.934379] time: 0:39:10.559400\n",
      "[Epoch 1/30] [Batch 267/816] [D loss: 0.426416, acc:  84%, op_acc: 95.00%] [G loss: 8.551730] time: 0:39:12.497973\n",
      "[Epoch 1/30] [Batch 268/816] [D loss: 1.167974, acc:  83%, op_acc: 95.00%] [G loss: 9.507184] time: 0:39:14.436065\n",
      "[Epoch 1/30] [Batch 269/816] [D loss: 1.674024, acc:  69%, op_acc: 90.00%] [G loss: 8.310422] time: 0:39:16.360048\n",
      "[Epoch 1/30] [Batch 270/816] [D loss: 0.253005, acc:  55%, op_acc: 100.00%] [G loss: 7.803006] time: 0:39:18.284643\n",
      "[Epoch 1/30] [Batch 271/816] [D loss: 0.260853, acc:  53%, op_acc: 100.00%] [G loss: 8.669440] time: 0:39:20.216533\n",
      "[Epoch 1/30] [Batch 272/816] [D loss: 1.426424, acc:  65%, op_acc: 95.00%] [G loss: 9.174958] time: 0:39:22.155920\n",
      "[Epoch 1/30] [Batch 273/816] [D loss: 0.237229, acc:  69%, op_acc: 100.00%] [G loss: 8.632593] time: 0:39:24.098223\n",
      "[Epoch 1/30] [Batch 274/816] [D loss: 0.221008, acc:  67%, op_acc: 100.00%] [G loss: 8.300646] time: 0:39:26.031440\n",
      "[Epoch 1/30] [Batch 275/816] [D loss: 0.220978, acc:  69%, op_acc: 100.00%] [G loss: 7.796673] time: 0:39:27.972321\n",
      "[Epoch 1/30] [Batch 276/816] [D loss: 1.516823, acc:  70%, op_acc: 95.00%] [G loss: 7.894737] time: 0:39:29.910727\n",
      "[Epoch 1/30] [Batch 277/816] [D loss: 0.262176, acc:  53%, op_acc: 100.00%] [G loss: 8.471229] time: 0:39:31.846356\n",
      "[Epoch 1/30] [Batch 278/816] [D loss: 1.580006, acc:  83%, op_acc: 95.00%] [G loss: 10.920975] time: 0:39:33.786670\n",
      "[Epoch 1/30] [Batch 279/816] [D loss: 0.237559, acc:  57%, op_acc: 100.00%] [G loss: 8.244164] time: 0:39:35.724331\n",
      "[Epoch 1/30] [Batch 280/816] [D loss: 0.280163, acc:  72%, op_acc: 100.00%] [G loss: 8.375926] time: 0:39:37.656702\n",
      "[Epoch 1/30] [Batch 281/816] [D loss: 0.240847, acc:  73%, op_acc: 100.00%] [G loss: 9.850450] time: 0:39:39.593634\n",
      "[Epoch 1/30] [Batch 282/816] [D loss: 1.707012, acc:  79%, op_acc: 95.00%] [G loss: 9.171494] time: 0:39:41.529806\n",
      "[Epoch 1/30] [Batch 283/816] [D loss: 0.219274, acc:  65%, op_acc: 100.00%] [G loss: 8.325588] time: 0:39:43.474688\n",
      "[Epoch 1/30] [Batch 284/816] [D loss: 0.240817, acc:  59%, op_acc: 100.00%] [G loss: 7.401749] time: 0:39:45.411902\n",
      "[Epoch 1/30] [Batch 285/816] [D loss: 0.141504, acc:  82%, op_acc: 100.00%] [G loss: 8.663181] time: 0:39:47.348447\n",
      "[Epoch 1/30] [Batch 286/816] [D loss: 0.230495, acc:  62%, op_acc: 100.00%] [G loss: 7.164358] time: 0:39:49.278036\n",
      "[Epoch 1/30] [Batch 287/816] [D loss: 0.645440, acc:  75%, op_acc: 95.00%] [G loss: 11.210382] time: 0:39:51.212319\n",
      "[Epoch 1/30] [Batch 288/816] [D loss: 0.165857, acc:  81%, op_acc: 100.00%] [G loss: 10.722866] time: 0:39:53.153203\n",
      "[Epoch 1/30] [Batch 289/816] [D loss: 0.580212, acc:  70%, op_acc: 85.00%] [G loss: 6.939547] time: 0:39:55.090034\n",
      "[Epoch 1/30] [Batch 290/816] [D loss: 0.298001, acc:  79%, op_acc: 95.00%] [G loss: 7.916233] time: 0:39:57.019382\n",
      "[Epoch 1/30] [Batch 291/816] [D loss: 0.599437, acc:  62%, op_acc: 95.00%] [G loss: 8.247545] time: 0:39:58.958236\n",
      "[Epoch 1/30] [Batch 292/816] [D loss: 0.206210, acc:  83%, op_acc: 100.00%] [G loss: 10.398025] time: 0:40:00.909557\n",
      "[Epoch 1/30] [Batch 293/816] [D loss: 0.204335, acc:  80%, op_acc: 100.00%] [G loss: 8.688972] time: 0:40:03.024187\n",
      "[Epoch 1/30] [Batch 294/816] [D loss: 0.189888, acc:  81%, op_acc: 100.00%] [G loss: 9.394434] time: 0:40:04.965006\n",
      "[Epoch 1/30] [Batch 295/816] [D loss: 0.189862, acc:  83%, op_acc: 100.00%] [G loss: 10.723674] time: 0:40:06.897794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/30] [Batch 296/816] [D loss: 0.512725, acc:  87%, op_acc: 90.00%] [G loss: 8.447756] time: 0:40:08.819966\n",
      "[Epoch 1/30] [Batch 297/816] [D loss: 0.148578, acc:  82%, op_acc: 100.00%] [G loss: 8.245149] time: 0:40:10.747530\n",
      "[Epoch 1/30] [Batch 298/816] [D loss: 1.608588, acc:  74%, op_acc: 90.00%] [G loss: 8.088674] time: 0:40:12.677579\n",
      "[Epoch 1/30] [Batch 299/816] [D loss: 0.384356, acc:  67%, op_acc: 100.00%] [G loss: 8.937245] time: 0:40:14.601611\n",
      "[Epoch 1/30] [Batch 300/816] [D loss: 0.802766, acc:  64%, op_acc: 95.00%] [G loss: 10.524908] time: 0:40:16.547494\n",
      "[Epoch 1/30] [Batch 301/816] [D loss: 0.249485, acc:  60%, op_acc: 100.00%] [G loss: 9.344266] time: 0:40:18.487115\n",
      "[Epoch 1/30] [Batch 302/816] [D loss: 0.235847, acc:  67%, op_acc: 100.00%] [G loss: 10.302162] time: 0:40:20.426461\n",
      "[Epoch 1/30] [Batch 303/816] [D loss: 0.225105, acc:  64%, op_acc: 100.00%] [G loss: 8.116152] time: 0:40:22.361299\n",
      "[Epoch 1/30] [Batch 304/816] [D loss: 0.738235, acc:  79%, op_acc: 90.00%] [G loss: 8.696387] time: 0:40:24.298763\n",
      "[Epoch 1/30] [Batch 305/816] [D loss: 0.587915, acc:  80%, op_acc: 95.00%] [G loss: 8.285152] time: 0:40:26.236221\n",
      "[Epoch 1/30] [Batch 306/816] [D loss: 0.225892, acc:  70%, op_acc: 100.00%] [G loss: 8.529008] time: 0:40:28.159767\n",
      "[Epoch 1/30] [Batch 307/816] [D loss: 0.231297, acc:  81%, op_acc: 100.00%] [G loss: 12.273321] time: 0:40:30.091596\n",
      "[Epoch 1/30] [Batch 308/816] [D loss: 0.784143, acc:  50%, op_acc: 95.00%] [G loss: 9.128432] time: 0:40:32.027966\n",
      "[Epoch 1/30] [Batch 309/816] [D loss: 0.796450, acc:  68%, op_acc: 95.00%] [G loss: 7.390035] time: 0:40:33.953484\n",
      "[Epoch 1/30] [Batch 310/816] [D loss: 0.149766, acc:  85%, op_acc: 100.00%] [G loss: 8.247381] time: 0:40:35.887943\n",
      "[Epoch 1/30] [Batch 311/816] [D loss: 0.190008, acc:  83%, op_acc: 100.00%] [G loss: 10.273335] time: 0:40:37.816913\n",
      "[Epoch 1/30] [Batch 312/816] [D loss: 0.177038, acc:  81%, op_acc: 100.00%] [G loss: 9.924720] time: 0:40:39.733528\n",
      "[Epoch 1/30] [Batch 313/816] [D loss: 0.338148, acc:  81%, op_acc: 95.00%] [G loss: 9.229070] time: 0:40:41.662659\n",
      "[Epoch 1/30] [Batch 314/816] [D loss: 2.164888, acc:  64%, op_acc: 90.00%] [G loss: 8.791202] time: 0:40:43.587295\n",
      "[Epoch 1/30] [Batch 315/816] [D loss: 0.416735, acc:  55%, op_acc: 95.00%] [G loss: 7.708818] time: 0:40:45.514131\n",
      "[Epoch 1/30] [Batch 316/816] [D loss: 0.231517, acc:  72%, op_acc: 100.00%] [G loss: 8.721693] time: 0:40:47.442092\n",
      "[Epoch 1/30] [Batch 317/816] [D loss: 0.257379, acc:  72%, op_acc: 100.00%] [G loss: 7.369048] time: 0:40:49.373587\n",
      "[Epoch 1/30] [Batch 318/816] [D loss: 0.227453, acc:  61%, op_acc: 100.00%] [G loss: 8.566134] time: 0:40:51.297381\n",
      "[Epoch 1/30] [Batch 319/816] [D loss: 0.657430, acc:  68%, op_acc: 95.00%] [G loss: 7.974135] time: 0:40:53.219988\n",
      "[Epoch 1/30] [Batch 320/816] [D loss: 0.731767, acc:  75%, op_acc: 85.00%] [G loss: 8.579486] time: 0:40:55.158288\n",
      "[Epoch 1/30] [Batch 321/816] [D loss: 0.357374, acc:  56%, op_acc: 100.00%] [G loss: 9.437309] time: 0:40:57.093633\n",
      "[Epoch 1/30] [Batch 322/816] [D loss: 0.582645, acc:  67%, op_acc: 95.00%] [G loss: 9.495518] time: 0:40:59.021660\n",
      "[Epoch 1/30] [Batch 323/816] [D loss: 0.303905, acc:  64%, op_acc: 100.00%] [G loss: 9.216092] time: 0:41:00.963074\n",
      "[Epoch 1/30] [Batch 324/816] [D loss: 0.265443, acc:  68%, op_acc: 100.00%] [G loss: 9.210725] time: 0:41:02.896411\n",
      "[Epoch 1/30] [Batch 325/816] [D loss: 1.615108, acc:  54%, op_acc: 90.00%] [G loss: 5.778457] time: 0:41:04.819334\n",
      "[Epoch 1/30] [Batch 326/816] [D loss: 0.113003, acc:  89%, op_acc: 100.00%] [G loss: 11.376475] time: 0:41:06.747762\n",
      "[Epoch 1/30] [Batch 327/816] [D loss: 1.461860, acc:  80%, op_acc: 85.00%] [G loss: 9.366755] time: 0:41:08.677949\n",
      "[Epoch 1/30] [Batch 328/816] [D loss: 0.244398, acc:  84%, op_acc: 100.00%] [G loss: 10.400358] time: 0:41:10.606267\n",
      "[Epoch 1/30] [Batch 329/816] [D loss: 0.193662, acc:  82%, op_acc: 100.00%] [G loss: 9.756695] time: 0:41:12.535105\n",
      "[Epoch 1/30] [Batch 330/816] [D loss: 0.209128, acc:  66%, op_acc: 100.00%] [G loss: 8.210882] time: 0:41:14.468805\n",
      "[Epoch 1/30] [Batch 331/816] [D loss: 0.215574, acc:  66%, op_acc: 100.00%] [G loss: 8.520494] time: 0:41:16.406272\n",
      "[Epoch 1/30] [Batch 332/816] [D loss: 0.173145, acc:  84%, op_acc: 100.00%] [G loss: 7.583764] time: 0:41:18.335324\n",
      "[Epoch 1/30] [Batch 333/816] [D loss: 3.589669, acc:  83%, op_acc: 95.00%] [G loss: 11.060837] time: 0:41:20.263121\n",
      "[Epoch 1/30] [Batch 334/816] [D loss: 0.974303, acc:  76%, op_acc: 95.00%] [G loss: 11.780695] time: 0:41:22.190618\n",
      "[Epoch 1/30] [Batch 335/816] [D loss: 1.842762, acc:  70%, op_acc: 85.00%] [G loss: 8.663912] time: 0:41:24.104629\n",
      "[Epoch 1/30] [Batch 336/816] [D loss: 1.265751, acc:  80%, op_acc: 95.00%] [G loss: 10.318592] time: 0:41:26.045726\n",
      "[Epoch 1/30] [Batch 337/816] [D loss: 0.393641, acc:  63%, op_acc: 90.00%] [G loss: 9.400025] time: 0:41:27.990671\n",
      "[Epoch 1/30] [Batch 338/816] [D loss: 2.635214, acc:  68%, op_acc: 90.00%] [G loss: 9.461620] time: 0:41:29.922025\n",
      "[Epoch 1/30] [Batch 339/816] [D loss: 1.565330, acc:  64%, op_acc: 85.00%] [G loss: 7.714255] time: 0:41:31.848644\n",
      "[Epoch 1/30] [Batch 340/816] [D loss: 0.293890, acc:  81%, op_acc: 100.00%] [G loss: 8.748955] time: 0:41:33.770877\n",
      "[Epoch 1/30] [Batch 341/816] [D loss: 0.235022, acc:  62%, op_acc: 100.00%] [G loss: 8.591136] time: 0:41:35.700103\n",
      "[Epoch 1/30] [Batch 342/816] [D loss: 0.317762, acc:  80%, op_acc: 95.00%] [G loss: 9.113859] time: 0:41:37.628594\n",
      "[Epoch 1/30] [Batch 343/816] [D loss: 0.279720, acc:  71%, op_acc: 95.00%] [G loss: 8.641570] time: 0:41:39.565246\n",
      "[Epoch 1/30] [Batch 344/816] [D loss: 0.711867, acc:  74%, op_acc: 95.00%] [G loss: 8.900438] time: 0:41:41.490489\n",
      "[Epoch 1/30] [Batch 345/816] [D loss: 0.217812, acc:  77%, op_acc: 100.00%] [G loss: 8.265044] time: 0:41:43.428469\n",
      "[Epoch 1/30] [Batch 346/816] [D loss: 0.209502, acc:  72%, op_acc: 100.00%] [G loss: 7.664193] time: 0:41:45.365967\n",
      "[Epoch 1/30] [Batch 347/816] [D loss: 0.194008, acc:  72%, op_acc: 100.00%] [G loss: 8.096961] time: 0:41:47.303376\n",
      "[Epoch 1/30] [Batch 348/816] [D loss: 0.469187, acc:  73%, op_acc: 95.00%] [G loss: 8.894479] time: 0:41:49.235113\n",
      "[Epoch 1/30] [Batch 349/816] [D loss: 0.560561, acc:  60%, op_acc: 95.00%] [G loss: 8.396075] time: 0:41:51.163356\n",
      "[Epoch 1/30] [Batch 350/816] [D loss: 0.235251, acc:  61%, op_acc: 100.00%] [G loss: 8.484991] time: 0:41:53.100270\n",
      "[Epoch 1/30] [Batch 351/816] [D loss: 0.183492, acc:  79%, op_acc: 100.00%] [G loss: 8.192486] time: 0:41:55.036171\n",
      "[Epoch 1/30] [Batch 352/816] [D loss: 0.167871, acc:  83%, op_acc: 100.00%] [G loss: 7.786855] time: 0:41:56.969416\n",
      "[Epoch 1/30] [Batch 353/816] [D loss: 0.184951, acc:  86%, op_acc: 100.00%] [G loss: 9.987348] time: 0:41:58.905459\n",
      "[Epoch 1/30] [Batch 354/816] [D loss: 0.139476, acc:  89%, op_acc: 100.00%] [G loss: 8.308329] time: 0:42:00.845057\n",
      "[Epoch 1/30] [Batch 355/816] [D loss: 0.841837, acc:  82%, op_acc: 95.00%] [G loss: 8.036750] time: 0:42:02.785028\n",
      "[Epoch 1/30] [Batch 356/816] [D loss: 5.914242, acc:  61%, op_acc: 90.00%] [G loss: 19.020031] time: 0:42:04.717404\n",
      "[Epoch 1/30] [Batch 357/816] [D loss: 1.153616, acc:  49%, op_acc: 95.00%] [G loss: 12.227066] time: 0:42:06.645536\n",
      "[Epoch 1/30] [Batch 358/816] [D loss: 1.386333, acc:  60%, op_acc: 95.00%] [G loss: 12.187761] time: 0:42:08.561170\n",
      "[Epoch 1/30] [Batch 359/816] [D loss: 1.002348, acc:  61%, op_acc: 100.00%] [G loss: 12.087799] time: 0:42:10.490846\n",
      "[Epoch 1/30] [Batch 360/816] [D loss: 6.517118, acc:  43%, op_acc: 80.00%] [G loss: 12.084981] time: 0:42:12.414712\n",
      "[Epoch 1/30] [Batch 361/816] [D loss: 0.203910, acc:  79%, op_acc: 100.00%] [G loss: 8.724501] time: 0:42:14.348148\n",
      "[Epoch 1/30] [Batch 362/816] [D loss: 0.295749, acc:  68%, op_acc: 95.00%] [G loss: 10.292388] time: 0:42:16.278074\n",
      "[Epoch 1/30] [Batch 363/816] [D loss: 2.452094, acc:  70%, op_acc: 85.00%] [G loss: 10.689454] time: 0:42:18.219108\n",
      "[Epoch 1/30] [Batch 364/816] [D loss: 0.301264, acc:  61%, op_acc: 100.00%] [G loss: 8.569311] time: 0:42:20.141924\n",
      "[Epoch 1/30] [Batch 365/816] [D loss: 0.513134, acc:  74%, op_acc: 95.00%] [G loss: 10.638887] time: 0:42:22.064702\n",
      "[Epoch 1/30] [Batch 366/816] [D loss: 0.757975, acc:  72%, op_acc: 95.00%] [G loss: 9.591397] time: 0:42:24.002160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/30] [Batch 367/816] [D loss: 0.700608, acc:  74%, op_acc: 95.00%] [G loss: 7.956904] time: 0:42:25.940736\n",
      "[Epoch 1/30] [Batch 368/816] [D loss: 4.021754, acc:  72%, op_acc: 80.00%] [G loss: 42.772957] time: 0:42:27.875361\n",
      "[Epoch 1/30] [Batch 369/816] [D loss: 0.499031, acc:  53%, op_acc: 95.00%] [G loss: 25.726151] time: 0:42:29.805980\n",
      "[Epoch 1/30] [Batch 370/816] [D loss: 0.379930, acc:  37%, op_acc: 100.00%] [G loss: 22.873354] time: 0:42:31.748440\n",
      "[Epoch 1/30] [Batch 371/816] [D loss: 2.768995, acc:  53%, op_acc: 85.00%] [G loss: 18.398289] time: 0:42:33.687130\n",
      "[Epoch 1/30] [Batch 372/816] [D loss: 0.870654, acc:  41%, op_acc: 90.00%] [G loss: 13.571785] time: 0:42:35.625897\n",
      "[Epoch 1/30] [Batch 373/816] [D loss: 5.440298, acc:  51%, op_acc: 85.00%] [G loss: 11.174944] time: 0:42:37.550744\n",
      "[Epoch 1/30] [Batch 374/816] [D loss: 0.295783, acc:  59%, op_acc: 100.00%] [G loss: 11.210739] time: 0:42:39.472653\n",
      "[Epoch 1/30] [Batch 375/816] [D loss: 0.261472, acc:  62%, op_acc: 100.00%] [G loss: 9.802703] time: 0:42:41.397398\n",
      "[Epoch 1/30] [Batch 376/816] [D loss: 0.832267, acc:  58%, op_acc: 95.00%] [G loss: 11.843161] time: 0:42:43.334857\n",
      "[Epoch 1/30] [Batch 377/816] [D loss: 0.231240, acc:  65%, op_acc: 100.00%] [G loss: 10.866093] time: 0:42:45.268301\n",
      "[Epoch 1/30] [Batch 378/816] [D loss: 0.590093, acc:  54%, op_acc: 95.00%] [G loss: 10.329594] time: 0:42:47.206668\n",
      "[Epoch 1/30] [Batch 379/816] [D loss: 1.595474, acc:  60%, op_acc: 90.00%] [G loss: 7.945032] time: 0:42:49.138358\n",
      "[Epoch 1/30] [Batch 380/816] [D loss: 2.033898, acc:  62%, op_acc: 95.00%] [G loss: 9.839870] time: 0:42:51.080783\n",
      "[Epoch 1/30] [Batch 381/816] [D loss: 0.242430, acc:  67%, op_acc: 100.00%] [G loss: 9.294956] time: 0:42:53.010894\n",
      "[Epoch 1/30] [Batch 382/816] [D loss: 0.253817, acc:  78%, op_acc: 95.00%] [G loss: 10.922173] time: 0:42:54.941332\n",
      "[Epoch 1/30] [Batch 383/816] [D loss: 0.329548, acc:  57%, op_acc: 95.00%] [G loss: 9.468961] time: 0:42:56.877724\n",
      "[Epoch 1/30] [Batch 384/816] [D loss: 1.011532, acc:  85%, op_acc: 95.00%] [G loss: 9.008296] time: 0:42:58.803307\n",
      "[Epoch 1/30] [Batch 385/816] [D loss: 0.170187, acc:  79%, op_acc: 100.00%] [G loss: 15.098190] time: 0:43:00.736290\n",
      "[Epoch 1/30] [Batch 386/816] [D loss: 0.303545, acc:  51%, op_acc: 100.00%] [G loss: 11.957144] time: 0:43:02.673745\n",
      "[Epoch 1/30] [Batch 387/816] [D loss: 0.928761, acc:  46%, op_acc: 90.00%] [G loss: 9.917727] time: 0:43:04.602154\n",
      "[Epoch 1/30] [Batch 388/816] [D loss: 0.224829, acc:  66%, op_acc: 100.00%] [G loss: 9.399545] time: 0:43:06.540882\n",
      "[Epoch 1/30] [Batch 389/816] [D loss: 0.215494, acc:  61%, op_acc: 100.00%] [G loss: 10.112039] time: 0:43:08.486180\n",
      "[Epoch 1/30] [Batch 390/816] [D loss: 0.270600, acc:  74%, op_acc: 100.00%] [G loss: 10.113797] time: 0:43:10.426456\n",
      "[Epoch 1/30] [Batch 391/816] [D loss: 0.387035, acc:  67%, op_acc: 95.00%] [G loss: 10.046449] time: 0:43:12.368256\n",
      "[Epoch 1/30] [Batch 392/816] [D loss: 0.219196, acc:  75%, op_acc: 100.00%] [G loss: 7.948972] time: 0:43:14.303287\n",
      "[Epoch 1/30] [Batch 393/816] [D loss: 0.227905, acc:  61%, op_acc: 100.00%] [G loss: 8.547594] time: 0:43:16.225666\n",
      "[Epoch 1/30] [Batch 394/816] [D loss: 0.278953, acc:  45%, op_acc: 100.00%] [G loss: 8.282923] time: 0:43:18.156851\n",
      "[Epoch 1/30] [Batch 395/816] [D loss: 0.435943, acc:  77%, op_acc: 95.00%] [G loss: 8.169750] time: 0:43:20.086573\n",
      "[Epoch 1/30] [Batch 396/816] [D loss: 0.228746, acc:  61%, op_acc: 100.00%] [G loss: 8.529569] time: 0:43:22.025121\n",
      "[Epoch 1/30] [Batch 397/816] [D loss: 0.201673, acc:  77%, op_acc: 100.00%] [G loss: 8.949383] time: 0:43:23.953923\n",
      "[Epoch 1/30] [Batch 398/816] [D loss: 0.177903, acc:  77%, op_acc: 100.00%] [G loss: 10.701757] time: 0:43:25.903764\n",
      "[Epoch 1/30] [Batch 399/816] [D loss: 1.191447, acc:  74%, op_acc: 95.00%] [G loss: 8.471629] time: 0:43:27.841647\n",
      "[Epoch 1/30] [Batch 400/816] [D loss: 1.117937, acc:  78%, op_acc: 90.00%] [G loss: 8.487742] time: 0:43:29.763023\n",
      "SAVE\n",
      "1_400-T2-PD.png\n",
      "[Epoch 1/30] [Batch 401/816] [D loss: 0.458248, acc:  55%, op_acc: 95.00%] [G loss: 7.938488] time: 0:43:31.882345\n",
      "[Epoch 1/30] [Batch 402/816] [D loss: 0.179537, acc:  81%, op_acc: 100.00%] [G loss: 9.408413] time: 0:43:33.810174\n",
      "[Epoch 1/30] [Batch 403/816] [D loss: 0.179341, acc:  71%, op_acc: 100.00%] [G loss: 9.059210] time: 0:43:35.716664\n",
      "[Epoch 1/30] [Batch 404/816] [D loss: 0.182797, acc:  80%, op_acc: 100.00%] [G loss: 10.393001] time: 0:43:37.637641\n",
      "[Epoch 1/30] [Batch 405/816] [D loss: 0.193341, acc:  70%, op_acc: 100.00%] [G loss: 8.368718] time: 0:43:39.574969\n",
      "[Epoch 1/30] [Batch 406/816] [D loss: 0.155628, acc:  75%, op_acc: 100.00%] [G loss: 9.345586] time: 0:43:41.523089\n",
      "[Epoch 1/30] [Batch 407/816] [D loss: 1.778306, acc:  73%, op_acc: 95.00%] [G loss: 8.542105] time: 0:43:43.460922\n",
      "[Epoch 1/30] [Batch 408/816] [D loss: 0.850666, acc:  76%, op_acc: 90.00%] [G loss: 7.093329] time: 0:43:45.384089\n",
      "[Epoch 1/30] [Batch 409/816] [D loss: 0.347563, acc:  49%, op_acc: 100.00%] [G loss: 7.365481] time: 0:43:47.307334\n",
      "[Epoch 1/30] [Batch 410/816] [D loss: 0.258795, acc:  72%, op_acc: 100.00%] [G loss: 8.764100] time: 0:43:49.244775\n",
      "[Epoch 1/30] [Batch 411/816] [D loss: 0.414945, acc:  83%, op_acc: 95.00%] [G loss: 10.133934] time: 0:43:51.186238\n",
      "[Epoch 1/30] [Batch 412/816] [D loss: 1.761283, acc:  58%, op_acc: 90.00%] [G loss: 7.939038] time: 0:43:53.118669\n",
      "[Epoch 1/30] [Batch 413/816] [D loss: 0.254438, acc:  69%, op_acc: 100.00%] [G loss: 9.256531] time: 0:43:55.054685\n",
      "[Epoch 1/30] [Batch 414/816] [D loss: 0.380463, acc:  56%, op_acc: 100.00%] [G loss: 7.875707] time: 0:43:56.999991\n",
      "[Epoch 1/30] [Batch 415/816] [D loss: 0.525200, acc:  78%, op_acc: 95.00%] [G loss: 7.088785] time: 0:43:58.932304\n",
      "[Epoch 1/30] [Batch 416/816] [D loss: 0.562098, acc:  82%, op_acc: 95.00%] [G loss: 9.387261] time: 0:44:00.854111\n",
      "[Epoch 1/30] [Batch 417/816] [D loss: 0.222229, acc:  65%, op_acc: 100.00%] [G loss: 7.290252] time: 0:44:02.783734\n",
      "[Epoch 1/30] [Batch 418/816] [D loss: 0.189383, acc:  80%, op_acc: 100.00%] [G loss: 7.351034] time: 0:44:04.710799\n",
      "[Epoch 1/30] [Batch 419/816] [D loss: 0.243314, acc:  83%, op_acc: 100.00%] [G loss: 9.188167] time: 0:44:06.627448\n",
      "[Epoch 1/30] [Batch 420/816] [D loss: 0.252481, acc:  64%, op_acc: 100.00%] [G loss: 8.827855] time: 0:44:08.570731\n",
      "[Epoch 1/30] [Batch 421/816] [D loss: 0.327389, acc:  74%, op_acc: 95.00%] [G loss: 8.812791] time: 0:44:10.500867\n",
      "[Epoch 1/30] [Batch 422/816] [D loss: 0.202164, acc:  80%, op_acc: 100.00%] [G loss: 9.567928] time: 0:44:12.398868\n",
      "[Epoch 1/30] [Batch 423/816] [D loss: 0.858626, acc:  64%, op_acc: 95.00%] [G loss: 8.291397] time: 0:44:14.342014\n",
      "[Epoch 1/30] [Batch 424/816] [D loss: 0.294404, acc:  73%, op_acc: 95.00%] [G loss: 8.078449] time: 0:44:16.275535\n",
      "[Epoch 1/30] [Batch 425/816] [D loss: 0.350137, acc:  83%, op_acc: 95.00%] [G loss: 7.527948] time: 0:44:18.191518\n",
      "[Epoch 1/30] [Batch 426/816] [D loss: 0.149404, acc:  84%, op_acc: 100.00%] [G loss: 11.463669] time: 0:44:20.127361\n",
      "[Epoch 1/30] [Batch 427/816] [D loss: 0.924412, acc:  79%, op_acc: 90.00%] [G loss: 8.532914] time: 0:44:22.062196\n",
      "[Epoch 1/30] [Batch 428/816] [D loss: 0.327629, acc:  87%, op_acc: 95.00%] [G loss: 10.852242] time: 0:44:23.999730\n",
      "[Epoch 1/30] [Batch 429/816] [D loss: 0.163689, acc:  78%, op_acc: 100.00%] [G loss: 13.040509] time: 0:44:25.932601\n",
      "[Epoch 1/30] [Batch 430/816] [D loss: 0.235034, acc:  77%, op_acc: 100.00%] [G loss: 9.612692] time: 0:44:27.869799\n",
      "[Epoch 1/30] [Batch 431/816] [D loss: 1.122949, acc:  74%, op_acc: 95.00%] [G loss: 9.035610] time: 0:44:29.808960\n",
      "[Epoch 1/30] [Batch 432/816] [D loss: 0.193163, acc:  82%, op_acc: 100.00%] [G loss: 10.247459] time: 0:44:31.751396\n",
      "[Epoch 1/30] [Batch 433/816] [D loss: 0.206358, acc:  80%, op_acc: 100.00%] [G loss: 8.742460] time: 0:44:33.681935\n",
      "[Epoch 1/30] [Batch 434/816] [D loss: 0.224909, acc:  70%, op_acc: 100.00%] [G loss: 9.434175] time: 0:44:35.608690\n",
      "[Epoch 1/30] [Batch 435/816] [D loss: 0.163687, acc:  83%, op_acc: 100.00%] [G loss: 8.973164] time: 0:44:37.539171\n",
      "[Epoch 1/30] [Batch 436/816] [D loss: 0.209042, acc:  75%, op_acc: 100.00%] [G loss: 9.113599] time: 0:44:39.472057\n",
      "[Epoch 1/30] [Batch 437/816] [D loss: 0.849052, acc:  78%, op_acc: 95.00%] [G loss: 8.965193] time: 0:44:41.406701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/30] [Batch 438/816] [D loss: 0.178993, acc:  73%, op_acc: 100.00%] [G loss: 7.592864] time: 0:44:43.330392\n",
      "[Epoch 1/30] [Batch 439/816] [D loss: 0.666159, acc:  88%, op_acc: 95.00%] [G loss: 8.578846] time: 0:44:45.229849\n",
      "[Epoch 1/30] [Batch 440/816] [D loss: 0.249607, acc:  70%, op_acc: 100.00%] [G loss: 9.224240] time: 0:44:47.166693\n",
      "[Epoch 1/30] [Batch 441/816] [D loss: 0.134609, acc:  88%, op_acc: 100.00%] [G loss: 9.574926] time: 0:44:49.106032\n",
      "[Epoch 1/30] [Batch 442/816] [D loss: 0.172718, acc:  84%, op_acc: 100.00%] [G loss: 7.931029] time: 0:44:51.035849\n",
      "[Epoch 1/30] [Batch 443/816] [D loss: 0.122931, acc:  84%, op_acc: 100.00%] [G loss: 7.712873] time: 0:44:52.968353\n",
      "[Epoch 1/30] [Batch 444/816] [D loss: 0.212151, acc:  78%, op_acc: 100.00%] [G loss: 8.514504] time: 0:44:54.897557\n",
      "[Epoch 1/30] [Batch 445/816] [D loss: 0.256550, acc:  54%, op_acc: 100.00%] [G loss: 8.584805] time: 0:44:56.817627\n",
      "[Epoch 1/30] [Batch 446/816] [D loss: 0.220243, acc:  78%, op_acc: 100.00%] [G loss: 7.738457] time: 0:44:58.755384\n",
      "[Epoch 1/30] [Batch 447/816] [D loss: 0.144672, acc:  86%, op_acc: 100.00%] [G loss: 9.435696] time: 0:45:00.698051\n",
      "[Epoch 1/30] [Batch 448/816] [D loss: 0.152481, acc:  74%, op_acc: 100.00%] [G loss: 8.089694] time: 0:45:02.628041\n",
      "[Epoch 1/30] [Batch 449/816] [D loss: 0.166526, acc:  81%, op_acc: 100.00%] [G loss: 9.073036] time: 0:45:04.573455\n",
      "[Epoch 1/30] [Batch 450/816] [D loss: 0.211836, acc:  72%, op_acc: 100.00%] [G loss: 9.325089] time: 0:45:06.505105\n",
      "[Epoch 1/30] [Batch 451/816] [D loss: 0.229937, acc:  66%, op_acc: 100.00%] [G loss: 7.556081] time: 0:45:08.429514\n",
      "[Epoch 1/30] [Batch 452/816] [D loss: 0.261695, acc:  81%, op_acc: 95.00%] [G loss: 8.642187] time: 0:45:10.352955\n",
      "[Epoch 1/30] [Batch 453/816] [D loss: 2.578632, acc:  62%, op_acc: 95.00%] [G loss: 8.315773] time: 0:45:12.289726\n",
      "[Epoch 1/30] [Batch 454/816] [D loss: 0.205979, acc:  80%, op_acc: 100.00%] [G loss: 7.757330] time: 0:45:14.216244\n",
      "[Epoch 1/30] [Batch 455/816] [D loss: 1.049241, acc:  79%, op_acc: 90.00%] [G loss: 7.361785] time: 0:45:16.143092\n",
      "[Epoch 1/30] [Batch 456/816] [D loss: 0.199844, acc:  71%, op_acc: 100.00%] [G loss: 7.649328] time: 0:45:18.078068\n",
      "[Epoch 1/30] [Batch 457/816] [D loss: 0.169621, acc:  83%, op_acc: 100.00%] [G loss: 8.030767] time: 0:45:19.986070\n",
      "[Epoch 1/30] [Batch 458/816] [D loss: 2.242506, acc:  76%, op_acc: 90.00%] [G loss: 7.885934] time: 0:45:21.905363\n",
      "[Epoch 1/30] [Batch 459/816] [D loss: 0.203338, acc:  82%, op_acc: 100.00%] [G loss: 10.193462] time: 0:45:23.819223\n",
      "[Epoch 1/30] [Batch 460/816] [D loss: 0.203918, acc:  80%, op_acc: 100.00%] [G loss: 9.182069] time: 0:45:25.738308\n",
      "[Epoch 1/30] [Batch 461/816] [D loss: 0.180474, acc:  80%, op_acc: 100.00%] [G loss: 9.650269] time: 0:45:27.653038\n",
      "[Epoch 1/30] [Batch 462/816] [D loss: 0.262835, acc:  77%, op_acc: 100.00%] [G loss: 9.039638] time: 0:45:29.577344\n",
      "[Epoch 1/30] [Batch 463/816] [D loss: 0.335826, acc:  76%, op_acc: 100.00%] [G loss: 7.925539] time: 0:45:31.504310\n",
      "[Epoch 1/30] [Batch 464/816] [D loss: 0.585692, acc:  70%, op_acc: 95.00%] [G loss: 11.650063] time: 0:45:33.439933\n",
      "[Epoch 1/30] [Batch 465/816] [D loss: 0.164113, acc:  85%, op_acc: 100.00%] [G loss: 11.494213] time: 0:45:35.356296\n",
      "[Epoch 1/30] [Batch 466/816] [D loss: 0.574095, acc:  68%, op_acc: 95.00%] [G loss: 9.663626] time: 0:45:37.280906\n",
      "[Epoch 1/30] [Batch 467/816] [D loss: 0.267014, acc:  60%, op_acc: 100.00%] [G loss: 8.482880] time: 0:45:39.221578\n",
      "[Epoch 1/30] [Batch 468/816] [D loss: 1.700429, acc:  73%, op_acc: 95.00%] [G loss: 7.996432] time: 0:45:41.160432\n",
      "[Epoch 1/30] [Batch 469/816] [D loss: 0.230239, acc:  70%, op_acc: 100.00%] [G loss: 8.382176] time: 0:45:43.096597\n",
      "[Epoch 1/30] [Batch 470/816] [D loss: 0.197869, acc:  77%, op_acc: 100.00%] [G loss: 7.375866] time: 0:45:45.019526\n",
      "[Epoch 1/30] [Batch 471/816] [D loss: 0.935850, acc:  76%, op_acc: 95.00%] [G loss: 8.710494] time: 0:45:46.955693\n",
      "[Epoch 1/30] [Batch 472/816] [D loss: 0.180568, acc:  80%, op_acc: 100.00%] [G loss: 8.683593] time: 0:45:48.881307\n",
      "[Epoch 1/30] [Batch 473/816] [D loss: 0.188929, acc:  80%, op_acc: 100.00%] [G loss: 9.344138] time: 0:45:50.810193\n",
      "[Epoch 1/30] [Batch 474/816] [D loss: 0.851583, acc:  79%, op_acc: 95.00%] [G loss: 10.211658] time: 0:45:52.737645\n",
      "[Epoch 1/30] [Batch 475/816] [D loss: 0.740314, acc:  78%, op_acc: 95.00%] [G loss: 7.893119] time: 0:45:54.666705\n",
      "[Epoch 1/30] [Batch 476/816] [D loss: 1.340039, acc:  82%, op_acc: 90.00%] [G loss: 9.787273] time: 0:45:56.618001\n",
      "[Epoch 1/30] [Batch 477/816] [D loss: 0.184763, acc:  74%, op_acc: 100.00%] [G loss: 11.719347] time: 0:45:58.549139\n",
      "[Epoch 1/30] [Batch 478/816] [D loss: 2.588281, acc:  66%, op_acc: 85.00%] [G loss: 9.326443] time: 0:46:00.472149\n",
      "[Epoch 1/30] [Batch 479/816] [D loss: 1.049567, acc:  78%, op_acc: 95.00%] [G loss: 11.503113] time: 0:46:02.411397\n",
      "[Epoch 1/30] [Batch 480/816] [D loss: 0.414196, acc:  56%, op_acc: 100.00%] [G loss: 6.943596] time: 0:46:04.340896\n",
      "[Epoch 1/30] [Batch 481/816] [D loss: 0.141314, acc:  87%, op_acc: 100.00%] [G loss: 10.437552] time: 0:46:06.272140\n",
      "[Epoch 1/30] [Batch 482/816] [D loss: 0.354469, acc:  83%, op_acc: 95.00%] [G loss: 8.978239] time: 0:46:08.206056\n",
      "[Epoch 1/30] [Batch 483/816] [D loss: 0.173757, acc:  84%, op_acc: 100.00%] [G loss: 9.394921] time: 0:46:10.139050\n",
      "[Epoch 1/30] [Batch 484/816] [D loss: 0.543704, acc:  68%, op_acc: 95.00%] [G loss: 8.109871] time: 0:46:12.071489\n",
      "[Epoch 1/30] [Batch 485/816] [D loss: 0.151522, acc:  86%, op_acc: 100.00%] [G loss: 9.404982] time: 0:46:14.009796\n",
      "[Epoch 1/30] [Batch 486/816] [D loss: 0.125081, acc:  88%, op_acc: 100.00%] [G loss: 9.147629] time: 0:46:15.950324\n",
      "[Epoch 1/30] [Batch 487/816] [D loss: 0.114280, acc:  91%, op_acc: 100.00%] [G loss: 10.317531] time: 0:46:17.874705\n",
      "[Epoch 1/30] [Batch 488/816] [D loss: 0.176672, acc:  81%, op_acc: 100.00%] [G loss: 7.658280] time: 0:46:19.803222\n",
      "[Epoch 1/30] [Batch 489/816] [D loss: 0.125488, acc:  90%, op_acc: 100.00%] [G loss: 8.234548] time: 0:46:21.731402\n",
      "[Epoch 1/30] [Batch 490/816] [D loss: 0.470600, acc:  91%, op_acc: 95.00%] [G loss: 8.595796] time: 0:46:23.651038\n",
      "[Epoch 1/30] [Batch 491/816] [D loss: 0.259069, acc:  85%, op_acc: 95.00%] [G loss: 8.733808] time: 0:46:25.570426\n",
      "[Epoch 1/30] [Batch 492/816] [D loss: 0.133509, acc:  88%, op_acc: 100.00%] [G loss: 9.305263] time: 0:46:27.490772\n",
      "[Epoch 1/30] [Batch 493/816] [D loss: 1.176260, acc:  69%, op_acc: 90.00%] [G loss: 6.221792] time: 0:46:29.430946\n",
      "[Epoch 1/30] [Batch 494/816] [D loss: 0.116335, acc:  92%, op_acc: 100.00%] [G loss: 8.524679] time: 0:46:31.401096\n",
      "[Epoch 1/30] [Batch 495/816] [D loss: 0.214682, acc:  84%, op_acc: 95.00%] [G loss: 7.969557] time: 0:46:33.321277\n",
      "[Epoch 1/30] [Batch 496/816] [D loss: 0.156993, acc:  87%, op_acc: 100.00%] [G loss: 9.339654] time: 0:46:35.252999\n",
      "[Epoch 1/30] [Batch 497/816] [D loss: 0.124995, acc:  88%, op_acc: 100.00%] [G loss: 8.535454] time: 0:46:37.194140\n",
      "[Epoch 1/30] [Batch 498/816] [D loss: 0.090642, acc:  94%, op_acc: 100.00%] [G loss: 10.326404] time: 0:46:39.125751\n",
      "[Epoch 1/30] [Batch 499/816] [D loss: 0.144517, acc:  90%, op_acc: 100.00%] [G loss: 7.649508] time: 0:46:41.062659\n",
      "[Epoch 1/30] [Batch 500/816] [D loss: 0.196518, acc:  82%, op_acc: 100.00%] [G loss: 6.825827] time: 0:46:42.994264\n",
      "[Epoch 1/30] [Batch 501/816] [D loss: 0.558964, acc:  90%, op_acc: 95.00%] [G loss: 9.063482] time: 0:46:44.911223\n",
      "[Epoch 1/30] [Batch 502/816] [D loss: 0.143745, acc:  78%, op_acc: 100.00%] [G loss: 7.375358] time: 0:46:46.848227\n",
      "[Epoch 1/30] [Batch 503/816] [D loss: 0.162059, acc:  85%, op_acc: 100.00%] [G loss: 7.435959] time: 0:46:48.784663\n",
      "[Epoch 1/30] [Batch 504/816] [D loss: 0.179460, acc:  70%, op_acc: 100.00%] [G loss: 7.395188] time: 0:46:50.717947\n",
      "[Epoch 1/30] [Batch 505/816] [D loss: 1.707923, acc:  91%, op_acc: 95.00%] [G loss: 7.678442] time: 0:46:52.652519\n",
      "[Epoch 1/30] [Batch 506/816] [D loss: 4.718656, acc:  94%, op_acc: 90.00%] [G loss: 7.964140] time: 0:46:54.579901\n",
      "[Epoch 1/30] [Batch 507/816] [D loss: 0.785845, acc:  74%, op_acc: 95.00%] [G loss: 7.414381] time: 0:46:56.499414\n",
      "[Epoch 1/30] [Batch 508/816] [D loss: 0.157750, acc:  86%, op_acc: 100.00%] [G loss: 8.496334] time: 0:46:58.432156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/30] [Batch 509/816] [D loss: 1.070175, acc:  70%, op_acc: 95.00%] [G loss: 7.595388] time: 0:47:00.360800\n",
      "[Epoch 1/30] [Batch 510/816] [D loss: 0.143981, acc:  85%, op_acc: 100.00%] [G loss: 8.989377] time: 0:47:02.292028\n",
      "[Epoch 1/30] [Batch 511/816] [D loss: 0.203533, acc:  83%, op_acc: 100.00%] [G loss: 7.284585] time: 0:47:04.240567\n",
      "[Epoch 1/30] [Batch 512/816] [D loss: 0.097412, acc:  93%, op_acc: 100.00%] [G loss: 8.612860] time: 0:47:06.178365\n",
      "[Epoch 1/30] [Batch 513/816] [D loss: 0.163694, acc:  82%, op_acc: 100.00%] [G loss: 7.978029] time: 0:47:08.118736\n",
      "[Epoch 1/30] [Batch 514/816] [D loss: 0.183627, acc:  78%, op_acc: 100.00%] [G loss: 7.011695] time: 0:47:10.050975\n",
      "[Epoch 1/30] [Batch 515/816] [D loss: 0.397699, acc:  84%, op_acc: 95.00%] [G loss: 8.906738] time: 0:47:11.988186\n",
      "[Epoch 1/30] [Batch 516/816] [D loss: 0.292964, acc:  60%, op_acc: 100.00%] [G loss: 8.571377] time: 0:47:13.900307\n",
      "[Epoch 1/30] [Batch 517/816] [D loss: 0.233632, acc:  74%, op_acc: 100.00%] [G loss: 9.644165] time: 0:47:15.817309\n",
      "[Epoch 1/30] [Batch 518/816] [D loss: 0.287664, acc:  78%, op_acc: 100.00%] [G loss: 7.871455] time: 0:47:17.751676\n",
      "[Epoch 1/30] [Batch 519/816] [D loss: 0.173088, acc:  82%, op_acc: 100.00%] [G loss: 9.090601] time: 0:47:19.685337\n",
      "[Epoch 1/30] [Batch 520/816] [D loss: 0.186858, acc:  78%, op_acc: 100.00%] [G loss: 7.938604] time: 0:47:21.622562\n",
      "[Epoch 1/30] [Batch 521/816] [D loss: 0.259126, acc:  56%, op_acc: 100.00%] [G loss: 8.467523] time: 0:47:23.550107\n",
      "[Epoch 1/30] [Batch 522/816] [D loss: 0.141727, acc:  86%, op_acc: 100.00%] [G loss: 9.050045] time: 0:47:25.477700\n",
      "[Epoch 1/30] [Batch 523/816] [D loss: 0.124898, acc:  82%, op_acc: 100.00%] [G loss: 9.085031] time: 0:47:27.438249\n",
      "[Epoch 1/30] [Batch 524/816] [D loss: 0.141644, acc:  88%, op_acc: 100.00%] [G loss: 9.624013] time: 0:47:29.367812\n",
      "[Epoch 1/30] [Batch 525/816] [D loss: 0.169495, acc:  83%, op_acc: 100.00%] [G loss: 8.362387] time: 0:47:31.304900\n",
      "[Epoch 1/30] [Batch 526/816] [D loss: 0.189339, acc:  81%, op_acc: 100.00%] [G loss: 7.518140] time: 0:47:33.230907\n",
      "[Epoch 1/30] [Batch 527/816] [D loss: 0.186811, acc:  87%, op_acc: 100.00%] [G loss: 8.646229] time: 0:47:35.175856\n",
      "[Epoch 1/30] [Batch 528/816] [D loss: 0.271524, acc:  44%, op_acc: 100.00%] [G loss: 6.813851] time: 0:47:37.117617\n",
      "[Epoch 1/30] [Batch 529/816] [D loss: 0.168406, acc:  84%, op_acc: 100.00%] [G loss: 8.858236] time: 0:47:39.052844\n",
      "[Epoch 1/30] [Batch 530/816] [D loss: 0.125536, acc:  86%, op_acc: 100.00%] [G loss: 10.121231] time: 0:47:40.980439\n",
      "[Epoch 1/30] [Batch 531/816] [D loss: 0.602285, acc:  84%, op_acc: 95.00%] [G loss: 7.827435] time: 0:47:42.921986\n",
      "[Epoch 1/30] [Batch 532/816] [D loss: 0.172656, acc:  77%, op_acc: 100.00%] [G loss: 10.052733] time: 0:47:44.852277\n",
      "[Epoch 1/30] [Batch 533/816] [D loss: 0.089148, acc:  93%, op_acc: 100.00%] [G loss: 9.194236] time: 0:47:46.783580\n",
      "[Epoch 1/30] [Batch 534/816] [D loss: 0.104353, acc:  91%, op_acc: 100.00%] [G loss: 8.449351] time: 0:47:48.721607\n",
      "[Epoch 1/30] [Batch 535/816] [D loss: 0.124535, acc:  88%, op_acc: 100.00%] [G loss: 10.548191] time: 0:47:50.660728\n",
      "[Epoch 1/30] [Batch 536/816] [D loss: 0.403522, acc:  90%, op_acc: 95.00%] [G loss: 8.329321] time: 0:47:52.578023\n",
      "[Epoch 1/30] [Batch 537/816] [D loss: 0.228355, acc:  74%, op_acc: 100.00%] [G loss: 8.422271] time: 0:47:54.509898\n",
      "[Epoch 1/30] [Batch 538/816] [D loss: 0.992176, acc:  71%, op_acc: 95.00%] [G loss: 9.650916] time: 0:47:56.451606\n",
      "[Epoch 1/30] [Batch 539/816] [D loss: 0.105789, acc:  90%, op_acc: 100.00%] [G loss: 9.628034] time: 0:47:58.431069\n",
      "[Epoch 1/30] [Batch 540/816] [D loss: 0.149726, acc:  76%, op_acc: 100.00%] [G loss: 8.156745] time: 0:48:00.365392\n",
      "[Epoch 1/30] [Batch 541/816] [D loss: 0.187447, acc:  75%, op_acc: 100.00%] [G loss: 9.049436] time: 0:48:02.293068\n",
      "[Epoch 1/30] [Batch 542/816] [D loss: 0.116127, acc:  88%, op_acc: 100.00%] [G loss: 7.841437] time: 0:48:04.229088\n",
      "[Epoch 1/30] [Batch 543/816] [D loss: 0.418401, acc:  70%, op_acc: 95.00%] [G loss: 7.426347] time: 0:48:06.167289\n",
      "[Epoch 1/30] [Batch 544/816] [D loss: 0.208459, acc:  81%, op_acc: 100.00%] [G loss: 9.452836] time: 0:48:08.109649\n",
      "[Epoch 1/30] [Batch 545/816] [D loss: 1.596290, acc:  78%, op_acc: 95.00%] [G loss: 7.908598] time: 0:48:10.036074\n",
      "[Epoch 1/30] [Batch 546/816] [D loss: 0.263502, acc:  80%, op_acc: 100.00%] [G loss: 8.901160] time: 0:48:11.971183\n",
      "[Epoch 1/30] [Batch 547/816] [D loss: 0.165728, acc:  78%, op_acc: 100.00%] [G loss: 7.861192] time: 0:48:13.906660\n",
      "[Epoch 1/30] [Batch 548/816] [D loss: 0.084391, acc:  97%, op_acc: 100.00%] [G loss: 7.261328] time: 0:48:15.814418\n",
      "[Epoch 1/30] [Batch 549/816] [D loss: 0.250387, acc:  77%, op_acc: 95.00%] [G loss: 9.580097] time: 0:48:17.751062\n",
      "[Epoch 1/30] [Batch 550/816] [D loss: 0.179238, acc:  77%, op_acc: 100.00%] [G loss: 8.803747] time: 0:48:19.681303\n",
      "[Epoch 1/30] [Batch 551/816] [D loss: 0.140911, acc:  82%, op_acc: 100.00%] [G loss: 9.809073] time: 0:48:21.622131\n",
      "[Epoch 1/30] [Batch 552/816] [D loss: 0.166762, acc:  84%, op_acc: 100.00%] [G loss: 8.320407] time: 0:48:23.558190\n",
      "[Epoch 1/30] [Batch 553/816] [D loss: 0.944755, acc:  81%, op_acc: 95.00%] [G loss: 7.858610] time: 0:48:25.488366\n",
      "[Epoch 1/30] [Batch 554/816] [D loss: 0.252134, acc:  57%, op_acc: 100.00%] [G loss: 8.151837] time: 0:48:27.422340\n",
      "[Epoch 1/30] [Batch 555/816] [D loss: 0.215180, acc:  76%, op_acc: 100.00%] [G loss: 7.940635] time: 0:48:29.409935\n",
      "[Epoch 1/30] [Batch 556/816] [D loss: 0.159900, acc:  86%, op_acc: 100.00%] [G loss: 9.106257] time: 0:48:31.349241\n",
      "[Epoch 1/30] [Batch 557/816] [D loss: 0.164770, acc:  86%, op_acc: 100.00%] [G loss: 8.298814] time: 0:48:33.289739\n",
      "[Epoch 1/30] [Batch 558/816] [D loss: 0.162313, acc:  82%, op_acc: 100.00%] [G loss: 7.339028] time: 0:48:35.227085\n",
      "[Epoch 1/30] [Batch 559/816] [D loss: 0.143781, acc:  90%, op_acc: 100.00%] [G loss: 8.262069] time: 0:48:37.159535\n",
      "[Epoch 1/30] [Batch 560/816] [D loss: 0.223576, acc:  62%, op_acc: 100.00%] [G loss: 8.317272] time: 0:48:39.096220\n",
      "[Epoch 1/30] [Batch 561/816] [D loss: 0.138488, acc:  88%, op_acc: 100.00%] [G loss: 8.330349] time: 0:48:41.013177\n",
      "[Epoch 1/30] [Batch 562/816] [D loss: 0.322535, acc:  85%, op_acc: 95.00%] [G loss: 11.086046] time: 0:48:42.935571\n",
      "[Epoch 1/30] [Batch 563/816] [D loss: 0.351787, acc:  70%, op_acc: 95.00%] [G loss: 7.903773] time: 0:48:44.867114\n",
      "[Epoch 1/30] [Batch 564/816] [D loss: 0.121521, acc:  89%, op_acc: 100.00%] [G loss: 10.342334] time: 0:48:46.796309\n",
      "[Epoch 1/30] [Batch 565/816] [D loss: 0.238340, acc:  66%, op_acc: 100.00%] [G loss: 7.719994] time: 0:48:48.722046\n",
      "[Epoch 1/30] [Batch 566/816] [D loss: 0.223879, acc:  73%, op_acc: 100.00%] [G loss: 9.776414] time: 0:48:50.645481\n",
      "[Epoch 1/30] [Batch 567/816] [D loss: 0.820983, acc:  85%, op_acc: 90.00%] [G loss: 9.710379] time: 0:48:52.565205\n",
      "[Epoch 1/30] [Batch 568/816] [D loss: 0.328709, acc:  71%, op_acc: 95.00%] [G loss: 7.717398] time: 0:48:54.496147\n",
      "[Epoch 1/30] [Batch 569/816] [D loss: 0.999527, acc:  89%, op_acc: 95.00%] [G loss: 8.566873] time: 0:48:56.425676\n",
      "[Epoch 1/30] [Batch 570/816] [D loss: 0.223294, acc:  84%, op_acc: 95.00%] [G loss: 9.182592] time: 0:48:58.361850\n",
      "[Epoch 1/30] [Batch 571/816] [D loss: 1.057195, acc:  90%, op_acc: 90.00%] [G loss: 7.817382] time: 0:49:00.294834\n",
      "[Epoch 1/30] [Batch 572/816] [D loss: 0.158873, acc:  89%, op_acc: 100.00%] [G loss: 9.154072] time: 0:49:02.236250\n",
      "[Epoch 1/30] [Batch 573/816] [D loss: 0.182999, acc:  89%, op_acc: 100.00%] [G loss: 8.461967] time: 0:49:04.163789\n",
      "[Epoch 1/30] [Batch 574/816] [D loss: 1.594215, acc:  93%, op_acc: 95.00%] [G loss: 8.550627] time: 0:49:06.103747\n",
      "[Epoch 1/30] [Batch 575/816] [D loss: 0.142995, acc:  87%, op_acc: 100.00%] [G loss: 9.124537] time: 0:49:08.040051\n",
      "[Epoch 1/30] [Batch 576/816] [D loss: 0.201875, acc:  83%, op_acc: 100.00%] [G loss: 8.897168] time: 0:49:09.960441\n",
      "[Epoch 1/30] [Batch 577/816] [D loss: 0.249616, acc:  77%, op_acc: 100.00%] [G loss: 8.295194] time: 0:49:11.897947\n",
      "[Epoch 1/30] [Batch 578/816] [D loss: 0.179457, acc:  79%, op_acc: 100.00%] [G loss: 9.309263] time: 0:49:13.840751\n",
      "[Epoch 1/30] [Batch 579/816] [D loss: 0.924887, acc:  92%, op_acc: 95.00%] [G loss: 7.363732] time: 0:49:15.765396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/30] [Batch 580/816] [D loss: 0.245148, acc:  79%, op_acc: 95.00%] [G loss: 9.770905] time: 0:49:17.706297\n",
      "[Epoch 1/30] [Batch 581/816] [D loss: 1.138983, acc:  88%, op_acc: 95.00%] [G loss: 10.057013] time: 0:49:19.650679\n",
      "[Epoch 1/30] [Batch 582/816] [D loss: 0.491816, acc:  68%, op_acc: 95.00%] [G loss: 7.386639] time: 0:49:21.587303\n",
      "[Epoch 1/30] [Batch 583/816] [D loss: 0.129591, acc:  84%, op_acc: 100.00%] [G loss: 6.834816] time: 0:49:23.517380\n",
      "[Epoch 1/30] [Batch 584/816] [D loss: 0.137532, acc:  88%, op_acc: 100.00%] [G loss: 7.838040] time: 0:49:25.439890\n",
      "[Epoch 1/30] [Batch 585/816] [D loss: 0.257919, acc:  79%, op_acc: 100.00%] [G loss: 5.828763] time: 0:49:27.372556\n",
      "[Epoch 1/30] [Batch 586/816] [D loss: 0.160605, acc:  87%, op_acc: 100.00%] [G loss: 8.298542] time: 0:49:29.309185\n",
      "[Epoch 1/30] [Batch 587/816] [D loss: 0.115954, acc:  94%, op_acc: 100.00%] [G loss: 7.330610] time: 0:49:31.243533\n",
      "[Epoch 1/30] [Batch 588/816] [D loss: 0.203053, acc:  66%, op_acc: 100.00%] [G loss: 7.604563] time: 0:49:33.171567\n",
      "[Epoch 1/30] [Batch 589/816] [D loss: 0.173409, acc:  85%, op_acc: 100.00%] [G loss: 7.674114] time: 0:49:35.106213\n",
      "[Epoch 1/30] [Batch 590/816] [D loss: 0.102651, acc:  95%, op_acc: 100.00%] [G loss: 9.830890] time: 0:49:37.038495\n",
      "[Epoch 1/30] [Batch 591/816] [D loss: 0.141895, acc:  82%, op_acc: 100.00%] [G loss: 7.293014] time: 0:49:38.970171\n",
      "[Epoch 1/30] [Batch 592/816] [D loss: 0.100413, acc:  97%, op_acc: 100.00%] [G loss: 7.421877] time: 0:49:40.904973\n",
      "[Epoch 1/30] [Batch 593/816] [D loss: 0.262996, acc:  83%, op_acc: 100.00%] [G loss: 7.725888] time: 0:49:42.834748\n",
      "[Epoch 1/30] [Batch 594/816] [D loss: 0.437709, acc:  75%, op_acc: 100.00%] [G loss: 8.503645] time: 0:49:44.756293\n",
      "[Epoch 1/30] [Batch 595/816] [D loss: 0.359756, acc:  75%, op_acc: 100.00%] [G loss: 8.535765] time: 0:49:46.689567\n",
      "[Epoch 1/30] [Batch 596/816] [D loss: 0.289457, acc:  70%, op_acc: 100.00%] [G loss: 8.527153] time: 0:49:48.620570\n",
      "[Epoch 1/30] [Batch 597/816] [D loss: 0.176553, acc:  82%, op_acc: 100.00%] [G loss: 10.510540] time: 0:49:50.545231\n",
      "[Epoch 1/30] [Batch 598/816] [D loss: 0.225036, acc:  83%, op_acc: 100.00%] [G loss: 8.330977] time: 0:49:52.472594\n",
      "[Epoch 1/30] [Batch 599/816] [D loss: 0.200043, acc:  69%, op_acc: 100.00%] [G loss: 8.028837] time: 0:49:54.424605\n",
      "[Epoch 1/30] [Batch 600/816] [D loss: 0.136308, acc:  86%, op_acc: 100.00%] [G loss: 8.792951] time: 0:49:56.354798\n",
      "SAVE\n",
      "1_600-T1-PD.png\n",
      "[Epoch 1/30] [Batch 601/816] [D loss: 0.086391, acc:  96%, op_acc: 100.00%] [G loss: 8.572901] time: 0:49:58.523966\n",
      "[Epoch 1/30] [Batch 602/816] [D loss: 0.146099, acc:  86%, op_acc: 100.00%] [G loss: 7.095975] time: 0:50:00.457006\n",
      "[Epoch 1/30] [Batch 603/816] [D loss: 0.118414, acc:  92%, op_acc: 100.00%] [G loss: 9.778292] time: 0:50:02.391111\n",
      "[Epoch 1/30] [Batch 604/816] [D loss: 0.213805, acc:  63%, op_acc: 100.00%] [G loss: 6.973376] time: 0:50:04.326761\n",
      "[Epoch 1/30] [Batch 605/816] [D loss: 0.245571, acc:  70%, op_acc: 100.00%] [G loss: 8.674590] time: 0:50:06.237764\n",
      "[Epoch 1/30] [Batch 606/816] [D loss: 0.105508, acc:  90%, op_acc: 100.00%] [G loss: 8.696338] time: 0:50:08.177438\n",
      "[Epoch 1/30] [Batch 607/816] [D loss: 0.101673, acc:  94%, op_acc: 100.00%] [G loss: 7.130957] time: 0:50:10.114288\n",
      "[Epoch 1/30] [Batch 608/816] [D loss: 0.109766, acc:  93%, op_acc: 100.00%] [G loss: 8.103188] time: 0:50:12.044805\n",
      "[Epoch 1/30] [Batch 609/816] [D loss: 0.074117, acc:  95%, op_acc: 100.00%] [G loss: 8.253924] time: 0:50:13.968685\n",
      "[Epoch 1/30] [Batch 610/816] [D loss: 0.143316, acc:  85%, op_acc: 100.00%] [G loss: 8.256285] time: 0:50:15.903349\n",
      "[Epoch 1/30] [Batch 611/816] [D loss: 0.133619, acc:  86%, op_acc: 100.00%] [G loss: 7.942142] time: 0:50:17.843435\n",
      "[Epoch 1/30] [Batch 612/816] [D loss: 0.100844, acc:  92%, op_acc: 100.00%] [G loss: 9.415666] time: 0:50:19.783545\n",
      "[Epoch 1/30] [Batch 613/816] [D loss: 0.113254, acc:  89%, op_acc: 100.00%] [G loss: 8.958268] time: 0:50:21.702798\n",
      "[Epoch 1/30] [Batch 614/816] [D loss: 0.111088, acc:  90%, op_acc: 100.00%] [G loss: 7.935210] time: 0:50:23.648565\n",
      "[Epoch 1/30] [Batch 615/816] [D loss: 0.088782, acc:  92%, op_acc: 100.00%] [G loss: 6.773691] time: 0:50:25.586626\n",
      "[Epoch 1/30] [Batch 616/816] [D loss: 0.108263, acc:  94%, op_acc: 100.00%] [G loss: 7.653148] time: 0:50:27.526058\n",
      "[Epoch 1/30] [Batch 617/816] [D loss: 0.205478, acc:  71%, op_acc: 100.00%] [G loss: 6.679507] time: 0:50:29.466747\n",
      "[Epoch 1/30] [Batch 618/816] [D loss: 0.242078, acc:  55%, op_acc: 100.00%] [G loss: 7.679670] time: 0:50:31.398463\n",
      "[Epoch 1/30] [Batch 619/816] [D loss: 0.824230, acc:  57%, op_acc: 95.00%] [G loss: 6.837766] time: 0:50:33.332290\n",
      "[Epoch 1/30] [Batch 620/816] [D loss: 0.143868, acc:  87%, op_acc: 100.00%] [G loss: 8.751324] time: 0:50:35.270093\n",
      "[Epoch 1/30] [Batch 621/816] [D loss: 0.455016, acc:  76%, op_acc: 95.00%] [G loss: 9.688340] time: 0:50:37.206487\n",
      "[Epoch 1/30] [Batch 622/816] [D loss: 0.539306, acc:  59%, op_acc: 100.00%] [G loss: 10.294400] time: 0:50:39.125367\n",
      "[Epoch 1/30] [Batch 623/816] [D loss: 0.376804, acc:  71%, op_acc: 100.00%] [G loss: 13.174088] time: 0:50:41.066599\n",
      "[Epoch 1/30] [Batch 624/816] [D loss: 1.772862, acc:  68%, op_acc: 100.00%] [G loss: 10.420099] time: 0:50:43.007974\n",
      "[Epoch 1/30] [Batch 625/816] [D loss: 0.842517, acc:  59%, op_acc: 100.00%] [G loss: 10.399403] time: 0:50:44.935522\n",
      "[Epoch 1/30] [Batch 626/816] [D loss: 1.261582, acc:  50%, op_acc: 100.00%] [G loss: 10.208165] time: 0:50:46.859552\n",
      "[Epoch 1/30] [Batch 627/816] [D loss: 0.628730, acc:  43%, op_acc: 100.00%] [G loss: 8.866649] time: 0:50:48.785758\n",
      "[Epoch 1/30] [Batch 628/816] [D loss: 0.356449, acc:  81%, op_acc: 95.00%] [G loss: 9.231148] time: 0:50:50.709269\n",
      "[Epoch 1/30] [Batch 629/816] [D loss: 0.792258, acc:  80%, op_acc: 95.00%] [G loss: 7.184960] time: 0:50:52.641325\n",
      "[Epoch 1/30] [Batch 630/816] [D loss: 0.654269, acc:  76%, op_acc: 95.00%] [G loss: 7.677804] time: 0:50:54.578835\n",
      "[Epoch 1/30] [Batch 631/816] [D loss: 0.245294, acc:  83%, op_acc: 95.00%] [G loss: 8.011280] time: 0:50:56.498816\n",
      "[Epoch 1/30] [Batch 632/816] [D loss: 0.142019, acc:  85%, op_acc: 100.00%] [G loss: 9.581945] time: 0:50:58.441740\n",
      "[Epoch 1/30] [Batch 633/816] [D loss: 0.242850, acc:  88%, op_acc: 95.00%] [G loss: 8.891189] time: 0:51:00.382470\n",
      "[Epoch 1/30] [Batch 634/816] [D loss: 1.238847, acc:  91%, op_acc: 95.00%] [G loss: 10.226463] time: 0:51:02.323281\n",
      "[Epoch 1/30] [Batch 635/816] [D loss: 1.571351, acc:  72%, op_acc: 95.00%] [G loss: 10.349832] time: 0:51:04.256870\n",
      "[Epoch 1/30] [Batch 636/816] [D loss: 0.472277, acc:  48%, op_acc: 95.00%] [G loss: 8.385221] time: 0:51:06.191786\n",
      "[Epoch 1/30] [Batch 637/816] [D loss: 0.164275, acc:  79%, op_acc: 100.00%] [G loss: 8.590362] time: 0:51:08.127100\n",
      "[Epoch 1/30] [Batch 638/816] [D loss: 3.571103, acc:  85%, op_acc: 90.00%] [G loss: 11.018533] time: 0:51:10.069092\n",
      "[Epoch 1/30] [Batch 639/816] [D loss: 0.101916, acc:  92%, op_acc: 100.00%] [G loss: 10.622089] time: 0:51:12.002947\n",
      "[Epoch 1/30] [Batch 640/816] [D loss: 0.652159, acc:  68%, op_acc: 95.00%] [G loss: 8.467329] time: 0:51:13.943913\n",
      "[Epoch 1/30] [Batch 641/816] [D loss: 0.137543, acc:  87%, op_acc: 100.00%] [G loss: 10.771037] time: 0:51:15.884771\n",
      "[Epoch 1/30] [Batch 642/816] [D loss: 0.096365, acc:  93%, op_acc: 100.00%] [G loss: 8.761912] time: 0:51:17.815468\n",
      "[Epoch 1/30] [Batch 643/816] [D loss: 0.084794, acc:  94%, op_acc: 100.00%] [G loss: 8.008974] time: 0:51:19.758687\n",
      "[Epoch 1/30] [Batch 644/816] [D loss: 0.093647, acc:  92%, op_acc: 100.00%] [G loss: 9.256824] time: 0:51:21.696174\n",
      "[Epoch 1/30] [Batch 645/816] [D loss: 0.081801, acc:  93%, op_acc: 100.00%] [G loss: 8.234191] time: 0:51:23.621756\n",
      "[Epoch 1/30] [Batch 646/816] [D loss: 0.072554, acc:  95%, op_acc: 100.00%] [G loss: 8.718344] time: 0:51:25.549305\n",
      "[Epoch 1/30] [Batch 647/816] [D loss: 0.581139, acc:  95%, op_acc: 90.00%] [G loss: 8.691649] time: 0:51:27.479131\n",
      "[Epoch 1/30] [Batch 648/816] [D loss: 0.129533, acc:  87%, op_acc: 100.00%] [G loss: 9.474128] time: 0:51:29.409628\n",
      "[Epoch 1/30] [Batch 649/816] [D loss: 0.095236, acc:  93%, op_acc: 100.00%] [G loss: 8.638911] time: 0:51:31.341209\n",
      "[Epoch 1/30] [Batch 650/816] [D loss: 0.090933, acc:  91%, op_acc: 100.00%] [G loss: 9.376719] time: 0:51:33.282325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/30] [Batch 651/816] [D loss: 0.260708, acc:  64%, op_acc: 100.00%] [G loss: 11.074028] time: 0:51:35.202586\n",
      "[Epoch 1/30] [Batch 652/816] [D loss: 0.588516, acc:  82%, op_acc: 95.00%] [G loss: 10.396732] time: 0:51:37.148202\n",
      "[Epoch 1/30] [Batch 653/816] [D loss: 0.136972, acc:  88%, op_acc: 100.00%] [G loss: 11.017860] time: 0:51:39.082115\n",
      "[Epoch 1/30] [Batch 654/816] [D loss: 1.344929, acc:  81%, op_acc: 95.00%] [G loss: 8.018446] time: 0:51:41.017450\n",
      "[Epoch 1/30] [Batch 655/816] [D loss: 3.498665, acc:  93%, op_acc: 95.00%] [G loss: 8.534565] time: 0:51:42.954028\n",
      "[Epoch 1/30] [Batch 656/816] [D loss: 1.218872, acc:  61%, op_acc: 95.00%] [G loss: 6.198162] time: 0:51:44.879548\n",
      "[Epoch 1/30] [Batch 657/816] [D loss: 0.484843, acc:  75%, op_acc: 95.00%] [G loss: 8.397116] time: 0:51:46.818947\n",
      "[Epoch 1/30] [Batch 658/816] [D loss: 0.254825, acc:  59%, op_acc: 100.00%] [G loss: 11.242249] time: 0:51:48.765619\n",
      "[Epoch 1/30] [Batch 659/816] [D loss: 1.749221, acc:  81%, op_acc: 90.00%] [G loss: 8.450493] time: 0:51:50.705354\n",
      "[Epoch 1/30] [Batch 660/816] [D loss: 0.145144, acc:  91%, op_acc: 100.00%] [G loss: 7.559062] time: 0:51:52.637623\n",
      "[Epoch 1/30] [Batch 661/816] [D loss: 0.122901, acc:  90%, op_acc: 100.00%] [G loss: 8.643043] time: 0:51:54.588726\n",
      "[Epoch 1/30] [Batch 662/816] [D loss: 0.114993, acc:  88%, op_acc: 100.00%] [G loss: 8.824084] time: 0:51:56.525230\n",
      "[Epoch 1/30] [Batch 663/816] [D loss: 0.129692, acc:  88%, op_acc: 100.00%] [G loss: 9.199165] time: 0:51:58.463800\n",
      "[Epoch 1/30] [Batch 664/816] [D loss: 0.222837, acc:  76%, op_acc: 100.00%] [G loss: 9.100316] time: 0:52:00.396748\n",
      "[Epoch 1/30] [Batch 665/816] [D loss: 0.357586, acc:  81%, op_acc: 100.00%] [G loss: 8.249833] time: 0:52:02.330897\n",
      "[Epoch 1/30] [Batch 666/816] [D loss: 0.241806, acc:  79%, op_acc: 100.00%] [G loss: 9.027043] time: 0:52:04.263778\n",
      "[Epoch 1/30] [Batch 667/816] [D loss: 0.247175, acc:  95%, op_acc: 95.00%] [G loss: 8.615542] time: 0:52:06.205922\n",
      "[Epoch 1/30] [Batch 668/816] [D loss: 0.431368, acc:  86%, op_acc: 95.00%] [G loss: 8.890240] time: 0:52:08.139397\n",
      "[Epoch 1/30] [Batch 669/816] [D loss: 0.177918, acc:  84%, op_acc: 100.00%] [G loss: 7.276098] time: 0:52:10.071723\n",
      "[Epoch 1/30] [Batch 670/816] [D loss: 0.114531, acc:  90%, op_acc: 100.00%] [G loss: 9.127535] time: 0:52:12.007632\n",
      "[Epoch 1/30] [Batch 671/816] [D loss: 0.208424, acc:  80%, op_acc: 100.00%] [G loss: 8.340609] time: 0:52:13.930703\n",
      "[Epoch 1/30] [Batch 672/816] [D loss: 1.981498, acc:  85%, op_acc: 95.00%] [G loss: 8.050723] time: 0:52:15.857559\n",
      "[Epoch 1/30] [Batch 673/816] [D loss: 1.115165, acc:  92%, op_acc: 90.00%] [G loss: 9.641073] time: 0:52:17.772591\n",
      "[Epoch 1/30] [Batch 674/816] [D loss: 1.093793, acc:  94%, op_acc: 95.00%] [G loss: 13.660789] time: 0:52:19.707135\n",
      "[Epoch 1/30] [Batch 675/816] [D loss: 0.591395, acc:  86%, op_acc: 95.00%] [G loss: 9.692421] time: 0:52:21.633479\n",
      "[Epoch 1/30] [Batch 676/816] [D loss: 1.464962, acc:  74%, op_acc: 90.00%] [G loss: 8.455937] time: 0:52:23.555452\n",
      "[Epoch 1/30] [Batch 677/816] [D loss: 0.199124, acc:  75%, op_acc: 100.00%] [G loss: 8.087932] time: 0:52:25.498711\n",
      "[Epoch 1/30] [Batch 678/816] [D loss: 0.207804, acc:  80%, op_acc: 100.00%] [G loss: 7.330792] time: 0:52:27.437498\n",
      "[Epoch 1/30] [Batch 679/816] [D loss: 0.249621, acc:  89%, op_acc: 95.00%] [G loss: 9.345580] time: 0:52:29.373552\n",
      "[Epoch 1/30] [Batch 680/816] [D loss: 0.196490, acc:  85%, op_acc: 100.00%] [G loss: 7.775068] time: 0:52:31.299412\n",
      "[Epoch 1/30] [Batch 681/816] [D loss: 0.097736, acc:  93%, op_acc: 100.00%] [G loss: 8.379453] time: 0:52:33.222882\n",
      "[Epoch 1/30] [Batch 682/816] [D loss: 0.238666, acc:  93%, op_acc: 95.00%] [G loss: 5.871349] time: 0:52:35.128935\n",
      "[Epoch 1/30] [Batch 683/816] [D loss: 0.128386, acc:  89%, op_acc: 100.00%] [G loss: 10.345397] time: 0:52:37.061698\n",
      "[Epoch 1/30] [Batch 684/816] [D loss: 0.441807, acc:  88%, op_acc: 95.00%] [G loss: 9.338943] time: 0:52:38.999829\n",
      "[Epoch 1/30] [Batch 685/816] [D loss: 0.231366, acc:  76%, op_acc: 100.00%] [G loss: 7.630577] time: 0:52:40.931646\n",
      "[Epoch 1/30] [Batch 686/816] [D loss: 0.208485, acc:  96%, op_acc: 95.00%] [G loss: 8.473841] time: 0:52:42.850907\n",
      "[Epoch 1/30] [Batch 687/816] [D loss: 0.389247, acc:  93%, op_acc: 95.00%] [G loss: 9.360255] time: 0:52:44.791116\n",
      "[Epoch 1/30] [Batch 688/816] [D loss: 0.484699, acc:  55%, op_acc: 95.00%] [G loss: 7.646319] time: 0:52:46.729379\n",
      "[Epoch 1/30] [Batch 689/816] [D loss: 0.202140, acc:  74%, op_acc: 100.00%] [G loss: 7.169001] time: 0:52:48.664216\n",
      "[Epoch 1/30] [Batch 690/816] [D loss: 0.468695, acc:  86%, op_acc: 95.00%] [G loss: 8.424756] time: 0:52:50.596049\n",
      "[Epoch 1/30] [Batch 691/816] [D loss: 2.758608, acc:  88%, op_acc: 95.00%] [G loss: 7.676448] time: 0:52:52.522788\n",
      "[Epoch 1/30] [Batch 692/816] [D loss: 0.159500, acc:  79%, op_acc: 100.00%] [G loss: 8.635616] time: 0:52:54.463902\n",
      "[Epoch 1/30] [Batch 693/816] [D loss: 0.085372, acc:  94%, op_acc: 100.00%] [G loss: 8.137986] time: 0:52:56.399270\n",
      "[Epoch 1/30] [Batch 694/816] [D loss: 0.147999, acc:  88%, op_acc: 100.00%] [G loss: 7.324183] time: 0:52:58.323955\n",
      "[Epoch 1/30] [Batch 695/816] [D loss: 0.159265, acc:  86%, op_acc: 100.00%] [G loss: 7.845627] time: 0:53:00.258662\n",
      "[Epoch 1/30] [Batch 696/816] [D loss: 0.074219, acc:  95%, op_acc: 100.00%] [G loss: 9.541456] time: 0:53:02.185961\n",
      "[Epoch 1/30] [Batch 697/816] [D loss: 0.493715, acc:  91%, op_acc: 95.00%] [G loss: 7.254611] time: 0:53:04.124888\n",
      "[Epoch 1/30] [Batch 698/816] [D loss: 0.097649, acc:  94%, op_acc: 100.00%] [G loss: 6.780278] time: 0:53:06.060703\n",
      "[Epoch 1/30] [Batch 699/816] [D loss: 0.163951, acc:  83%, op_acc: 100.00%] [G loss: 8.380766] time: 0:53:07.995150\n",
      "[Epoch 1/30] [Batch 700/816] [D loss: 0.199925, acc:  66%, op_acc: 100.00%] [G loss: 8.567373] time: 0:53:09.927438\n",
      "[Epoch 1/30] [Batch 701/816] [D loss: 0.101413, acc:  93%, op_acc: 100.00%] [G loss: 9.028932] time: 0:53:11.858859\n",
      "[Epoch 1/30] [Batch 702/816] [D loss: 0.117528, acc:  90%, op_acc: 100.00%] [G loss: 8.702041] time: 0:53:13.802223\n",
      "[Epoch 1/30] [Batch 703/816] [D loss: 0.166726, acc:  84%, op_acc: 100.00%] [G loss: 9.501194] time: 0:53:15.756940\n",
      "[Epoch 1/30] [Batch 704/816] [D loss: 0.078603, acc:  97%, op_acc: 100.00%] [G loss: 6.867234] time: 0:53:17.684426\n",
      "[Epoch 1/30] [Batch 705/816] [D loss: 0.436132, acc:  94%, op_acc: 95.00%] [G loss: 7.083924] time: 0:53:19.625512\n",
      "[Epoch 1/30] [Batch 706/816] [D loss: 0.348345, acc:  62%, op_acc: 95.00%] [G loss: 6.697666] time: 0:53:21.566686\n",
      "[Epoch 1/30] [Batch 707/816] [D loss: 0.116085, acc:  89%, op_acc: 100.00%] [G loss: 7.517028] time: 0:53:23.492356\n",
      "[Epoch 1/30] [Batch 708/816] [D loss: 0.120964, acc:  85%, op_acc: 100.00%] [G loss: 9.034853] time: 0:53:25.431269\n",
      "[Epoch 1/30] [Batch 709/816] [D loss: 0.193969, acc:  68%, op_acc: 100.00%] [G loss: 7.765201] time: 0:53:27.371040\n",
      "[Epoch 1/30] [Batch 710/816] [D loss: 0.382253, acc:  74%, op_acc: 95.00%] [G loss: 8.514448] time: 0:53:29.284537\n",
      "[Epoch 1/30] [Batch 711/816] [D loss: 0.145847, acc:  89%, op_acc: 100.00%] [G loss: 9.479910] time: 0:53:31.222499\n",
      "[Epoch 1/30] [Batch 712/816] [D loss: 0.236031, acc:  82%, op_acc: 100.00%] [G loss: 5.996891] time: 0:53:33.155805\n",
      "[Epoch 1/30] [Batch 713/816] [D loss: 0.177512, acc:  82%, op_acc: 100.00%] [G loss: 7.247293] time: 0:53:35.081339\n",
      "[Epoch 1/30] [Batch 714/816] [D loss: 1.045357, acc:  88%, op_acc: 95.00%] [G loss: 6.523837] time: 0:53:37.023479\n",
      "[Epoch 1/30] [Batch 715/816] [D loss: 0.195121, acc:  79%, op_acc: 100.00%] [G loss: 8.057290] time: 0:53:38.964420\n",
      "[Epoch 1/30] [Batch 716/816] [D loss: 0.063834, acc:  95%, op_acc: 100.00%] [G loss: 8.455675] time: 0:53:40.876808\n",
      "[Epoch 1/30] [Batch 717/816] [D loss: 1.749592, acc:  92%, op_acc: 90.00%] [G loss: 8.729889] time: 0:53:42.790158\n",
      "[Epoch 1/30] [Batch 718/816] [D loss: 0.173125, acc:  85%, op_acc: 100.00%] [G loss: 12.355521] time: 0:53:44.729590\n",
      "[Epoch 1/30] [Batch 719/816] [D loss: 3.464747, acc:  28%, op_acc: 80.00%] [G loss: 7.923460] time: 0:53:46.656399\n",
      "[Epoch 1/30] [Batch 720/816] [D loss: 0.330659, acc:  67%, op_acc: 100.00%] [G loss: 9.345720] time: 0:53:48.573203\n",
      "[Epoch 1/30] [Batch 721/816] [D loss: 2.634243, acc:  91%, op_acc: 95.00%] [G loss: 8.062246] time: 0:53:50.509659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/30] [Batch 722/816] [D loss: 1.276715, acc:  69%, op_acc: 95.00%] [G loss: 9.156111] time: 0:53:52.435967\n",
      "[Epoch 1/30] [Batch 723/816] [D loss: 2.043846, acc:  60%, op_acc: 90.00%] [G loss: 7.616390] time: 0:53:54.346589\n",
      "[Epoch 1/30] [Batch 724/816] [D loss: 0.206489, acc:  75%, op_acc: 100.00%] [G loss: 8.276230] time: 0:53:56.295163\n",
      "[Epoch 1/30] [Batch 725/816] [D loss: 0.260778, acc:  68%, op_acc: 100.00%] [G loss: 8.126133] time: 0:53:58.221417\n",
      "[Epoch 1/30] [Batch 726/816] [D loss: 2.121241, acc:  89%, op_acc: 95.00%] [G loss: 6.905454] time: 0:54:00.152620\n",
      "[Epoch 1/30] [Batch 727/816] [D loss: 0.180525, acc:  80%, op_acc: 100.00%] [G loss: 7.722677] time: 0:54:02.095532\n",
      "[Epoch 1/30] [Batch 728/816] [D loss: 0.134135, acc:  89%, op_acc: 100.00%] [G loss: 9.164671] time: 0:54:04.019772\n",
      "[Epoch 1/30] [Batch 729/816] [D loss: 0.240513, acc:  81%, op_acc: 100.00%] [G loss: 7.288038] time: 0:54:05.943512\n",
      "[Epoch 1/30] [Batch 730/816] [D loss: 0.238915, acc:  92%, op_acc: 95.00%] [G loss: 6.349214] time: 0:54:07.869201\n",
      "[Epoch 1/30] [Batch 731/816] [D loss: 0.107045, acc:  91%, op_acc: 100.00%] [G loss: 6.716499] time: 0:54:09.808999\n",
      "[Epoch 1/30] [Batch 732/816] [D loss: 0.135043, acc:  90%, op_acc: 100.00%] [G loss: 10.859787] time: 0:54:11.734948\n",
      "[Epoch 1/30] [Batch 733/816] [D loss: 0.128091, acc:  89%, op_acc: 100.00%] [G loss: 8.311546] time: 0:54:13.672669\n",
      "[Epoch 1/30] [Batch 734/816] [D loss: 0.104587, acc:  93%, op_acc: 100.00%] [G loss: 8.890232] time: 0:54:15.612384\n",
      "[Epoch 1/30] [Batch 735/816] [D loss: 0.757972, acc:  93%, op_acc: 95.00%] [G loss: 11.876990] time: 0:54:17.556814\n",
      "[Epoch 1/30] [Batch 736/816] [D loss: 0.505644, acc:  56%, op_acc: 95.00%] [G loss: 7.953139] time: 0:54:19.479739\n",
      "[Epoch 1/30] [Batch 737/816] [D loss: 0.927125, acc:  61%, op_acc: 95.00%] [G loss: 7.946531] time: 0:54:21.418713\n",
      "[Epoch 1/30] [Batch 738/816] [D loss: 0.189259, acc:  76%, op_acc: 100.00%] [G loss: 8.244292] time: 0:54:23.412583\n",
      "[Epoch 1/30] [Batch 739/816] [D loss: 0.133597, acc:  89%, op_acc: 100.00%] [G loss: 8.457338] time: 0:54:25.336247\n",
      "[Epoch 1/30] [Batch 740/816] [D loss: 0.163932, acc:  83%, op_acc: 100.00%] [G loss: 8.320469] time: 0:54:27.273894\n",
      "[Epoch 1/30] [Batch 741/816] [D loss: 0.094664, acc:  94%, op_acc: 100.00%] [G loss: 7.963798] time: 0:54:29.201276\n",
      "[Epoch 1/30] [Batch 742/816] [D loss: 0.074393, acc:  96%, op_acc: 100.00%] [G loss: 9.426315] time: 0:54:31.133379\n",
      "[Epoch 1/30] [Batch 743/816] [D loss: 0.120271, acc:  91%, op_acc: 100.00%] [G loss: 7.676225] time: 0:54:33.047324\n",
      "[Epoch 1/30] [Batch 744/816] [D loss: 0.340708, acc:  90%, op_acc: 95.00%] [G loss: 8.057734] time: 0:54:34.976733\n",
      "[Epoch 1/30] [Batch 745/816] [D loss: 0.140914, acc:  88%, op_acc: 100.00%] [G loss: 8.215256] time: 0:54:36.911917\n",
      "[Epoch 1/30] [Batch 746/816] [D loss: 0.183816, acc:  67%, op_acc: 100.00%] [G loss: 7.602112] time: 0:54:38.851904\n",
      "[Epoch 1/30] [Batch 747/816] [D loss: 0.136644, acc:  83%, op_acc: 100.00%] [G loss: 7.460023] time: 0:54:40.785068\n",
      "[Epoch 1/30] [Batch 748/816] [D loss: 0.140513, acc:  92%, op_acc: 100.00%] [G loss: 7.592953] time: 0:54:42.702212\n",
      "[Epoch 1/30] [Batch 749/816] [D loss: 0.118364, acc:  85%, op_acc: 100.00%] [G loss: 6.827676] time: 0:54:44.640214\n",
      "[Epoch 1/30] [Batch 750/816] [D loss: 0.343855, acc:  74%, op_acc: 95.00%] [G loss: 7.695180] time: 0:54:46.586633\n",
      "[Epoch 1/30] [Batch 751/816] [D loss: 2.089530, acc:  77%, op_acc: 95.00%] [G loss: 7.283447] time: 0:54:48.522063\n",
      "[Epoch 1/30] [Batch 752/816] [D loss: 0.210437, acc:  62%, op_acc: 100.00%] [G loss: 11.728187] time: 0:54:50.459496\n",
      "[Epoch 1/30] [Batch 753/816] [D loss: 0.123087, acc:  89%, op_acc: 100.00%] [G loss: 8.303406] time: 0:54:52.421181\n",
      "[Epoch 1/30] [Batch 754/816] [D loss: 0.449646, acc:  87%, op_acc: 95.00%] [G loss: 8.776579] time: 0:54:54.350711\n",
      "[Epoch 1/30] [Batch 755/816] [D loss: 0.136694, acc:  87%, op_acc: 100.00%] [G loss: 9.640064] time: 0:54:56.288692\n",
      "[Epoch 1/30] [Batch 756/816] [D loss: 0.223242, acc:  62%, op_acc: 100.00%] [G loss: 7.659948] time: 0:54:58.224081\n",
      "[Epoch 1/30] [Batch 757/816] [D loss: 2.766239, acc:  86%, op_acc: 90.00%] [G loss: 7.621640] time: 0:55:00.141878\n",
      "[Epoch 1/30] [Batch 758/816] [D loss: 0.539461, acc:  50%, op_acc: 95.00%] [G loss: 9.346090] time: 0:55:02.069085\n",
      "[Epoch 1/30] [Batch 759/816] [D loss: 0.292622, acc:  64%, op_acc: 100.00%] [G loss: 9.395147] time: 0:55:04.002776\n",
      "[Epoch 1/30] [Batch 760/816] [D loss: 0.363421, acc:  52%, op_acc: 100.00%] [G loss: 6.099434] time: 0:55:05.932508\n",
      "[Epoch 1/30] [Batch 761/816] [D loss: 0.229714, acc:  74%, op_acc: 100.00%] [G loss: 8.303655] time: 0:55:07.861522\n",
      "[Epoch 1/30] [Batch 762/816] [D loss: 0.258139, acc:  50%, op_acc: 100.00%] [G loss: 8.644956] time: 0:55:09.794262\n",
      "[Epoch 1/30] [Batch 763/816] [D loss: 2.568098, acc:  73%, op_acc: 95.00%] [G loss: 8.288815] time: 0:55:11.730251\n",
      "[Epoch 1/30] [Batch 764/816] [D loss: 0.173377, acc:  82%, op_acc: 100.00%] [G loss: 7.703066] time: 0:55:13.659648\n",
      "[Epoch 1/30] [Batch 765/816] [D loss: 0.152223, acc:  83%, op_acc: 100.00%] [G loss: 9.510601] time: 0:55:15.597866\n",
      "[Epoch 1/30] [Batch 766/816] [D loss: 0.331434, acc:  51%, op_acc: 100.00%] [G loss: 6.826174] time: 0:55:17.535373\n",
      "[Epoch 1/30] [Batch 767/816] [D loss: 0.148790, acc:  86%, op_acc: 100.00%] [G loss: 7.740591] time: 0:55:19.457375\n",
      "[Epoch 1/30] [Batch 768/816] [D loss: 0.207297, acc:  67%, op_acc: 100.00%] [G loss: 8.607019] time: 0:55:21.400570\n",
      "[Epoch 1/30] [Batch 769/816] [D loss: 0.113480, acc:  94%, op_acc: 100.00%] [G loss: 7.002529] time: 0:55:23.365773\n",
      "[Epoch 1/30] [Batch 770/816] [D loss: 0.171010, acc:  75%, op_acc: 100.00%] [G loss: 8.199121] time: 0:55:25.301158\n",
      "[Epoch 1/30] [Batch 771/816] [D loss: 0.157638, acc:  86%, op_acc: 100.00%] [G loss: 7.830541] time: 0:55:27.238918\n",
      "[Epoch 1/30] [Batch 772/816] [D loss: 0.356553, acc:  87%, op_acc: 95.00%] [G loss: 7.915843] time: 0:55:29.182739\n",
      "[Epoch 1/30] [Batch 773/816] [D loss: 0.517059, acc:  81%, op_acc: 95.00%] [G loss: 7.978544] time: 0:55:31.121620\n",
      "[Epoch 1/30] [Batch 774/816] [D loss: 0.833840, acc:  84%, op_acc: 90.00%] [G loss: 7.803788] time: 0:55:33.043663\n",
      "[Epoch 1/30] [Batch 775/816] [D loss: 1.732642, acc:  84%, op_acc: 95.00%] [G loss: 6.470794] time: 0:55:34.954364\n",
      "[Epoch 1/30] [Batch 776/816] [D loss: 0.078508, acc:  96%, op_acc: 100.00%] [G loss: 9.391568] time: 0:55:36.892313\n",
      "[Epoch 1/30] [Batch 777/816] [D loss: 1.334958, acc:  77%, op_acc: 90.00%] [G loss: 9.972211] time: 0:55:38.826142\n",
      "[Epoch 1/30] [Batch 778/816] [D loss: 0.190179, acc:  78%, op_acc: 100.00%] [G loss: 9.441255] time: 0:55:40.754122\n",
      "[Epoch 1/30] [Batch 779/816] [D loss: 0.229556, acc:  78%, op_acc: 100.00%] [G loss: 8.317373] time: 0:55:42.676269\n",
      "[Epoch 1/30] [Batch 780/816] [D loss: 0.633048, acc:  94%, op_acc: 95.00%] [G loss: 8.108729] time: 0:55:44.601821\n",
      "[Epoch 1/30] [Batch 781/816] [D loss: 2.460920, acc:  93%, op_acc: 90.00%] [G loss: 7.715725] time: 0:55:46.543405\n",
      "[Epoch 1/30] [Batch 782/816] [D loss: 0.709219, acc:  78%, op_acc: 95.00%] [G loss: 7.785058] time: 0:55:48.477022\n",
      "[Epoch 1/30] [Batch 783/816] [D loss: 0.197212, acc:  82%, op_acc: 100.00%] [G loss: 8.096686] time: 0:55:50.417759\n",
      "[Epoch 1/30] [Batch 784/816] [D loss: 0.146063, acc:  90%, op_acc: 100.00%] [G loss: 8.367142] time: 0:55:52.355829\n",
      "[Epoch 1/30] [Batch 785/816] [D loss: 0.229859, acc:  90%, op_acc: 95.00%] [G loss: 8.551089] time: 0:55:54.291321\n",
      "[Epoch 1/30] [Batch 786/816] [D loss: 0.192308, acc:  74%, op_acc: 100.00%] [G loss: 7.616827] time: 0:55:56.222778\n",
      "[Epoch 1/30] [Batch 787/816] [D loss: 0.180326, acc:  82%, op_acc: 100.00%] [G loss: 8.846498] time: 0:55:58.163850\n",
      "[Epoch 1/30] [Batch 788/816] [D loss: 0.879516, acc:  82%, op_acc: 90.00%] [G loss: 7.347661] time: 0:56:00.100510\n",
      "[Epoch 1/30] [Batch 789/816] [D loss: 0.151978, acc:  80%, op_acc: 100.00%] [G loss: 8.100231] time: 0:56:02.033715\n",
      "[Epoch 1/30] [Batch 790/816] [D loss: 0.078683, acc:  94%, op_acc: 100.00%] [G loss: 8.304161] time: 0:56:03.970625\n",
      "[Epoch 1/30] [Batch 791/816] [D loss: 0.674888, acc:  45%, op_acc: 95.00%] [G loss: 6.724523] time: 0:56:05.896722\n",
      "[Epoch 1/30] [Batch 792/816] [D loss: 0.387511, acc:  78%, op_acc: 95.00%] [G loss: 8.042750] time: 0:56:07.828084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/30] [Batch 793/816] [D loss: 0.587795, acc:  93%, op_acc: 95.00%] [G loss: 8.169079] time: 0:56:09.757442\n",
      "[Epoch 1/30] [Batch 794/816] [D loss: 0.155640, acc:  83%, op_acc: 100.00%] [G loss: 7.362740] time: 0:56:11.672354\n",
      "[Epoch 1/30] [Batch 795/816] [D loss: 0.078424, acc:  95%, op_acc: 100.00%] [G loss: 7.642003] time: 0:56:13.613768\n",
      "[Epoch 1/30] [Batch 796/816] [D loss: 0.076854, acc:  96%, op_acc: 100.00%] [G loss: 7.886372] time: 0:56:15.557747\n",
      "[Epoch 1/30] [Batch 797/816] [D loss: 0.147763, acc:  87%, op_acc: 100.00%] [G loss: 8.371256] time: 0:56:17.500738\n",
      "[Epoch 1/30] [Batch 798/816] [D loss: 0.080242, acc:  96%, op_acc: 100.00%] [G loss: 9.220791] time: 0:56:19.438111\n",
      "[Epoch 1/30] [Batch 799/816] [D loss: 0.074091, acc:  95%, op_acc: 100.00%] [G loss: 8.142314] time: 0:56:21.381863\n",
      "[Epoch 1/30] [Batch 800/816] [D loss: 0.088394, acc:  95%, op_acc: 100.00%] [G loss: 8.641461] time: 0:56:23.304668\n",
      "SAVE\n",
      "1_800-T1-T2.png\n",
      "[Epoch 1/30] [Batch 801/816] [D loss: 0.042278, acc:  98%, op_acc: 100.00%] [G loss: 9.025454] time: 0:56:25.451969\n",
      "[Epoch 1/30] [Batch 802/816] [D loss: 1.210929, acc:  82%, op_acc: 95.00%] [G loss: 8.494409] time: 0:56:27.380311\n",
      "[Epoch 1/30] [Batch 803/816] [D loss: 0.102133, acc:  93%, op_acc: 100.00%] [G loss: 8.715897] time: 0:56:29.316358\n",
      "[Epoch 1/30] [Batch 804/816] [D loss: 0.080011, acc:  96%, op_acc: 100.00%] [G loss: 8.149344] time: 0:56:31.285266\n",
      "[Epoch 1/30] [Batch 805/816] [D loss: 0.145672, acc:  79%, op_acc: 100.00%] [G loss: 8.149201] time: 0:56:33.209341\n",
      "[Epoch 1/30] [Batch 806/816] [D loss: 0.176870, acc:  71%, op_acc: 100.00%] [G loss: 7.407088] time: 0:56:35.137232\n",
      "save generator\n",
      "[Epoch 1/30] [Batch 807/816] [D loss: 0.088731, acc:  94%, op_acc: 100.00%] [G loss: 8.295523] time: 0:56:42.065435\n",
      "[Epoch 1/30] [Batch 808/816] [D loss: 0.082738, acc:  94%, op_acc: 100.00%] [G loss: 7.568416] time: 0:56:43.895215\n",
      "[Epoch 1/30] [Batch 809/816] [D loss: 2.714037, acc:  70%, op_acc: 95.00%] [G loss: 8.985629] time: 0:56:45.799412\n",
      "[Epoch 1/30] [Batch 810/816] [D loss: 2.010901, acc:  94%, op_acc: 95.00%] [G loss: 7.208755] time: 0:56:47.738651\n",
      "[Epoch 1/30] [Batch 811/816] [D loss: 0.219603, acc:  81%, op_acc: 100.00%] [G loss: 8.596578] time: 0:56:49.678260\n",
      "[Epoch 1/30] [Batch 812/816] [D loss: 0.169281, acc:  75%, op_acc: 100.00%] [G loss: 7.360324] time: 0:56:51.624381\n",
      "[Epoch 1/30] [Batch 813/816] [D loss: 0.294712, acc:  53%, op_acc: 100.00%] [G loss: 7.274333] time: 0:56:53.570789\n",
      "[Epoch 1/30] [Batch 814/816] [D loss: 0.156703, acc:  87%, op_acc: 100.00%] [G loss: 8.377892] time: 0:56:55.508413\n",
      "[Epoch 1/30] [Batch 815/816] [D loss: 0.342215, acc:  57%, op_acc: 100.00%] [G loss: 7.961607] time: 0:56:57.445321\n",
      "[Epoch 2/30] [Batch 0/816] [D loss: 0.247700, acc:  77%, op_acc: 100.00%] [G loss: 6.935716] time: 0:56:59.405935\n",
      "SAVE\n",
      "2_0-T2-PD.png\n",
      "[Epoch 2/30] [Batch 1/816] [D loss: 0.072564, acc:  93%, op_acc: 100.00%] [G loss: 7.873077] time: 0:57:01.521360\n",
      "[Epoch 2/30] [Batch 2/816] [D loss: 0.086914, acc:  95%, op_acc: 100.00%] [G loss: 7.641083] time: 0:57:03.450637\n",
      "save generator\n",
      "[Epoch 2/30] [Batch 3/816] [D loss: 0.200717, acc:  98%, op_acc: 95.00%] [G loss: 10.636770] time: 0:57:07.083756\n",
      "[Epoch 2/30] [Batch 4/816] [D loss: 1.614320, acc:  91%, op_acc: 95.00%] [G loss: 9.844650] time: 0:57:09.009655\n",
      "[Epoch 2/30] [Batch 5/816] [D loss: 0.525971, acc:  72%, op_acc: 100.00%] [G loss: 8.356099] time: 0:57:10.945258\n",
      "[Epoch 2/30] [Batch 6/816] [D loss: 1.737627, acc:  80%, op_acc: 95.00%] [G loss: 9.584891] time: 0:57:12.879466\n",
      "[Epoch 2/30] [Batch 7/816] [D loss: 1.015315, acc:  54%, op_acc: 90.00%] [G loss: 7.498001] time: 0:57:14.812577\n",
      "[Epoch 2/30] [Batch 8/816] [D loss: 1.383028, acc:  75%, op_acc: 90.00%] [G loss: 7.090686] time: 0:57:16.728227\n",
      "[Epoch 2/30] [Batch 9/816] [D loss: 0.242176, acc:  76%, op_acc: 100.00%] [G loss: 8.192473] time: 0:57:18.654880\n",
      "[Epoch 2/30] [Batch 10/816] [D loss: 0.113021, acc:  92%, op_acc: 100.00%] [G loss: 11.076642] time: 0:57:20.579303\n",
      "[Epoch 2/30] [Batch 11/816] [D loss: 1.599056, acc:  90%, op_acc: 85.00%] [G loss: 7.727470] time: 0:57:22.517927\n",
      "[Epoch 2/30] [Batch 12/816] [D loss: 0.142861, acc:  84%, op_acc: 100.00%] [G loss: 8.298620] time: 0:57:24.461105\n",
      "[Epoch 2/30] [Batch 13/816] [D loss: 0.094115, acc:  93%, op_acc: 100.00%] [G loss: 8.787598] time: 0:57:26.391629\n",
      "[Epoch 2/30] [Batch 14/816] [D loss: 0.078701, acc:  98%, op_acc: 100.00%] [G loss: 9.722980] time: 0:57:28.320498\n",
      "[Epoch 2/30] [Batch 15/816] [D loss: 0.162211, acc:  79%, op_acc: 100.00%] [G loss: 6.799344] time: 0:57:30.252788\n",
      "[Epoch 2/30] [Batch 16/816] [D loss: 0.162653, acc:  82%, op_acc: 100.00%] [G loss: 8.596857] time: 0:57:32.187394\n",
      "[Epoch 2/30] [Batch 17/816] [D loss: 0.160439, acc:  81%, op_acc: 100.00%] [G loss: 7.207731] time: 0:57:34.116523\n",
      "[Epoch 2/30] [Batch 18/816] [D loss: 0.181880, acc:  83%, op_acc: 100.00%] [G loss: 8.257164] time: 0:57:36.033500\n",
      "[Epoch 2/30] [Batch 19/816] [D loss: 0.415269, acc:  98%, op_acc: 95.00%] [G loss: 7.307465] time: 0:57:37.974089\n",
      "[Epoch 2/30] [Batch 20/816] [D loss: 4.371829, acc:  94%, op_acc: 90.00%] [G loss: 9.891923] time: 0:57:39.906439\n",
      "[Epoch 2/30] [Batch 21/816] [D loss: 0.106439, acc:  91%, op_acc: 100.00%] [G loss: 8.261226] time: 0:57:41.827630\n",
      "[Epoch 2/30] [Batch 22/816] [D loss: 0.107002, acc:  94%, op_acc: 100.00%] [G loss: 8.297536] time: 0:57:43.759661\n",
      "[Epoch 2/30] [Batch 23/816] [D loss: 0.956670, acc:  85%, op_acc: 95.00%] [G loss: 7.727145] time: 0:57:45.845866\n",
      "[Epoch 2/30] [Batch 24/816] [D loss: 0.202989, acc:  75%, op_acc: 100.00%] [G loss: 6.625278] time: 0:57:47.768962\n",
      "[Epoch 2/30] [Batch 25/816] [D loss: 0.159754, acc:  89%, op_acc: 100.00%] [G loss: 7.541591] time: 0:57:49.707225\n",
      "[Epoch 2/30] [Batch 26/816] [D loss: 1.978533, acc:  73%, op_acc: 95.00%] [G loss: 8.107229] time: 0:57:51.644317\n",
      "[Epoch 2/30] [Batch 27/816] [D loss: 0.154831, acc:  83%, op_acc: 100.00%] [G loss: 7.333431] time: 0:57:53.582907\n",
      "[Epoch 2/30] [Batch 28/816] [D loss: 0.391038, acc:  97%, op_acc: 95.00%] [G loss: 7.999896] time: 0:57:55.510210\n",
      "[Epoch 2/30] [Batch 29/816] [D loss: 0.077576, acc:  95%, op_acc: 100.00%] [G loss: 8.078421] time: 0:57:57.449519\n",
      "[Epoch 2/30] [Batch 30/816] [D loss: 0.138631, acc:  87%, op_acc: 100.00%] [G loss: 8.974057] time: 0:57:59.395377\n",
      "[Epoch 2/30] [Batch 31/816] [D loss: 0.090885, acc:  93%, op_acc: 100.00%] [G loss: 7.301418] time: 0:58:01.316653\n",
      "[Epoch 2/30] [Batch 32/816] [D loss: 0.071911, acc:  95%, op_acc: 100.00%] [G loss: 8.888953] time: 0:58:03.253424\n",
      "[Epoch 2/30] [Batch 33/816] [D loss: 0.295861, acc:  95%, op_acc: 95.00%] [G loss: 8.205032] time: 0:58:05.173733\n",
      "[Epoch 2/30] [Batch 34/816] [D loss: 0.085613, acc:  96%, op_acc: 100.00%] [G loss: 6.940379] time: 0:58:07.114554\n",
      "[Epoch 2/30] [Batch 35/816] [D loss: 0.059091, acc:  97%, op_acc: 100.00%] [G loss: 7.469116] time: 0:58:09.050000\n",
      "[Epoch 2/30] [Batch 36/816] [D loss: 0.968484, acc:  91%, op_acc: 95.00%] [G loss: 7.461903] time: 0:58:10.969689\n",
      "[Epoch 2/30] [Batch 37/816] [D loss: 0.079676, acc:  93%, op_acc: 100.00%] [G loss: 8.514379] time: 0:58:12.887211\n",
      "[Epoch 2/30] [Batch 38/816] [D loss: 0.078387, acc:  95%, op_acc: 100.00%] [G loss: 8.091915] time: 0:58:14.802520\n",
      "[Epoch 2/30] [Batch 39/816] [D loss: 0.068932, acc:  96%, op_acc: 100.00%] [G loss: 8.916398] time: 0:58:16.733199\n",
      "[Epoch 2/30] [Batch 40/816] [D loss: 0.978229, acc:  68%, op_acc: 95.00%] [G loss: 7.219547] time: 0:58:18.658389\n",
      "[Epoch 2/30] [Batch 41/816] [D loss: 0.107545, acc:  91%, op_acc: 100.00%] [G loss: 8.575799] time: 0:58:20.575896\n",
      "[Epoch 2/30] [Batch 42/816] [D loss: 0.152642, acc:  85%, op_acc: 100.00%] [G loss: 6.989542] time: 0:58:22.497971\n",
      "[Epoch 2/30] [Batch 43/816] [D loss: 0.095488, acc:  92%, op_acc: 100.00%] [G loss: 8.319953] time: 0:58:24.430878\n",
      "[Epoch 2/30] [Batch 44/816] [D loss: 0.133835, acc:  87%, op_acc: 100.00%] [G loss: 7.254422] time: 0:58:26.367177\n",
      "[Epoch 2/30] [Batch 45/816] [D loss: 0.396745, acc:  95%, op_acc: 95.00%] [G loss: 8.016293] time: 0:58:28.299778\n",
      "[Epoch 2/30] [Batch 46/816] [D loss: 0.316075, acc:  88%, op_acc: 95.00%] [G loss: 6.748843] time: 0:58:30.218282\n",
      "[Epoch 2/30] [Batch 47/816] [D loss: 0.141277, acc:  86%, op_acc: 100.00%] [G loss: 7.884797] time: 0:58:32.155830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/30] [Batch 48/816] [D loss: 0.555905, acc:  86%, op_acc: 95.00%] [G loss: 15.520541] time: 0:58:34.092447\n",
      "[Epoch 2/30] [Batch 49/816] [D loss: 3.775545, acc:  70%, op_acc: 95.00%] [G loss: 14.260210] time: 0:58:36.033742\n",
      "[Epoch 2/30] [Batch 50/816] [D loss: 0.129799, acc:  91%, op_acc: 100.00%] [G loss: 10.698509] time: 0:58:37.971514\n",
      "[Epoch 2/30] [Batch 51/816] [D loss: 3.643901, acc:  84%, op_acc: 90.00%] [G loss: 10.327879] time: 0:58:39.910443\n",
      "[Epoch 2/30] [Batch 52/816] [D loss: 0.090093, acc:  91%, op_acc: 100.00%] [G loss: 8.523052] time: 0:58:41.853990\n",
      "[Epoch 2/30] [Batch 53/816] [D loss: 0.078660, acc:  93%, op_acc: 100.00%] [G loss: 11.866480] time: 0:58:43.795121\n",
      "[Epoch 2/30] [Batch 54/816] [D loss: 0.298388, acc:  86%, op_acc: 95.00%] [G loss: 7.562788] time: 0:58:45.724927\n",
      "[Epoch 2/30] [Batch 55/816] [D loss: 0.110884, acc:  88%, op_acc: 100.00%] [G loss: 9.591752] time: 0:58:47.659412\n",
      "[Epoch 2/30] [Batch 56/816] [D loss: 0.888136, acc:  96%, op_acc: 85.00%] [G loss: 7.374809] time: 0:58:49.591530\n",
      "[Epoch 2/30] [Batch 57/816] [D loss: 0.142731, acc:  92%, op_acc: 100.00%] [G loss: 9.425273] time: 0:58:51.529491\n",
      "[Epoch 2/30] [Batch 58/816] [D loss: 0.552957, acc:  85%, op_acc: 100.00%] [G loss: 8.546354] time: 0:58:53.476390\n",
      "[Epoch 2/30] [Batch 59/816] [D loss: 0.178205, acc:  95%, op_acc: 100.00%] [G loss: 10.782468] time: 0:58:55.415057\n",
      "[Epoch 2/30] [Batch 60/816] [D loss: 0.805663, acc:  86%, op_acc: 95.00%] [G loss: 8.903743] time: 0:58:57.345760\n",
      "[Epoch 2/30] [Batch 61/816] [D loss: 0.084812, acc:  95%, op_acc: 100.00%] [G loss: 8.138641] time: 0:58:59.282539\n",
      "[Epoch 2/30] [Batch 62/816] [D loss: 0.191511, acc:  84%, op_acc: 95.00%] [G loss: 8.784263] time: 0:59:01.220755\n",
      "[Epoch 2/30] [Batch 63/816] [D loss: 0.117651, acc:  95%, op_acc: 95.00%] [G loss: 8.908497] time: 0:59:03.159286\n",
      "[Epoch 2/30] [Batch 64/816] [D loss: 0.092638, acc:  96%, op_acc: 100.00%] [G loss: 9.037665] time: 0:59:05.091205\n",
      "[Epoch 2/30] [Batch 65/816] [D loss: 0.093836, acc:  93%, op_acc: 100.00%] [G loss: 9.080367] time: 0:59:07.025277\n",
      "[Epoch 2/30] [Batch 66/816] [D loss: 0.729542, acc:  96%, op_acc: 95.00%] [G loss: 10.085120] time: 0:59:08.956829\n",
      "[Epoch 2/30] [Batch 67/816] [D loss: 0.042523, acc:  98%, op_acc: 100.00%] [G loss: 10.545018] time: 0:59:10.905234\n",
      "[Epoch 2/30] [Batch 68/816] [D loss: 0.113735, acc:  91%, op_acc: 100.00%] [G loss: 8.112205] time: 0:59:12.846435\n",
      "[Epoch 2/30] [Batch 69/816] [D loss: 0.201249, acc:  77%, op_acc: 100.00%] [G loss: 8.960206] time: 0:59:14.782852\n",
      "[Epoch 2/30] [Batch 70/816] [D loss: 0.140591, acc:  85%, op_acc: 100.00%] [G loss: 9.426820] time: 0:59:16.723925\n",
      "[Epoch 2/30] [Batch 71/816] [D loss: 0.183956, acc:  91%, op_acc: 100.00%] [G loss: 8.651560] time: 0:59:18.656698\n",
      "[Epoch 2/30] [Batch 72/816] [D loss: 0.077768, acc:  96%, op_acc: 100.00%] [G loss: 8.497505] time: 0:59:20.578289\n",
      "[Epoch 2/30] [Batch 73/816] [D loss: 0.145393, acc:  89%, op_acc: 100.00%] [G loss: 7.509064] time: 0:59:22.510078\n",
      "[Epoch 2/30] [Batch 74/816] [D loss: 0.057807, acc:  96%, op_acc: 100.00%] [G loss: 7.798961] time: 0:59:24.453731\n",
      "[Epoch 2/30] [Batch 75/816] [D loss: 0.089793, acc:  94%, op_acc: 100.00%] [G loss: 8.677135] time: 0:59:26.383310\n",
      "[Epoch 2/30] [Batch 76/816] [D loss: 0.470807, acc:  90%, op_acc: 90.00%] [G loss: 6.491878] time: 0:59:28.317292\n",
      "[Epoch 2/30] [Batch 77/816] [D loss: 0.051368, acc:  98%, op_acc: 100.00%] [G loss: 8.323035] time: 0:59:30.259004\n",
      "[Epoch 2/30] [Batch 78/816] [D loss: 0.055353, acc:  97%, op_acc: 100.00%] [G loss: 8.707758] time: 0:59:32.199708\n",
      "[Epoch 2/30] [Batch 79/816] [D loss: 0.048014, acc:  98%, op_acc: 100.00%] [G loss: 8.239513] time: 0:59:34.136372\n",
      "[Epoch 2/30] [Batch 80/816] [D loss: 0.077428, acc:  94%, op_acc: 100.00%] [G loss: 9.437788] time: 0:59:36.073052\n",
      "[Epoch 2/30] [Batch 81/816] [D loss: 0.121787, acc:  90%, op_acc: 100.00%] [G loss: 7.189357] time: 0:59:38.006423\n",
      "[Epoch 2/30] [Batch 82/816] [D loss: 0.078784, acc:  92%, op_acc: 100.00%] [G loss: 8.214414] time: 0:59:39.944352\n",
      "[Epoch 2/30] [Batch 83/816] [D loss: 1.067704, acc:  99%, op_acc: 95.00%] [G loss: 8.557776] time: 0:59:41.863830\n",
      "[Epoch 2/30] [Batch 84/816] [D loss: 0.092382, acc:  96%, op_acc: 100.00%] [G loss: 7.488846] time: 0:59:43.804465\n",
      "[Epoch 2/30] [Batch 85/816] [D loss: 0.104363, acc:  92%, op_acc: 100.00%] [G loss: 10.112501] time: 0:59:45.744786\n",
      "[Epoch 2/30] [Batch 86/816] [D loss: 0.165003, acc:  84%, op_acc: 100.00%] [G loss: 7.318636] time: 0:59:47.680584\n",
      "[Epoch 2/30] [Batch 87/816] [D loss: 0.426598, acc:  97%, op_acc: 95.00%] [G loss: 8.542449] time: 0:59:49.617072\n",
      "[Epoch 2/30] [Batch 88/816] [D loss: 2.326080, acc:  86%, op_acc: 95.00%] [G loss: 8.115519] time: 0:59:51.543494\n",
      "[Epoch 2/30] [Batch 89/816] [D loss: 0.130919, acc:  89%, op_acc: 100.00%] [G loss: 10.746226] time: 0:59:53.481322\n",
      "[Epoch 2/30] [Batch 90/816] [D loss: 0.146295, acc:  82%, op_acc: 100.00%] [G loss: 9.906488] time: 0:59:55.418518\n",
      "[Epoch 2/30] [Batch 91/816] [D loss: 0.176705, acc:  81%, op_acc: 100.00%] [G loss: 9.407882] time: 0:59:57.352554\n",
      "[Epoch 2/30] [Batch 92/816] [D loss: 0.106428, acc:  93%, op_acc: 100.00%] [G loss: 10.593073] time: 0:59:59.288300\n",
      "[Epoch 2/30] [Batch 93/816] [D loss: 0.054751, acc:  96%, op_acc: 100.00%] [G loss: 9.326029] time: 1:00:01.226062\n",
      "[Epoch 2/30] [Batch 94/816] [D loss: 0.075325, acc:  94%, op_acc: 100.00%] [G loss: 9.934184] time: 1:00:03.167833\n",
      "[Epoch 2/30] [Batch 95/816] [D loss: 0.041094, acc:  99%, op_acc: 100.00%] [G loss: 8.949367] time: 1:00:05.109068\n",
      "[Epoch 2/30] [Batch 96/816] [D loss: 0.041747, acc:  98%, op_acc: 100.00%] [G loss: 8.764235] time: 1:00:07.031980\n",
      "[Epoch 2/30] [Batch 97/816] [D loss: 0.046829, acc:  98%, op_acc: 100.00%] [G loss: 8.053269] time: 1:00:08.953648\n",
      "[Epoch 2/30] [Batch 98/816] [D loss: 0.090082, acc:  94%, op_acc: 100.00%] [G loss: 8.827529] time: 1:00:10.888344\n",
      "[Epoch 2/30] [Batch 99/816] [D loss: 0.867283, acc:  92%, op_acc: 95.00%] [G loss: 9.585509] time: 1:00:12.827627\n",
      "[Epoch 2/30] [Batch 100/816] [D loss: 0.146646, acc:  84%, op_acc: 100.00%] [G loss: 8.519588] time: 1:00:14.776561\n",
      "[Epoch 2/30] [Batch 101/816] [D loss: 1.318834, acc:  91%, op_acc: 95.00%] [G loss: 8.833477] time: 1:00:16.700955\n",
      "[Epoch 2/30] [Batch 102/816] [D loss: 2.429093, acc:  97%, op_acc: 90.00%] [G loss: 9.982876] time: 1:00:18.641609\n",
      "[Epoch 2/30] [Batch 103/816] [D loss: 0.136835, acc:  88%, op_acc: 100.00%] [G loss: 8.866883] time: 1:00:20.577632\n",
      "[Epoch 2/30] [Batch 104/816] [D loss: 0.074281, acc:  94%, op_acc: 100.00%] [G loss: 8.464783] time: 1:00:22.507650\n",
      "[Epoch 2/30] [Batch 105/816] [D loss: 4.829537, acc:  93%, op_acc: 95.00%] [G loss: 8.574749] time: 1:00:24.435639\n",
      "[Epoch 2/30] [Batch 106/816] [D loss: 0.113366, acc:  91%, op_acc: 100.00%] [G loss: 6.444726] time: 1:00:26.354145\n",
      "[Epoch 2/30] [Batch 107/816] [D loss: 0.115057, acc:  90%, op_acc: 100.00%] [G loss: 7.853044] time: 1:00:28.293317\n",
      "[Epoch 2/30] [Batch 108/816] [D loss: 0.184631, acc:  79%, op_acc: 100.00%] [G loss: 6.896463] time: 1:00:30.227524\n",
      "[Epoch 2/30] [Batch 109/816] [D loss: 0.085147, acc:  93%, op_acc: 100.00%] [G loss: 8.519267] time: 1:00:32.163873\n",
      "[Epoch 2/30] [Batch 110/816] [D loss: 0.197578, acc:  79%, op_acc: 100.00%] [G loss: 8.861113] time: 1:00:34.087035\n",
      "[Epoch 2/30] [Batch 111/816] [D loss: 0.684670, acc:  95%, op_acc: 95.00%] [G loss: 7.987277] time: 1:00:36.028541\n",
      "[Epoch 2/30] [Batch 112/816] [D loss: 0.112077, acc:  87%, op_acc: 100.00%] [G loss: 7.819733] time: 1:00:37.968866\n",
      "[Epoch 2/30] [Batch 113/816] [D loss: 0.113056, acc:  90%, op_acc: 100.00%] [G loss: 7.469626] time: 1:00:39.906835\n",
      "[Epoch 2/30] [Batch 114/816] [D loss: 0.102071, acc:  93%, op_acc: 100.00%] [G loss: 8.719052] time: 1:00:41.845296\n",
      "[Epoch 2/30] [Batch 115/816] [D loss: 0.112900, acc:  91%, op_acc: 100.00%] [G loss: 6.979570] time: 1:00:43.779656\n",
      "[Epoch 2/30] [Batch 116/816] [D loss: 0.136014, acc:  88%, op_acc: 100.00%] [G loss: 7.517030] time: 1:00:45.717677\n",
      "[Epoch 2/30] [Batch 117/816] [D loss: 0.069370, acc:  96%, op_acc: 100.00%] [G loss: 9.493151] time: 1:00:47.661542\n",
      "[Epoch 2/30] [Batch 118/816] [D loss: 0.278538, acc:  74%, op_acc: 100.00%] [G loss: 9.130478] time: 1:00:49.600015\n",
      "[Epoch 2/30] [Batch 119/816] [D loss: 0.302123, acc:  81%, op_acc: 100.00%] [G loss: 7.727482] time: 1:00:51.541541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/30] [Batch 120/816] [D loss: 0.182667, acc:  74%, op_acc: 100.00%] [G loss: 8.673884] time: 1:00:53.478371\n",
      "[Epoch 2/30] [Batch 121/816] [D loss: 0.139254, acc:  93%, op_acc: 100.00%] [G loss: 7.094369] time: 1:00:55.411307\n",
      "[Epoch 2/30] [Batch 122/816] [D loss: 2.470126, acc:  88%, op_acc: 90.00%] [G loss: 7.012941] time: 1:00:57.339319\n",
      "[Epoch 2/30] [Batch 123/816] [D loss: 0.089107, acc:  94%, op_acc: 100.00%] [G loss: 7.293682] time: 1:00:59.268150\n",
      "[Epoch 2/30] [Batch 124/816] [D loss: 0.088325, acc:  93%, op_acc: 100.00%] [G loss: 8.741898] time: 1:01:01.202410\n",
      "[Epoch 2/30] [Batch 125/816] [D loss: 1.508701, acc:  93%, op_acc: 95.00%] [G loss: 7.941904] time: 1:01:03.136716\n",
      "[Epoch 2/30] [Batch 126/816] [D loss: 0.177599, acc:  79%, op_acc: 100.00%] [G loss: 6.467797] time: 1:01:05.059797\n",
      "[Epoch 2/30] [Batch 127/816] [D loss: 0.136568, acc:  95%, op_acc: 95.00%] [G loss: 7.544991] time: 1:01:06.981039\n",
      "[Epoch 2/30] [Batch 128/816] [D loss: 0.039571, acc:  98%, op_acc: 100.00%] [G loss: 9.074165] time: 1:01:08.927901\n",
      "[Epoch 2/30] [Batch 129/816] [D loss: 0.077371, acc:  96%, op_acc: 100.00%] [G loss: 7.628839] time: 1:01:10.863667\n",
      "[Epoch 2/30] [Batch 130/816] [D loss: 0.046007, acc:  96%, op_acc: 100.00%] [G loss: 7.909417] time: 1:01:12.798220\n",
      "[Epoch 2/30] [Batch 131/816] [D loss: 0.072006, acc:  97%, op_acc: 100.00%] [G loss: 7.830308] time: 1:01:14.735924\n",
      "[Epoch 2/30] [Batch 132/816] [D loss: 0.054780, acc:  96%, op_acc: 100.00%] [G loss: 8.670689] time: 1:01:16.674583\n",
      "[Epoch 2/30] [Batch 133/816] [D loss: 0.154913, acc:  82%, op_acc: 100.00%] [G loss: 7.054849] time: 1:01:18.612861\n",
      "[Epoch 2/30] [Batch 134/816] [D loss: 0.046813, acc:  98%, op_acc: 100.00%] [G loss: 8.487735] time: 1:01:20.541744\n",
      "[Epoch 2/30] [Batch 135/816] [D loss: 0.626743, acc:  98%, op_acc: 95.00%] [G loss: 7.787251] time: 1:01:22.462425\n",
      "[Epoch 2/30] [Batch 136/816] [D loss: 0.066637, acc:  95%, op_acc: 100.00%] [G loss: 8.214600] time: 1:01:24.399325\n",
      "[Epoch 2/30] [Batch 137/816] [D loss: 0.058226, acc:  97%, op_acc: 100.00%] [G loss: 7.831882] time: 1:01:26.339850\n",
      "[Epoch 2/30] [Batch 138/816] [D loss: 0.457789, acc:  99%, op_acc: 95.00%] [G loss: 7.936227] time: 1:01:28.270549\n",
      "[Epoch 2/30] [Batch 139/816] [D loss: 2.968834, acc:  96%, op_acc: 85.00%] [G loss: 11.036614] time: 1:01:30.194549\n",
      "[Epoch 2/30] [Batch 140/816] [D loss: 0.188585, acc:  76%, op_acc: 100.00%] [G loss: 8.918085] time: 1:01:32.123060\n",
      "[Epoch 2/30] [Batch 141/816] [D loss: 2.632239, acc:  90%, op_acc: 90.00%] [G loss: 8.873776] time: 1:01:34.053447\n",
      "[Epoch 2/30] [Batch 142/816] [D loss: 0.145709, acc:  84%, op_acc: 100.00%] [G loss: 8.319662] time: 1:01:35.988666\n",
      "[Epoch 2/30] [Batch 143/816] [D loss: 0.112009, acc:  93%, op_acc: 100.00%] [G loss: 7.882883] time: 1:01:37.917223\n",
      "[Epoch 2/30] [Batch 144/816] [D loss: 0.722410, acc:  96%, op_acc: 95.00%] [G loss: 7.754500] time: 1:01:39.853004\n",
      "[Epoch 2/30] [Batch 145/816] [D loss: 1.396319, acc:  98%, op_acc: 90.00%] [G loss: 8.745965] time: 1:01:41.779975\n",
      "[Epoch 2/30] [Batch 146/816] [D loss: 0.256324, acc:  98%, op_acc: 95.00%] [G loss: 8.929007] time: 1:01:43.706264\n",
      "[Epoch 2/30] [Batch 147/816] [D loss: 0.145890, acc:  80%, op_acc: 100.00%] [G loss: 7.724170] time: 1:01:45.642631\n",
      "[Epoch 2/30] [Batch 148/816] [D loss: 0.151320, acc:  82%, op_acc: 100.00%] [G loss: 6.896728] time: 1:01:47.578475\n",
      "[Epoch 2/30] [Batch 149/816] [D loss: 0.182349, acc:  77%, op_acc: 100.00%] [G loss: 7.933204] time: 1:01:49.509395\n",
      "[Epoch 2/30] [Batch 150/816] [D loss: 0.093678, acc:  94%, op_acc: 100.00%] [G loss: 8.719595] time: 1:01:51.435408\n",
      "[Epoch 2/30] [Batch 151/816] [D loss: 0.082404, acc:  95%, op_acc: 100.00%] [G loss: 6.872480] time: 1:01:53.381438\n",
      "[Epoch 2/30] [Batch 152/816] [D loss: 1.559504, acc:  88%, op_acc: 95.00%] [G loss: 6.909639] time: 1:01:55.314238\n",
      "[Epoch 2/30] [Batch 153/816] [D loss: 0.062717, acc:  96%, op_acc: 100.00%] [G loss: 7.366412] time: 1:01:57.240785\n",
      "[Epoch 2/30] [Batch 154/816] [D loss: 0.082168, acc:  94%, op_acc: 100.00%] [G loss: 9.903214] time: 1:01:59.185242\n",
      "[Epoch 2/30] [Batch 155/816] [D loss: 0.077162, acc:  96%, op_acc: 100.00%] [G loss: 7.912869] time: 1:02:01.119954\n",
      "[Epoch 2/30] [Batch 156/816] [D loss: 0.103859, acc:  94%, op_acc: 100.00%] [G loss: 8.210339] time: 1:02:03.057995\n",
      "[Epoch 2/30] [Batch 157/816] [D loss: 0.237457, acc:  82%, op_acc: 100.00%] [G loss: 7.300714] time: 1:02:04.997602\n",
      "[Epoch 2/30] [Batch 158/816] [D loss: 0.103998, acc:  95%, op_acc: 100.00%] [G loss: 9.488039] time: 1:02:06.932260\n",
      "[Epoch 2/30] [Batch 159/816] [D loss: 0.143056, acc:  89%, op_acc: 100.00%] [G loss: 9.361006] time: 1:02:08.861734\n",
      "[Epoch 2/30] [Batch 160/816] [D loss: 1.135763, acc:  94%, op_acc: 90.00%] [G loss: 7.281139] time: 1:02:10.796743\n",
      "[Epoch 2/30] [Batch 161/816] [D loss: 0.165396, acc:  97%, op_acc: 95.00%] [G loss: 8.277777] time: 1:02:12.737836\n",
      "[Epoch 2/30] [Batch 162/816] [D loss: 0.346173, acc:  71%, op_acc: 95.00%] [G loss: 8.714476] time: 1:02:14.664520\n",
      "[Epoch 2/30] [Batch 163/816] [D loss: 0.075644, acc:  94%, op_acc: 100.00%] [G loss: 8.110056] time: 1:02:16.596705\n",
      "[Epoch 2/30] [Batch 164/816] [D loss: 0.053869, acc:  97%, op_acc: 100.00%] [G loss: 9.056330] time: 1:02:18.526026\n",
      "[Epoch 2/30] [Batch 165/816] [D loss: 0.172027, acc:  67%, op_acc: 100.00%] [G loss: 7.410734] time: 1:02:20.458693\n",
      "[Epoch 2/30] [Batch 166/816] [D loss: 0.126107, acc:  89%, op_acc: 100.00%] [G loss: 12.143033] time: 1:02:22.381140\n",
      "[Epoch 2/30] [Batch 167/816] [D loss: 9.900048, acc:  48%, op_acc: 75.00%] [G loss: 14.760782] time: 1:02:24.317101\n",
      "[Epoch 2/30] [Batch 168/816] [D loss: 26.866606, acc:  51%, op_acc: 65.00%] [G loss: 27.769733] time: 1:02:26.255287\n",
      "[Epoch 2/30] [Batch 169/816] [D loss: 1.763319, acc:  43%, op_acc: 90.00%] [G loss: 25.600491] time: 1:02:28.185234\n",
      "[Epoch 2/30] [Batch 170/816] [D loss: 0.900929, acc:  50%, op_acc: 90.00%] [G loss: 22.022930] time: 1:02:30.121764\n",
      "[Epoch 2/30] [Batch 171/816] [D loss: 3.123936, acc:  39%, op_acc: 85.00%] [G loss: 18.111992] time: 1:02:32.058011\n",
      "[Epoch 2/30] [Batch 172/816] [D loss: 0.437503, acc:  49%, op_acc: 95.00%] [G loss: 13.257857] time: 1:02:33.993695\n",
      "[Epoch 2/30] [Batch 173/816] [D loss: 11.522306, acc:  52%, op_acc: 80.00%] [G loss: 13.486300] time: 1:02:35.921645\n",
      "[Epoch 2/30] [Batch 174/816] [D loss: 0.849383, acc:  67%, op_acc: 100.00%] [G loss: 19.984228] time: 1:02:37.858624\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    typeA = \"T1\"\n",
    "    typeB = \"T2\"\n",
    "    typeC = \"PD\"\n",
    "\n",
    "    pathImageTrainAPaired = \"/data/acunha/LungDL/MMIGAN/datasets/IOP/%s\" % (typeA)\n",
    "    pathImageTrainAUnpaired = \"/data/acunha/LungDL/MMIGAN/datasets/IOP/%s\" % (typeA)\n",
    "    pathOfResult= \"/data/acunha/LungDL/MMIGAN/Results-MMIGAN-IOP-JOURNAL-%s-%s-%s\" % (typeA, typeB, typeC)\n",
    "\n",
    "    imageRegion=\"\"\n",
    "    \n",
    "    epochs = 30\n",
    "    batch_size = 10\n",
    "    sample_interval = 200\n",
    "    img_shape = (256, 256, 1)\n",
    "    \n",
    "    dataLoader = DataLoader(img_res= img_shape, pathImageTrainAPaired= pathImageTrainAPaired, pathImageTrainAUnpaired=pathImageTrainAUnpaired, typeA=typeA,\n",
    "              typeB=typeB, typeC=typeC, imageRegion=imageRegion, seed=0)\n",
    "    dataLoader.holdOut(60)\n",
    "    \n",
    "    gan = MMIGAN(img_shape=img_shape, pathOfResult= pathOfResult, dataLoader= dataLoader)\n",
    "    gan.train(epochs=epochs, batch_size=batch_size, sample_interval=sample_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T13:24:32.689547Z",
     "start_time": "2020-02-17T13:24:32.682809Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T13:24:34.920915Z",
     "start_time": "2020-02-17T13:24:32.693365Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CARS",
   "language": "python",
   "name": "cars"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
